---
title: "Package_Workflow"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Package_Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Documentation 

Before you start, put your read fastq reads into a directory where you would like to do your analysis.
Please note: **xx** files and directories will be created.  

Please ensure that you have enough storage to save intermediate files and reformatted directories on your computer to proceed.  

If you are using the Silva of Unite databases, an ordinary personal computer may not have enough memory.  
***TODO-by default, intermediate files will be saved to a temporary directory. Provide instruction to user on how to override this.***

Prepare and input metadata and primer files according to the templates provided. Using the provided templates, paste sample names in the first column.  In the second column, user will specify the barcode they have used. Currently, user can specify 'its' or 'rps10'. These primer names are case sensitive.

The metadata file should look like this:  

***TODO add screenshot***

You will also need to make a sheet containing info on the primers used and forward and reverse primer sequences.  

***TODO add screenshot***

If barcodes are pooled, user will include each sample name twice, and under primer_name, either its or rps10 barcode.  

***TODO add screenshot of pooled barcode scenario***

Include any other metadata columns you'd like to use during later steps of the analysis. Formatting of names does not matter, but ensure that there are no blank left cells. 


### Load package

```{r installation, message=FALSE, warnings=FALSE}
#temporary installation method
#ADJUST PATHS FOR MOMENT
devtools::load_all("/Users/masudermann/rps10package")
document()
```

### Specify directory paths

```{r set paths, message=FALSE, warnings=FALSE}
#Change as needed based on where test dataset is located
directory_path<-"/Users/masudermann/rps10package/raw_data/rps10_ITS" ##choose a directory for all downstream steps
primer_path <-file.path(directory_path, "primer_info.csv") ##modify .csv name or keep this name
metadata_path <-file.path(directory_path,"metadata.csv") ##modify .csv name or keep this name. The sample_name in the metadata sheet needs to match the first part (before first underscore), of the zipped raw FASTQ files
#Specify the location of where cutadapt is located
cutadapt_path<-"/opt/homebrew/bin/cutadapt"
```


### Reorganize data tables for downstream steps and obtain primer counts across all sample reads  
The sample names, primer sequences and other metadata are reorganized in preparation for running Cutadapt to remove primers. 

```{r create data tables, message=FALSE, warnings=FALSE}
data_tables <-
  prepare_reads(
    directory_path,
    primer_path,
    metadata_path,
    maxN = 0,
    multithread = TRUE
  )
```

### Remove primers with Cutadapt and check that no primers still remain  

If primers still remain, you may want to adjust the parameters or manually remove any reads that still have primers (TODO-adjust this instruction later). 

***TODO-include a function to remove any ASV sequences that may still have primers. There likely won't be many-but it might be worth incorporating a function like this***

```{r Trim function, include=TRUE}
cut_trim(
  directory_path,
  cutadapt_path,
  verbose = TRUE,
  maxEE = 2,
  truncQ = 5,
  minLen = 200,
  maxLen = 297,
  minCutadaptlength = 50
) 
```


### Generate ASV abundance matrix.   

Depending on chosen specifications, the minOverlap and maxMismatch can be changed. The default values are the DADA2 default values 

***To do-There is a  message if analysis has already been run and parts don't re-run. Change so there is a way to override and rewrite files if the user chooses***

```{r asv abund matrix, include=TRUE}
asv_abund_matrix <-
  make_asv_abund_matrix(
    directory_path,
    minOverlap = 15,
    maxMismatch = 2,
    verbose = TRUE,
    multithread = TRUE
  )
```

### Taxonomic identification steps  

After databases are formatted, taxonomic information will be assigned to the ASVs that were inferred previously. The output will be a csv file containing each unique ASV, the abundance of each ASV across each sample, and the taxonomic assignments, the bootstrap support values, as well as a column listing the percent identity of each sequence to the corresponding sequence in the reference database.  

The user must specify which barcode they are using. Either 'rps10', its, or 'rps10_its'

**Databases** Todo-clarify instructions

Within the main directory, user must also have retrieved the rps10 oomycetedb (add link) or the Unite fungal database of choice. The names of the databases must be 'oomycetedb.fasta' and 'fungidb.fasta', or else they won't be recognized. The headers will altered and new versions of the databse will be saved. This will ensure there is consistency across databases. 

***TODO-save renamed db in temporary db or else they will take up too much space.***

```{r assign tax, include=TRUE}
summary <- assignTax(
  directory_path,
  data_tables,
  asv_abund_matrix,
  multithread = TRUE,
  barcode = "rps10_its",
)
```

### Convert ASV matrix to Taxmap and Phyloseq objects

While some users will appreciate having a matrix with all ASV sequences, taxonomic assignments and abundances of each ASV per sample, others may prefer to reformat matrices into Phyloseq or Taxmap objects. 

Two wrapper functions are shared. They are derived from Metacoder functions 'parse_tax_data' and 'as_phyloseq'.

```{r Reformat ASV abundance matrix}
obj_dada<-asvmatrix_to_taxmap(asv_abund_matrix, min_read_depth=10, minimum_bootstrap=75)
phylo_obj<-taxmap_to_phyloseq(obj_dada)
```

