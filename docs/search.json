[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Martha . Sudermann, Zachary S.L. Foster, Samantha Dawson, Hung Phan, Jeﬀ H. Chang, Niklaus J. Grünwald Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"16S Mothur SOP Validation","text":"vignette shows demulticoder used analyze mothur 16S SOP dataset featured DADA2 tutorials raw read files information dataset used analysis can found ","code":""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"setup-data-directory","dir":"Articles","previous_headings":"","what":"Setup data directory","title":"16S Mothur SOP Validation","text":"Download read files associated data specified location. default write tempdir(). Ensure computer sufficient storage change destination_folder path location space can save files . now unzipped directory desired destination called MiSeq_SOP also need rename files line file name conventions specified demulticoder see Documentation","code":"temp_dir <- tempdir() demulticoder_temp_dir <- file.path(temp_dir, \"demulticoder_sop_run\") dir.create(demulticoder_temp_dir, recursive = TRUE)  # Adjust output directory names as you like run_folder <- demulticoder_temp_dir data_folder <- file.path(run_folder, \"MiSeq_SOP\") output_folder <- file.path(run_folder, \"sop_16S_demulticoder_outputs\")  mothur_sop_path <- \"https://mothur.s3.us-east-2.amazonaws.com/wiki/miseqsopdata.zip\" downloaded_read_path <- file.path(run_folder, \"miseqsopdata.zip\")  if (!file.exists(downloaded_read_path)) {   # Only download if the file does not already exist   utils::download.file(mothur_sop_path, downloaded_read_path, quiet = TRUE)   unzip(downloaded_read_path, exdir = run_folder)   cat(\"Files downloaded:\", downloaded_read_path, \"\\n\")   print(list.files(run_folder))  # Print contents of the directory } else {   cat(\"Files already exist here:\", downloaded_read_path, \"! Skipping download.\\n\") } #> Files downloaded: /tmp/RtmpZ8vuI3/demulticoder_sop_run/miseqsopdata.zip  #> [1] \"MiSeq_SOP\"        \"miseqsopdata.zip\" #We will remove the \"001 appended at the end of the filename files <- list.files(data_folder)  for (file in files) {   new_name <- gsub(\"_001\", \"\", file)   old_file_path <- file.path(data_folder, file)   new_file_path <- file.path(data_folder, new_name)   rename_result <- file.rename(old_file_path, new_file_path) }"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"add-csv-input-files-to-the-data-directory","dir":"Articles","previous_headings":"","what":"Add CSV input files to the data directory","title":"16S Mothur SOP Validation","text":"Let’s now create metadata.csv input file required columns first sample names, second primer name/barcode used. subsequent columns (‘Day’ ‘’) necessary downstream steps phyloseq package. can construct CSV files Excel, construct save metadata file data directory directly R. metadata.csv Now, make primerinfo_params.csv input file make save second CSV input file name metabarcode used, primer sequences, optional dada2 parameter options. referenced DADA2 tutorial select parameter options. Note, primers already trimmed reads demultiplexing step occurs sequencing, just certain, included Earth Microbiome primers sequences described , searched across reads. primerinfo_params.csv construct save primerinfo_params file data directory","code":"metadata <- data.frame(   sample_name = c(\"F3D0_S188_L001\", \"F3D1_S189_L001\", \"F3D141_S207_L001\", \"F3D142_S208_L001\",   \"F3D143_S209_L001\", \"F3D144_S210_L001\", \"F3D145_S211_L001\", \"F3D146_S212_L001\",   \"F3D147_S213_L001\", \"F3D148_S214_L001\", \"F3D149_S215_L001\", \"F3D150_S216_L001\",   \"F3D2_S190_L001\", \"F3D3_S191_L001\", \"F3D5_S193_L001\", \"F3D6_S194_L001\",   \"F3D7_S195_L001\", \"F3D8_S196_L001\", \"F3D9_S197_L001\", \"Mock_S280_L001\"),   primer_name = rep(\"r16S\", 20),   Day = c(0, 1, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 2, 3, 5, 6, 7, 8, 9, NA),   When = c(rep(\"Early\", 2), rep(\"Late\", 10), rep(\"Early\", 7), NA) ) #Let's define a variable that stores our data directory path metadata_dir_path <- file.path(data_folder, \"metadata.csv\") write.csv(metadata, metadata_dir_path, row.names = FALSE) print(metadata) #>         sample_name primer_name Day  When #> 1    F3D0_S188_L001        r16S   0 Early #> 2    F3D1_S189_L001        r16S   1 Early #> 3  F3D141_S207_L001        r16S 141  Late #> 4  F3D142_S208_L001        r16S 142  Late #> 5  F3D143_S209_L001        r16S 143  Late #> 6  F3D144_S210_L001        r16S 144  Late #> 7  F3D145_S211_L001        r16S 145  Late #> 8  F3D146_S212_L001        r16S 146  Late #> 9  F3D147_S213_L001        r16S 147  Late #> 10 F3D148_S214_L001        r16S 148  Late #> 11 F3D149_S215_L001        r16S 149  Late #> 12 F3D150_S216_L001        r16S 150  Late #> 13   F3D2_S190_L001        r16S   2 Early #> 14   F3D3_S191_L001        r16S   3 Early #> 15   F3D5_S193_L001        r16S   5 Early #> 16   F3D6_S194_L001        r16S   6 Early #> 17   F3D7_S195_L001        r16S   7 Early #> 18   F3D8_S196_L001        r16S   8 Early #> 19   F3D9_S197_L001        r16S   9 Early #> 20   Mock_S280_L001        r16S  NA  <NA> #>         sample_name primer_name Day  When #> 1    F3D0_S188_L001        r16S   0 Early #> 2    F3D1_S189_L001        r16S   1 Early #> 3  F3D141_S207_L001        r16S 141  Late #> 4  F3D142_S208_L001        r16S 142  Late #> 5  F3D143_S209_L001        r16S 143  Late #> 6  F3D144_S210_L001        r16S 144  Late #> 7  F3D145_S211_L001        r16S 145  Late #> 8  F3D146_S212_L001        r16S 146  Late #> 9  F3D147_S213_L001        r16S 147  Late #> 10 F3D148_S214_L001        r16S 148  Late #> 11 F3D149_S215_L001        r16S 149  Late #> 12 F3D150_S216_L001        r16S 150  Late #> 13   F3D2_S190_L001        r16S   2 Early #> 14   F3D3_S191_L001        r16S   3 Early #> 15   F3D5_S193_L001        r16S   5 Early #> 16   F3D6_S194_L001        r16S   6 Early #> 17   F3D7_S195_L001        r16S   7 Early #> 18   F3D8_S196_L001        r16S   8 Early #> 19   F3D9_S197_L001        r16S   9 Early #> 20   Mock_S280_L001        r16S  NA  <NA> # Create the data frame primer_info <- data.frame(   primer_name = \"r16S\",   forward = \"GTGYCAGCMGCCGCGGTAA\",   reverse = \"GGACTACNVGGGTWTCTAAT\",   already_trimmed = TRUE,   minCutadaptlength = 0,   multithread = TRUE,   verbose = TRUE,   maxN = 0,   maxEE_forward = 2,   maxEE_reverse = 2,   truncLen_forward = 240,   truncLen_reverse = 160,   truncQ = 2,   minLen = 20,   maxLen = Inf,   minQ = 0,   trimLeft = 0,   trimRight = 0,   rm.lowcomplex = 0,   minOverlap = 12,   maxMismatch = 0,   min_asv_length = 50,   seed = 1 ) primer_params_path <- file.path(data_folder, \"primerinfo_params.csv\") write.csv(primer_info, primer_params_path, row.names = FALSE) #>   primer_name             forward              reverse already_trimmed #> 1        r16S GTGYCAGCMGCCGCGGTAA GGACTACNVGGGTWTCTAAT            TRUE #>   minCutadaptlength multithread verbose maxN maxEE_forward maxEE_reverse #> 1                 0        TRUE    TRUE    0             2             2 #>   truncLen_forward truncLen_reverse truncQ minLen maxLen minQ trimLeft #> 1              240              160      2     20    Inf    0        0 #>   trimRight rm.lowcomplex minOverlap maxMismatch min_asv_length seed #> 1         0             0         12           0             50    1"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"download-silva-reference-database","dir":"Articles","previous_headings":"","what":"Download SILVA reference database","title":"16S Mothur SOP Validation","text":"now retrieve SILVA database (v.138.2) (taxonomic level species). information ","code":"# We will also download the Silva database **with species** assignments here: silva_path <- \"https://zenodo.org/records/14169026/files/silva_nr99_v138.2_toSpecies_trainset.fa.gz?download=1\" silva_download_file_path <- file.path(data_folder, \"silva_nr99_v138.2_toSpecies_trainset.fa.gz\")  if (!file.exists(silva_download_file_path)) {   utils::download.file(silva_path, silva_download_file_path, quiet = TRUE)   cat(\"File downloaded to:\", silva_download_file_path, \"\\n\") } else {   cat(\"File already exists. Skipping download.\\n\") } #> File downloaded to: /tmp/RtmpZ8vuI3/demulticoder_sop_run/MiSeq_SOP/silva_nr99_v138.2_toSpecies_trainset.fa.gz"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"load-the-demulticoder-r-package-and-necessary-dependencies","dir":"Articles","previous_headings":"","what":"Load the demulticoder R package and necessary dependencies","title":"16S Mothur SOP Validation","text":"now, package loaded retrieving GitHub. submitting package CRAN.","code":"devtools::install_github(\"grunwaldlab/demulticoder\", force=TRUE) library(\"demulticoder\") library(\"metacoder\") library(\"phyloseq\") library(\"dplyr\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"prepare-reads-for-dada2-analyses","dir":"Articles","previous_headings":"","what":"Prepare reads for DADA2 analyses","title":"16S Mothur SOP Validation","text":"Remove N’s create directory structure downstream steps Note-intermediate files saved temporary folder automatically. prefer define temp directory path, refer documentation prepare_reads function","code":"# The file names will first need to be renamed to go through the demulticoder workflow, since it is looking for files that have suffixes like R1.fastq.gz or R2.fastq.gz output<-prepare_reads(   data_directory = data_folder, #Use data_directory path defined above    output_directory = output_folder, # Uses output_directory path defined above   overwrite_existing = TRUE)  #> Rows: 1 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (17): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 1 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (17): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 20 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, When #> dbl (1): Day #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpZ8vuI3/demulticoder_run/prefiltered_sequences"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"remove-primers-and-trim-reads","dir":"Articles","previous_headings":"","what":"Remove primers and trim reads","title":"16S Mothur SOP Validation","text":"Run Cutadapt remove primers trim reads dada2 filterAndTrim function  can now visualize outputs primer removal trimming steps. CSV files output showing samples still primer sequences barplot summarizes outputs. circumstances primer sequences may still remain. , ASVs residual primer sequences filtered end.","code":"cut_trim(   output,   cutadapt_path=\"/usr/bin/cutadapt\",   overwrite_existing = TRUE) #> Running cutadapt 3.5 for r16S sequence data  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D0_S188_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D0_S188_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D1_S189_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D1_S189_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D141_S207_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D141_S207_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D142_S208_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D142_S208_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D143_S209_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D143_S209_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D144_S210_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D144_S210_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D145_S211_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D145_S211_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D146_S212_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D146_S212_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D147_S213_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D147_S213_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D148_S214_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D148_S214_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D149_S215_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D149_S215_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D150_S216_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D150_S216_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D2_S190_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D2_S190_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D3_S191_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D3_S191_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D5_S193_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D5_S193_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D6_S194_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D6_S194_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D7_S195_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D7_S195_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D8_S196_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D8_S196_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D9_S197_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/F3D9_S197_L001_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/Mock_S280_L001_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/RtmpZ8vuI3/demulticoder_run/trimmed_sequences/Mock_S280_L001_R2_r16S.fastq.gz"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"core-asv-inference-step","dir":"Articles","previous_headings":"","what":"Core ASV inference step","title":"16S Mothur SOP Validation","text":"can now visualize outputs ASV inference step. first plot shows reads merged terms mismatches indels. second plot shows overlap lengths across inferred ASVs. can also look distribution ASV lengths","code":"make_asv_abund_matrix(   output,   overwrite_existing = TRUE) #> 33513360 total bases in 139639 reads from 20 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .................... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #> Convergence after  5  rounds. #> Error rate plot for the Forward read of primer pair r16S #> Sample 1 - 7112 reads in 1978 unique sequences. #> Sample 2 - 5299 reads in 1639 unique sequences. #> Sample 3 - 5463 reads in 1477 unique sequences. #> Sample 4 - 2914 reads in 904 unique sequences. #> Sample 5 - 2941 reads in 939 unique sequences. #> Sample 6 - 4312 reads in 1267 unique sequences. #> Sample 7 - 6741 reads in 1756 unique sequences. #> Sample 8 - 4560 reads in 1438 unique sequences. #> Sample 9 - 15636 reads in 3589 unique sequences. #> Sample 10 - 11412 reads in 2761 unique sequences. #> Sample 11 - 12017 reads in 3021 unique sequences. #> Sample 12 - 5032 reads in 1566 unique sequences. #> Sample 13 - 18075 reads in 3707 unique sequences. #> Sample 14 - 6250 reads in 1479 unique sequences. #> Sample 15 - 4052 reads in 1195 unique sequences. #> Sample 16 - 7369 reads in 1832 unique sequences. #> Sample 17 - 4765 reads in 1183 unique sequences. #> Sample 18 - 4871 reads in 1382 unique sequences. #> Sample 19 - 6504 reads in 1709 unique sequences. #> Sample 20 - 4314 reads in 897 unique sequences. #> 22342240 total bases in 139639 reads from 20 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .................... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #>    selfConsist step 7 #> Convergence after  7  rounds. #> Error rate plot for the Reverse read of primer pair r16S #> Sample 1 - 7112 reads in 1659 unique sequences. #> Sample 2 - 5299 reads in 1349 unique sequences. #> Sample 3 - 5463 reads in 1335 unique sequences. #> Sample 4 - 2914 reads in 853 unique sequences. #> Sample 5 - 2941 reads in 880 unique sequences. #> Sample 6 - 4312 reads in 1286 unique sequences. #> Sample 7 - 6741 reads in 1803 unique sequences. #> Sample 8 - 4560 reads in 1265 unique sequences. #> Sample 9 - 15636 reads in 3413 unique sequences. #> Sample 10 - 11412 reads in 2522 unique sequences. #> Sample 11 - 12017 reads in 2771 unique sequences. #> Sample 12 - 5032 reads in 1415 unique sequences. #> Sample 13 - 18075 reads in 3290 unique sequences. #> Sample 14 - 6250 reads in 1390 unique sequences. #> Sample 15 - 4052 reads in 1134 unique sequences. #> Sample 16 - 7369 reads in 1635 unique sequences. #> Sample 17 - 4765 reads in 1084 unique sequences. #> Sample 18 - 4871 reads in 1161 unique sequences. #> Sample 19 - 6504 reads in 1502 unique sequences. #> Sample 20 - 4314 reads in 732 unique sequences. #> 6540 paired-reads (in 107 unique pairings) successfully merged out of 6891 (in 197 pairings) input. #> 5027 paired-reads (in 101 unique pairings) successfully merged out of 5189 (in 157 pairings) input. #> 4986 paired-reads (in 81 unique pairings) successfully merged out of 5267 (in 166 pairings) input. #> 2595 paired-reads (in 52 unique pairings) successfully merged out of 2754 (in 108 pairings) input. #> 2553 paired-reads (in 60 unique pairings) successfully merged out of 2785 (in 119 pairings) input. #> 3646 paired-reads (in 55 unique pairings) successfully merged out of 4109 (in 157 pairings) input. #> 6079 paired-reads (in 81 unique pairings) successfully merged out of 6514 (in 198 pairings) input. #> 3968 paired-reads (in 91 unique pairings) successfully merged out of 4388 (in 187 pairings) input. #> 14233 paired-reads (in 143 unique pairings) successfully merged out of 15355 (in 352 pairings) input. #> 10529 paired-reads (in 120 unique pairings) successfully merged out of 11165 (in 277 pairings) input. #> 11154 paired-reads (in 137 unique pairings) successfully merged out of 11797 (in 298 pairings) input. #> 4349 paired-reads (in 85 unique pairings) successfully merged out of 4802 (in 179 pairings) input. #> 17431 paired-reads (in 153 unique pairings) successfully merged out of 17812 (in 272 pairings) input. #> 5850 paired-reads (in 81 unique pairings) successfully merged out of 6095 (in 159 pairings) input. #> 3713 paired-reads (in 86 unique pairings) successfully merged out of 3891 (in 147 pairings) input. #> 6865 paired-reads (in 99 unique pairings) successfully merged out of 7191 (in 187 pairings) input. #> 4428 paired-reads (in 68 unique pairings) successfully merged out of 4605 (in 128 pairings) input. #> 4576 paired-reads (in 101 unique pairings) successfully merged out of 4739 (in 174 pairings) input. #> 6092 paired-reads (in 109 unique pairings) successfully merged out of 6315 (in 173 pairings) input. #> 4269 paired-reads (in 20 unique pairings) successfully merged out of 4281 (in 28 pairings) input. #> Identified 61 bimeras out of 293 input sequences. #> $r16S #> [1] \"/tmp/RtmpZ8vuI3/demulticoder_run/asvabund_matrixDADA2_r16S.RData\""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"assign-taxonomy-step","dir":"Articles","previous_headings":"","what":"Assign taxonomy step","title":"16S Mothur SOP Validation","text":"check can take look read counts across workflow. sudden drops, reconsider adjusting certain dada2 parameters re-running analysis.","code":"assign_tax(   output,   asv_abund_matrix,   db_16S=\"silva_nr99_v138.2_toSpecies_trainset.fa.gz\",   retrieve_files=FALSE,   overwrite_existing = TRUE) #>          samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1    F3D0_S188_L001_R1_r16S  7733     7112      6976      6979   6540    6528 #> 2    F3D1_S189_L001_R1_r16S  5829     5299      5226      5239   5027    5016 #> 3  F3D141_S207_L001_R1_r16S  5926     5463      5331      5357   4986    4863 #> 4  F3D142_S208_L001_R1_r16S  3158     2914      2799      2830   2595    2521 #> 5  F3D143_S209_L001_R1_r16S  3164     2941      2822      2868   2553    2519 #> 6  F3D144_S210_L001_R1_r16S  4798     4312      4150      4228   3646    3507 #> 7  F3D145_S211_L001_R1_r16S  7331     6741      6592      6627   6079    5820 #> 8  F3D146_S212_L001_R1_r16S  4993     4560      4450      4470   3968    3879 #> 9  F3D147_S213_L001_R1_r16S 16956    15636     15433     15505  14233   13006 #> 10 F3D148_S214_L001_R1_r16S 12332    11412     11250     11267  10529    9935 #> 11 F3D149_S215_L001_R1_r16S 13006    12017     11857     11898  11154   10653 #> 12 F3D150_S216_L001_R1_r16S  5474     5032      4879      4925   4349    4240 #> 13   F3D2_S190_L001_R1_r16S 19489    18075     17907     17939  17431   16835 #> 14   F3D3_S191_L001_R1_r16S  6726     6250      6145      6176   5850    5486 #> 15   F3D5_S193_L001_R1_r16S  4418     4052      3930      3991   3713    3713 #> 16   F3D6_S194_L001_R1_r16S  7933     7369      7231      7294   6865    6678 #> 17   F3D7_S195_L001_R1_r16S  5103     4765      4646      4673   4428    4217 #> 18   F3D8_S196_L001_R1_r16S  5274     4871      4786      4802   4576    4547 #> 19   F3D9_S197_L001_R1_r16S  7023     6504      6341      6442   6092    6015 #> 20   Mock_S280_L001_R1_r16S  4748     4314      4287      4286   4269    4269"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"convert-asv-matrix-to-taxmap-and-phyloseq-objects","dir":"Articles","previous_headings":"","what":"Convert ASV matrix to taxmap and phyloseq objects","title":"16S Mothur SOP Validation","text":"Convert ASV matrix taxmap phyloseq objects one function","code":"objs<-convert_asv_matrix_to_objs(output) #> Rows: 232 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (20): F3D0_S188_L001_r16S, F3D1_S189_L001_r16S, F3D141_S207_L001_r16S, F... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For r16S dataset  #> Taxmap object saved in: /tmp/RtmpZ8vuI3/demulticoder_sop_run/sop_16S_demulticoder_outputs/taxmap_obj_r16S.RData  #> Phyloseq object saved in: /tmp/RtmpZ8vuI3/demulticoder_sop_run/sop_16S_demulticoder_outputs/phylo_obj_r16S.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"examine-accuracy-relative-to-a-mock-community","dir":"Articles","previous_headings":"","what":"Examine accuracy relative to a mock community","title":"16S Mothur SOP Validation","text":"Evaluate accuracy using mock community, shown dada2 tutorial looking mock community sample, able extract 20 bacterial sequences also matched present mock community.","code":"matrix_filepath <- file.path(output$directory_paths$output_directory, \"final_asv_abundance_matrix_r16S.csv\") tax_matrix<-read.csv(matrix_filepath)  unqs.mock <- tax_matrix[, c(2, which(colnames(tax_matrix) == \"Mock_S280_L001_r16S\"))]  unqs.mock <- unqs.mock[unqs.mock$Mock_S280_L001_r16S != 0,]  cat(\"DADA2 inferred\", nrow(unqs.mock), \"sample sequences present in the Mock community.\\n\") #> DADA2 inferred 20 sample sequences present in the Mock community."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"alpha-diversity-analysis","dir":"Articles","previous_headings":"","what":"Alpha diversity analysis","title":"16S Mothur SOP Validation","text":"Follow-work using phyloseq side--side comparison DADA2 workflow example examine alpha diversity results","code":"objs$phyloseq_r16S <- phyloseq::prune_samples(phyloseq::sample_names(objs$phyloseq_r16S) != \"Mock_S280_L001_r16S\", objs$phyloseq_r16S) # Remove mock sample  phyloseq::plot_richness(objs$phyloseq_r16S, x=\"Day\", measures=c(\"Shannon\", \"Simpson\"), color=\"When\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"beta-diversity-analysis","dir":"Articles","previous_headings":"","what":"Beta diversity analysis","title":"16S Mothur SOP Validation","text":"Examine ordination plots additional point comparison DADA2 tutorial","code":"# Transform data to proportions as appropriate for Bray-Curtis distances ps.prop <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(otu) otu/sum(otu)) ord.nmds.bray <- phyloseq::ordinate(ps.prop, method=\"NMDS\", distance=\"bray\") #> Run 0 stress 0.0808378  #> Run 1 stress 0.08096389  #> ... Procrustes: rmse 0.009177604  max resid 0.02810685  #> Run 2 stress 0.1231378  #> Run 3 stress 0.121153  #> Run 4 stress 0.08635462  #> Run 5 stress 0.0808378  #> ... Procrustes: rmse 5.812944e-06  max resid 1.554258e-05  #> ... Similar to previous best #> Run 6 stress 0.1231378  #> Run 7 stress 0.09061366  #> Run 8 stress 0.08096389  #> ... Procrustes: rmse 0.009176341  max resid 0.02810282  #> Run 9 stress 0.1433231  #> Run 10 stress 0.0808378  #> ... Procrustes: rmse 5.646214e-06  max resid 1.499963e-05  #> ... Similar to previous best #> Run 11 stress 0.1231378  #> Run 12 stress 0.0809639  #> ... Procrustes: rmse 0.009215858  max resid 0.02823353  #> Run 13 stress 0.121153  #> Run 14 stress 0.1320722  #> Run 15 stress 0.08635462  #> Run 16 stress 0.121153  #> Run 17 stress 0.08096389  #> ... Procrustes: rmse 0.009186834  max resid 0.02813753  #> Run 18 stress 0.08635462  #> Run 19 stress 0.08635462  #> Run 20 stress 0.09466617  #> *** Best solution repeated 2 times phyloseq::plot_ordination(ps.prop, ord.nmds.bray, color=\"When\", title=\"Bray NMDS\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"top-taxa-analysis","dir":"Articles","previous_headings":"","what":"Top taxa analysis","title":"16S Mothur SOP Validation","text":"Let’s look top 20 taxa early vs. late samples time points, shown DADA2 tutorial","code":"top20 <- names(sort(phyloseq::taxa_sums(objs$phyloseq_r16S), decreasing=TRUE))[1:20] ps.top20 <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(OTU) OTU/sum(OTU)) ps.top20 <- phyloseq::prune_taxa(top20, ps.top20) phyloseq::plot_bar(ps.top20, x=\"Day\", fill=\"Family\") + ggplot2::facet_wrap(~When, scales=\"free_x\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"16S Mothur SOP Validation","text":"Information 16S SOP Mothur dataset can found : https://mothur.org/wiki/miseq_sop/ Kozich, J. J., Westcott, S. L., Baxter, N. T., Highlander, S. K., Schloss, P. D. 2013. Development dual-index sequencing strategy curation pipeline analyzing amplicon sequence data MiSeq Illumina sequencing platform. Applied Environmental Microbiology 79:5112–5120. https://doi.org/10.1128/AEM.01043-13. Information DADA2 16S tutorial associated manuscript can found : https://benjjneb.github.io/dada2/tutorial.html Callahan, B. J., McMurdie, P. J., Rosen, M. J., Han, . W., Johnson, . J. ., Holmes, S. P. 2016. DADA2: high-resolution sample inference Illumina amplicon data. Nature Methods 13:581. https://doi.org/10.1038/nmeth.3869.","code":""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"software-and-packages","dir":"Articles","previous_headings":"","what":"Software and packages","title":"16S Mothur SOP Validation","text":"","code":"sessionInfo() #> R version 4.1.2 (2021-11-01) #> Platform: x86_64-pc-linux-gnu (64-bit) #> Running under: Pop!_OS 22.04 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 #> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0 #>  #> locale: #>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               #>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     #>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8    #>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  #>  [9] LC_ADDRESS=C               LC_TELEPHONE=C             #> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C        #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] purrr_1.0.4        future_1.40.0      dplyr_1.1.4        phyloseq_1.38.0    #> [5] metacoder_0.3.8    demulticoder_0.1.1 kableExtra_1.4.0   #>  #> loaded via a namespace (and not attached): #>   [1] systemfonts_1.0.4           plyr_1.8.9                  #>   [3] igraph_2.1.4                lazyeval_0.2.2              #>   [5] splines_4.1.2               listenv_0.9.1               #>   [7] BiocParallel_1.28.3         usethis_3.1.0               #>   [9] GenomeInfoDb_1.30.1         ggplot2_3.5.2               #>  [11] digest_0.6.37               foreach_1.5.2               #>  [13] htmltools_0.5.8.1           magrittr_2.0.3              #>  [15] memoise_2.0.1               cluster_2.1.2               #>  [17] tzdb_0.5.0                  remotes_2.5.0               #>  [19] globals_0.17.0              Biostrings_2.62.0           #>  [21] readr_2.1.5                 RcppParallel_5.1.10         #>  [23] matrixStats_1.5.0           vroom_1.6.5                 #>  [25] svglite_2.1.3               pkgdown_2.1.1               #>  [27] jpeg_0.1-10                 colorspace_2.1-1            #>  [29] textshaping_0.3.6           xfun_0.51                   #>  [31] callr_3.7.6                 crayon_1.5.3                #>  [33] RCurl_1.98-1.16             jsonlite_1.9.1              #>  [35] dada2_1.30.0                survival_3.2-13             #>  [37] iterators_1.0.14            ape_5.8-1                   #>  [39] glue_1.8.0                  gtable_0.3.6                #>  [41] zlibbioc_1.40.0             XVector_0.34.0              #>  [43] DelayedArray_0.20.0         pkgbuild_1.4.6              #>  [45] Rhdf5lib_1.16.0             BiocGenerics_0.40.0         #>  [47] scales_1.4.0                DBI_1.2.3                   #>  [49] miniUI_0.1.1.1              Rcpp_1.0.14                 #>  [51] viridisLite_0.4.2           xtable_1.8-4                #>  [53] bit_4.6.0                   stats4_4.1.2                #>  [55] profvis_0.4.0               htmlwidgets_1.6.4           #>  [57] RColorBrewer_1.1-3          ellipsis_0.3.2              #>  [59] urlchecker_1.0.1            pkgconfig_2.0.3             #>  [61] farver_2.1.2                sass_0.4.9                  #>  [63] deldir_2.0-4                labeling_0.4.3              #>  [65] tidyselect_1.2.1            rlang_1.1.6                 #>  [67] reshape2_1.4.4              later_1.4.1                 #>  [69] tools_4.1.2                 cachem_1.1.0                #>  [71] cli_3.6.5                   generics_0.1.3              #>  [73] ade4_1.7-23                 devtools_2.4.5              #>  [75] evaluate_1.0.3              biomformat_1.22.0           #>  [77] stringr_1.5.1               fastmap_1.2.0               #>  [79] yaml_2.3.10                 ragg_1.2.1                  #>  [81] processx_3.8.6              knitr_1.50                  #>  [83] bit64_4.6.0-1               fs_1.6.5                    #>  [85] nlme_3.1-155                mime_0.12                   #>  [87] xml2_1.3.7                  compiler_4.1.2              #>  [89] rstudioapi_0.17.1           curl_6.2.1                  #>  [91] png_0.1-8                   tibble_3.2.1                #>  [93] bslib_0.9.0                 stringi_1.8.7               #>  [95] ps_1.9.0                    desc_1.4.3                  #>  [97] lattice_0.20-45             Matrix_1.4-0                #>  [99] vegan_2.6-10                permute_0.9-7               #> [101] multtest_2.50.0             vctrs_0.6.5                 #> [103] furrr_0.3.1                 pillar_1.10.2               #> [105] lifecycle_1.0.4             rhdf5filters_1.6.0          #> [107] jquerylib_0.1.4             data.table_1.17.0           #> [109] bitops_1.0-7                httpuv_1.6.15               #> [111] GenomicRanges_1.46.1        R6_2.6.1                    #> [113] latticeExtra_0.6-30         hwriter_1.3.2.1             #> [115] promises_1.3.2              ShortRead_1.60.0            #> [117] parallelly_1.43.0           IRanges_2.28.0              #> [119] sessioninfo_1.2.3           codetools_0.2-18            #> [121] MASS_7.3-55                 pkgload_1.4.0               #> [123] rhdf5_2.38.1                SummarizedExperiment_1.24.0 #> [125] withr_3.0.2                 GenomicAlignments_1.30.0    #> [127] Rsamtools_2.10.0            S4Vectors_0.32.4            #> [129] GenomeInfoDbData_1.2.7      mgcv_1.8-39                 #> [131] parallel_4.1.2              hms_1.1.3                   #> [133] grid_4.1.2                  tidyr_1.3.1                 #> [135] rmarkdown_2.29              MatrixGenerics_1.6.0        #> [137] Biobase_2.54.0              shiny_1.10.0                #> [139] interp_1.1-6"},{"path":"/articles/Documentation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Documentation","text":"documentation page provides additional information use demulticoder R package processing analyzing metabarcode sequencing data. Specifically, provides detail input directory file requirements, key parameters.","code":""},{"path":"/articles/Documentation.html","id":"package-workflow","dir":"Articles","previous_headings":"","what":"Package workflow","title":"Documentation","text":"Prepare input files (metadata.csv, primerinfo_params.csv, unformatted reference databases, PE Illumina read files). Place input files single directory. Ensure file names comply specified format. Run four steps/functions pipeline default settings adjust parameters needed.","code":""},{"path":"/articles/Documentation.html","id":"data-directory-structure","dir":"Articles","previous_headings":"","what":"Data directory structure","title":"Documentation","text":"Place input files single directory. directory contain following files: PE Illumina read files metadata.csv primerinfo_params.csv Reference databases","code":""},{"path":"/articles/Documentation.html","id":"read-name-format","dir":"Articles","previous_headings":"Data directory structure","what":"Read Name Format","title":"Documentation","text":"avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz","code":""},{"path":"/articles/Documentation.html","id":"metadata-file-components","dir":"Articles","previous_headings":"Data directory structure","what":"Metadata file components","title":"Documentation","text":"metadata.csv file contains information samples primers (associated barcodes) used experiment. following two required columns: sample_name: Identifier sample (e.g., S1, S2) primer_name: Name primer used (applicable options: ‘rps10’, ‘’, ‘r16S’, ‘other1’, ‘other2’) Please add associated metadata file two required columns. can used downstream exploratory diversity analyses, sample data incorporated final phyloseq taxmap objects. Example file (optional third column):","code":"sample_name,primer_name,organism S1,rps10,Cry S2,rps10,Cin S1,its,Cry S2,its,Cin"},{"path":"/articles/Documentation.html","id":"primer-and-parameter-file-components","dir":"Articles","previous_headings":"Data directory structure","what":"Primer and parameter file components","title":"Documentation","text":"primerinfo_params.csv file contains information primer sequences used experiment, along optional additional parameters part DADA2 pipeline. anything specified, default values used. Required columns: primer_name: Name primer/barcode (e.g., , rps10) forward: Forward primer sequence reverse: Reverse primer sequence parameters can input primerinfo_params.csv file along defaults. Refer DADA2 documentation manual additional information. dada2 filterAndTrim function parameters: already_trimmed: Boolean indicating primers already trimmed (TRUE/FALSE) (default: FALSE) minCutadaptlength: Cutadapt parameter-Filter processed reads shorter specified length (default: 0) multithread: Boolean multithreading (TRUE/FALSE) (default: FALSE) verbose: Boolean verbose output (TRUE/FALSE) (default: FALSE) maxN: Maximum number N bases allowed (default: 0) maxEE_forward: Maximum expected errors forward reads (default: Inf) maxEE_reverse: Maximum expected errors reverse reads (default: Inf) truncLen_forward: Truncation length forward reads (default: 0) truncLen_reverse: Truncation length reverse reads (default: 0) truncQ: Truncation quality threshold (default: 2) minLen: Minimum length reads processing (default: 20) maxLen: Maximum length reads processing (default: Inf) minQ: Minimum quality score (default: 0) trimLeft: Number bases trim start reads (default: 0) trimRight: Number bases trim end reads (default: 0) rm.lowcomplex: Boolean removing low complexity sequences (default: TRUE) dada2 learnErrors function parameters: nbases: Number bases use error rate learning (default: 1e+08) randomize: Randomize reads error rate learning (default: FALSE) MAX_CONSIST: Maximum number self-consistency iterations (default: 10) OMEGA_C: Convergence threshold error rates (default: 0) qualityType: Quality score type (“Auto”, “FastqQuality”, “ShortRead”) (default: “Auto”) dada2 plot errors parameters: err_out: Return error rates used inference (default: TRUE) err_in: Use input error rates instead learning (default: FALSE) nominalQ: Use nominal Q-scores (default: FALSE) obs: Return observed error rates (default: TRUE) dada2 dada function parameters: OMP: Use OpenMP multi-threading available (default: TRUE) n: Number reads use error rate estimation (default: 1e+05) id.sep: Character separating sample ID sequence name (default: “\\s”) orient.fwd: NULL TRUE/FALSE orient sequences (default: NULL) pool: Pool samples error rate estimation (default: FALSE) selfConsist: Perform self-consistency iterations (default: FALSE) dada2 mergePairs function parameters: minOverlap: Minimum overlap merging paired-end reads (default: 12) maxMismatch: Maximum mismatches allowed overlap region (default: 0) dada2 removeBimeraDenovo function parameters: method: Method sample inference (“consensus” “pooled”) (default: “consensus”) dada2 assignTaxonomy function parameters: minBoot: minimum bootstrap confidence assigning taxonomic level (default: 0) tryRC: TRUE, reverse-complement sequences used classification better match reference sequences forward sequence (default FALSE) parameters include CSV input file: min_asv_length: Minimum length Amplicon Sequence Variants (ASVs) core dada ASV inference steps (default = 0) seed: greater reproducibility, user can specify integer set seed use following dada2 functions run: plotQualityProfile, learnErrors, dada, makeSequenceTable, assignTaxonomy (default: NULL) Example file (select optional columns forward reverse primer sequence columns):","code":"primer_name,forward,reverse,already_trimmed,minCutadaptlength,multithread,verbose,maxN,maxEE_forward,maxEE_reverse,truncLen_forward,truncLen_reverse,truncQ,minLen,maxLen,minQ,trimLeft,trimRight,rm.lowcomplex,minOverlap,maxMismatch,min_asv_length rps10,GTTGGTTAGAGYARAAGACT,ATRYYTAGAAAGAYTYGAACT,FALSE,100,TRUE,FALSE,1.00E+05,5,5,0,0,5,150,Inf,0,0,0,0,15,0,50 its,CTTGGTCATTTAGAGGAAGTAA,GCTGCGTTCTTCATCGATGC,FALSE,50,TRUE,FALSE,1.00E+05,5,5,0,0,5,50,Inf,0,0,0,0,15,0,50"},{"path":"/articles/Documentation.html","id":"reference-databases","dir":"Articles","previous_headings":"Data directory structure","what":"Reference Databases","title":"Documentation","text":"Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function. now, package compatible following databases: oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ easily accessible download found : https://zenodo.org/records/14169026 UNITE fungal database https://zenodo.org/records/14169026 two reference databases. user need reformat headers exactly outlined . user can specify path database input file. database FASTA format.","code":""},{"path":"/articles/Getting_started.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before You Start","title":"Getting Started","text":"following example, demonstrate key package functionality using test data set included package can follow along test data associated CSV input files contained package. Additional examples also available Example Vignettes","code":""},{"path":"/articles/Getting_started.html","id":"components-of-the-test-dataset","dir":"Articles","previous_headings":"","what":"Components of the test dataset","title":"Getting Started","text":"Paired-end short read amplicon data Files: S1_R1.fastq.gz, S1_R2.fastq.gz, S2_R1.fastq.gz, S2_R1.fastq.gz files must zipped end either R1.fastq.gz , R2.fastq.gz sample must R1 R2 files. metadata.csv New row unique sample Samples entered twice samples contain two pooled metabarcodes, test data template primerinfo_params.csv New row unique barcode associated primer sequence Optional Cutadapt dada2 parameters Taxonomy databases UNITE fungal database (abridged version) oomyceteDB","code":""},{"path":"/articles/Getting_started.html","id":"format-of-the-paired-end-read-files","dir":"Articles","previous_headings":"","what":"Format of the paired-end read files","title":"Getting Started","text":"package takes forward reverse Illumina short read sequence data. Format file names avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz.","code":""},{"path":"/articles/Getting_started.html","id":"format-of-metadata-file","dir":"Articles","previous_headings":"","what":"Format of metadata file","title":"Getting Started","text":"format CSV file named metadata.csv simple. template . Column 1. sample_name Column 2. primer_info (applicable options: ‘rps10’, ‘’, ‘r16S’, ‘other1’, other2’) Additional columns columns pasted two columns. can referenced later analysis steps save step loading metadata later. Notes S1 S2 come rhododendron rhizobiome dataset random subset full set reads. S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled metabarcodes (ITS1 rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. metadata.csv looks like test dataset:","code":""},{"path":"/articles/Getting_started.html","id":"format-of-primer-and-parameters-file","dir":"Articles","previous_headings":"","what":"Format of primer and parameters file","title":"Getting Started","text":"Primer sequence information user-defined parameters placed primerinfo_params.csv simplify functions called, user provide parameters within input file. recommend using template linked . Remember add additional optional dada2 parameters want use. three required columns (headers) : Column 1. sample_name Column 2. forward (primer sequence) Column 3. reverse(primer sequence) info parameter specifics, see Documentation","code":""},{"path":"/articles/Getting_started.html","id":"reference-database-format","dir":"Articles","previous_headings":"","what":"Reference Database Format","title":"Getting Started","text":"oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ UNITE fungal database https://unite.ut.ee/repository.php See ‘Documentation’ tab. Databases downloaded sources placed user-specified data folder raw data files csv files located.","code":""},{"path":"/articles/Getting_started.html","id":"additional-notes","dir":"Articles","previous_headings":"","what":"Additional Notes","title":"Getting Started","text":"Computer specifications may limiting factor. using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases reads subsetted following example run personal computer least 16 GB RAM. computer crashes taxonomic assignment step, need switch computer sufficient memory. must ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/Getting_started.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Getting Started","text":"Dependencies: First install Cutadapt program following instructions : https://cutadapt.readthedocs.io/en/stable/installation.html Let’s locate Cutadapt executable . must Terminal window: followed cutadapt installation instructions create conda environment called Cutadapt (change whatever named environment), install . Open Terminal window type commands: Second, make sure following R packages installed: install, follow instructions: https://www.bioconductor.org/packages/release/bioc/html/dada2.html install: https://www.bioconductor.org/packages/release/bioc/html/phyloseq.html metacoder (Available CRAN) install development version package (submission CRAN progress):","code":"#If you installed with pip or pipx, or homebrew, run this command from a Terminal window which cutadapt cutadapt --version #Run commands from a Terminal window conda activate cutadapt which cutadapt cutadapt --version"},{"path":"/articles/Getting_started.html","id":"loading-the-package","dir":"Articles","previous_headings":"","what":"Loading the Package","title":"Getting Started","text":"now, package loaded retrieving GitHub. Submission CRAN progress.","code":"#Here we install demulticoder (instructions will be updated once available through CRAN) devtools::install_github(\"grunwaldlab/demulticoder\") library(\"demulticoder\") #Let's make sure other packages are loaded: library(\"devtools\") library(\"dada2\") library(\"phyloseq\") library(\"metacoder\")"},{"path":"/articles/Getting_started.html","id":"reorganize-data-tables-and-set-up-data-directory-structure","dir":"Articles","previous_headings":"","what":"Reorganize data tables and set-up data directory structure","title":"Getting Started","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"output<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(), # Change to your desired output directory path or leave as is\",    overwrite_existing = TRUE) #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/Rtmp3NR9D0/demulticoder_run/prefiltered_sequences"},{"path":"/articles/Getting_started.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"","what":"Remove primers with Cutadapt","title":"Getting Started","text":"running Cutadapt, please ensure installed ","code":"cut_trim(   output,   cutadapt_path = \"/usr/bin/cutadapt\", #CHANGE LOCATION TO YOUR LOCAL INSTALLATION   overwrite_existing = TRUE) #> Running cutadapt 3.5 for its sequence data #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running cutadapt 3.5 for rps10 sequence data #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences."},{"path":"/articles/Getting_started.html","id":"asv-inference-step","dir":"Articles","previous_headings":"","what":"ASV inference step","title":"Getting Started","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   output,   overwrite_existing = TRUE) #> 710804 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its #> Sample 1 - 1479 reads in 660 unique sequences. #> Sample 2 - 1215 reads in 613 unique sequences. #> 724230 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821851 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> $its #> [1] \"/tmp/Rtmp3NR9D0/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/Rtmp3NR9D0/demulticoder_run/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/Getting_started.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"","what":"Taxonomic assignment step","title":"Getting Started","text":"Using core assignTaxonomy function dada2, taxonomic assignments given ASVs.","code":"assign_tax(   output,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing = TRUE) #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1          S1_R1_its  2564     1479      1425      1431   1315    1315 #> 2          S2_R1_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1        S1_R1_rps10  1830     1429      1429      1422   1420    1420 #> 2        S2_R1_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/Getting_started.html","id":"format-of-output-matrices","dir":"Articles","previous_headings":"","what":"Format of output matrices","title":"Getting Started","text":"default output CSV file per metabarcode inferred ASVs sequences, taxonomic assignments, bootstrap supports provided dada2. data can used input downstream steps. view files, locate files prefix final_asv_abundance_matrix specified output directory. example, home directory within demulticoder_test. can open matrices using Excel recommend checking column ASV sequences make sure aren’t truncated sequences *important also review column showing taxonomic assignments ASV associated bootstrap supports. can also load matrices R environment view R Studio. rps10 matrix can also inspect first rows ITS1 matrix","code":"rps10_matrix<-read.csv(\"~/demulticoder_test/final_asv_abundance_matrix_rps10.csv\", header=TRUE) head(rps10_matrix) #>   asv_id #> 1  asv_1 #> 2  asv_2 #> 3  asv_3 #> 4  asv_4 #> 5  asv_5 #>                                                                                                                                                                                                                                                                                                                                                                                                                                                    sequence #> 1 GAAAATCTTTGTGTCGGTGGTTCAAATCCACCTCCAGACAAATTAATTTTTTAAAACTTATGTTTATATTAAGAATTACATTTAAATCTATTAAAAAAATAAATAACATTAAACCTATTTTATTAAATCAAAAAAATTTAAATAAATTTAATAATATTAAAATAAATGGAATATTTAAAACAAAAAATAAAAATAAAATTTTTACAGTATTAAAATCACCACATGTTAATAAAAAAGCACGTGAACATTTTATTTATAAAAATTTTACACAAAAAGTTGAAATTAAATGTTTAAATATTTTTCAATTATTAAATTTTTTAATTTTGATTAAAAAAATCTTAACAGAAAATTTTATTATTACTACAAAAATAATTAAACAATAATTAATATATGCTTATAGCTTAATGGATAAAGCATTAGATTGCGGATCTACAAAATGAA #> 2    GAAAATCTTTGTGTCGGTGGTTCAAGTCCGCCTCCAAACATCTTATAAAAATGTATGTATATTTTACGAATTAGTTTTAAATCAGTAGAAAAAATAAATGAATTAAAAAAAAATATCTCAAAAATAAAAAAAATATATAATTTTAAAAATTTAAAAGTAAATGGTATTTTTAGAACAAAAAATAAAAATAAAATTTTTACATTATTAAAATCACCCCATGTTAATAAAAAATCACGTGAACATTTTATATATAAAAATTATATTCAAAAAATTGATTTAAATTTTTCAAATTTTTTTCAATTATTAAATTTTTTAGTAATTTTAAAAAAAATATTATCTAAAGATTTTTTAATCAATATCAAAATTATTAAAAAAAATAAAAAAAATATGCTTATAGCTTAATGGATAAAGCGTTAGATTGCGGATCTATAAAATGAA #> 3      GAAAATCTTTGTGTCGGTGGTTCAAGTCCACCTCCAGACAAAAATAATAACTATGTATATTTTAAGAATAACTTTTAAATCAATACAAAAAATAAATAATATTAAAAAAAATTTATTAAAATTAAAAAAAATAAATAAATTTAAAAATATTCAAATAAAAGGAATATTTAAAATAAAAGATAAAAATAAAATTTTTACTTTATTAAAATCACCTCACGTAAATAAAAAATCTCGTGAACATTTTATTTATAAAAATTATACTCAAAAAATAGATGTAAAATTTTCAAATATTATTGAATTATTTAATTTTATAATACTTATTAAAAAAGTTTTAACTAAAAATTTTATAATAAATTTTAAAATTATAAAATATAATAAAAAAAAAATGCTTATAGCTTAATGGATAAAGCGTTAGATTGCGGATCTATAAAATGAA #> 4  GAAAATCTTTGTGTCGGTGGTTCAAATCCGCCTCCAAACAAAAATATAAATTATTTTATATGTATATTTTAAGAATAAGTTTTAAATCTGTATCAAAAATAGATAAAATTAAAAAAGAATTATCAAGATTAAAAAAAATATATAATTTTAAAAATATAACAATTAATGGTATTTTTAAAACAAAAAATAAAGATAAAATTTTTACATTATTAAAATCACCTCATGTTAATAAAAAATCTCGCGAACATTTTATTTATAAAAATTATGTACAAAAAATAGATTTAAATTTTATTAATATTTTTCAATTAATAAATTTTTTAATAATTTTAAAAAAAAAATTATCAAAAAATGTATTAATAAATGTAAAAATAATTAAAAAATAAAAAAATATGCTTATAGCTTAATGGATAAAGCGTTAGATTGCGGATCTATAAAATGAA #> 5    GAAAATCTTTGTGTCGGTGGTTCAAGTCCGCCTCCAAACATCTTATAAAAATGTATGTATATTTTACGAATTAGTTTTAAATCAGTAGAAAAAATAAATGAATTAAAAAAAAATATCTCAAAAATAAAAAAAATATATAATTTTAAAAATTTAAAAGTAAAATGGTATTTTTAGAACAAAAAATAAAAATAAAATTTTTACATTATTAAAATCACCCCATGTTAATAAAAAATCACGTGAACATTTTATATATAAAAATTATATTCAAAAAATTGATTTAAATTTTTCAAATTTTTTTCAATTATTAAATTTTTTAGTAATTTTAAAAAAAACATTATCTAAAGATTTTTTAATCAATATTAAAATTATTAAAAAAAATAAAAAAATATGCTTATAGCTTAATGGATAAAGCGTTAGATTGCGGATCTATAAAATGAA #>                                                                                                                                                                                                                   dada2_tax #> 1 Stramenopila--100--Kingdom;Oomycota--100--Phylum;Oomycetes--100--Class;Peronosporales--26--Order;Peronosporaceae--26--Family;Phytophthora--23--Genus;Phytophthora_sp.kununurra--13--Species;oomycetedb_650--13--Reference #> 2                    Stramenopila--100--Kingdom;Oomycota--100--Phylum;Oomycetes--100--Class;Pythiales--100--Order;Pythiaceae--100--Family;Pythium--100--Genus;Pythium_dissotocum--49--Species;oomycetedb_749--24--Reference #> 3 Stramenopila--100--Kingdom;Oomycota--100--Phylum;Oomycetes--100--Class;Pythiales--100--Order;Pythiaceae--100--Family;Globisporangium--100--Genus;Globisporangium_cylindrosporum--22--Species;oomycetedb_41--22--Reference #> 4                   Stramenopila--100--Kingdom;Oomycota--100--Phylum;Oomycetes--100--Class;Pythiales--94--Order;Pythiaceae--94--Family;Pythium--89--Genus;Pythium_aphanidermatum--19--Species;oomycetedb_727--19--Reference #> 5                    Stramenopila--100--Kingdom;Oomycota--100--Phylum;Oomycetes--100--Class;Pythiales--100--Order;Pythiaceae--100--Family;Pythium--100--Genus;Pythium_oopapillum--67--Species;oomycetedb_771--67--Reference #>   S1_rps10 S2_rps10 #> 1      842       17 #> 2      578      220 #> 3        0      587 #> 4        0      516 #> 5        0      163 its_matrix<-read.csv(\"~/demulticoder_test/final_asv_abundance_matrix_its.csv\", header=TRUE) head(its_matrix) #>   asv_id #> 1  asv_1 #> 2  asv_2 #> 3  asv_3 #> 4  asv_4 #> 5  asv_5 #> 6  asv_6 #>                                                                                                                                                                                                                                                                                                                                 sequence #> 1                                              AAAAAGTCGTAACAAGGTTTCCGTAGGTGAACCTGCGGAAGGATCATTAGTGAATCTTCAAAGTCGGCTCGTCAGATTGTGCTGGTGGAGACACATGTGCACGTCTACGAGTCGCAAACCCACACACCTGTGCATCTATGACTCTGAGTGCCGCTTTGCATGGCCCCTTGATTTGGGCCTGGCGCTCGAGTACTTTCACACACTCTCGAATGTAATGGAATGTCTTCTTGTGCATAACGTACAAACAGAAACAACTTTCAACAACGGATCTCTTGGCTCTC #> 2                                              AAAAAGTCGTAACAAGGTTTCCGTAGGTGAACCTGCGGAAGGATCATTAGTGAATCTTCAAAGTCGGCTCGTCGGATTGTGCTGGTGGAGACACATGTGCACGTCTACGAGTCGCAAACCCACACACCTGTGCATCTATGACTCTGAGTGCCGCTTTGCATGGCCCCTTGATTTGGGCCTGGCGCTCGAGTACTTTCACACACTCTCGAATGTAATGGAATGTCTTGTTGTGCATAACGTACAAACAGAAACAACTTTCAACAACGGATCTCTTGGCTCTC #> 3                                              AAAAAGTCGTAACAAGGTTTCCGTAGGTGAACCTGCGGAAGGATCATTAGTGAATCTTCAAAGTCGGCTCGTCGGATTGTGCTGGTGGAGACACATGTGCACGTCTACGAGTCGCAAACCCACACACCTGTGCATCTATGACTCTGAGTGCCGCTTTGCATGGCCCCTTGATTTGGGCCTGGCGCTCGAGTACTTTCACACACTCTCGAATGTAATGGAATGACTTGTTGTGCATAACGTACAAACAGAAACAACTTTCAACAACGGATCTCTTGGCTCTC #> 4 AAGTCGTAACAAGGTTTCCGTAGGTGAACCTGCGGAAGGATCATTAACGAGTTCAACTGGGGTCTGTGGGGATTCGATGCTGGCCTCCGGGCATGTGCTCGTCCCCCGGACCTCACATCCACTCACAACCCCTGTGCATCATGAGAGGTGTGGTCCTCCGTCATGGGAGCAGCTCCCCTCCGGGCGTGTTTGTACACCCCGGGCGGTCGAGCCGGGACTGCACTTCGACGCCTTTACACAAACCTTTGAATCAGTGATGTAGAATGTCATGGCCAGCGATGGTCGAACTTTAAATACAACTTTCAACAACGGATCTCTTGGCTCTC #> 5                                              AAAAAGTCGTAACAAGGTTTCCGTAGGTGAACCTGCGGAAGGATCATTAGTGAATCTTCAAAGTCGGCTCGTCGGATTGTGCTGGTGGAGACACATGTGCACATCTACGAGTCGCAAACCCACACACCTGTGCATCTATGACTCTGAGTGCCGCTTTGCATGGCCCCTTGATTTGGGCCTGGCGCTCGAGTACTTTCACACACTCTCGAATGTAATGGAATGTCTTGTTGTGCATAACGTACAAACAGAAACAACTTTCAACAACGGATCTCTTGGCTCTC #> 6                                                                                                    AAGTCGTAACAAGGTTTCCGTAGGTGAACCTGCGGAAGGATCATTAAAAATGAAGCCGGGAAACCGGTCCTTCTAACCCTTGATTATCTTAATTTGTTGCTTTGGTGGGCCGCGCAAGCACCGGCTTCGGCTGGATCGTGCCCGCCAGAGGACCACAACTCTGTATCAAATGTCGTCTGAGTACTATATAATAGTTAAAACTTTCAACAACGGATCTCTTGGTTCTG #>                                                                                                                                                                        dada2_tax #> 1 Fungi--100--Kingdom;Basidiomycota--100--Phylum;Agaricomycetes--100--Class;Sebacinales--100--Order;unidentified--98--Family;unidentified--98--Genus;Sebacinales_sp--98--Species #> 2 Fungi--100--Kingdom;Basidiomycota--100--Phylum;Agaricomycetes--100--Class;Sebacinales--100--Order;unidentified--97--Family;unidentified--97--Genus;Sebacinales_sp--97--Species #> 3 Fungi--100--Kingdom;Basidiomycota--100--Phylum;Agaricomycetes--100--Class;Sebacinales--100--Order;unidentified--99--Family;unidentified--99--Genus;Sebacinales_sp--99--Species #> 4 Fungi--100--Kingdom;Basidiomycota--62--Phylum;Agaricomycetes--59--Class;Hymenochaetales--21--Order;Repetobasidiaceae--8--Family;Rickenella--8--Genus;Rickenella_sp--8--Species #> 5 Fungi--100--Kingdom;Basidiomycota--100--Phylum;Agaricomycetes--100--Class;Sebacinales--100--Order;unidentified--96--Family;unidentified--96--Genus;Sebacinales_sp--96--Species #> 6         Fungi--100--Kingdom;Ascomycota--100--Phylum;Leotiomycetes--98--Class;Helotiales--98--Order;unidentified--86--Family;unidentified--86--Genus;Helotiales_sp--86--Species #>   S1_its S2_its #> 1    436      0 #> 2    405      0 #> 3      0    299 #> 4    209     65 #> 5      0    200 #> 6     16    116"},{"path":"/articles/Getting_started.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Getting Started","text":"","code":"objs<-convert_asv_matrix_to_objs(output, minimum_bootstrap = 0, save_outputs = TRUE)"},{"path":"/articles/Getting_started.html","id":"objects-can-now-be-used-for-downstream-data-analysis","dir":"Articles","previous_headings":"","what":"Objects can now be used for downstream data analysis","title":"Getting Started","text":"make heattrees using taxmap object. First make heat tree examine fungal community composition using ITS1 data.  Now make heat tree showcase oomycete community composition using rps10 data  can also variety analyses, convert phyloseq object demonstrate make stacked bar plot relative abundance taxa sample ITS1-barcoded samples  Finally, demonstrate make stacked bar plot relative abundance taxa sample rps10-barcoded samples","code":"objs$taxmap_its %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE)) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") objs$taxmap_rps10 %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE)) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),       strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)       )   )  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martha . Sudermann. Author, maintainer, copyright holder. Zachary S. L Foster. Author. Samantha Dawson. Author. Hung Phan. Author. Jeff H. Chang. Author. Niklaus Grünwald. Author, copyright holder.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sudermann M, Foster Z, Dawson S, Phan H, H. Chang J, Grünwald N (2025). demulticoder: Set Tools Simultaneous Analysis Multiplexed Metabarcodes. https://grunwaldlab.github.io/demulticoder/, https://github.com/grunwaldlab/demulticoder.","code":"@Manual{,   title = {demulticoder: A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes},   author = {Martha A. Sudermann and Zachary S. L Foster and Samantha Dawson and Hung Phan and Jeff {H. Chang} and Niklaus Grünwald},   year = {2025},   note = {https://grunwaldlab.github.io/demulticoder/, https://github.com/grunwaldlab/demulticoder}, }"},{"path":[]},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Introduction","title":"A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes","text":"demulticoder package cutadapt DADA2 wrapper package metabarcodng analyses. main commands outputs intuitive comprehensive, helps account complex iterative nature metabarcoding analyses. brief schematic general workflow:","code":""},{"path":"/index.html","id":"key-features","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Key Features","title":"A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes","text":"automates use DADA2 analyze data derived multiple metabarcodes. reduces number manual input steps Handles analysis two metabarcodes multiplexed sequencing batch Analyze different types metabarcodes simultaneously Reproducible workflows oomycetes Supported metabarcodes: 16S rDNA, ITS1, rps10, two additional metabarcodes","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Installation","title":"A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes","text":"Dependencies: First install cutadapt program following instructions : https://cutadapt.readthedocs.io/en/stable/installation.html Let’s locate cutadapt executable . must Terminal window: followed cutadapt installation instructions create conda environment called cutadapt (change whatever named environment), install , open Terminal window type commands: Second, make sure following R packages installed: install, follow instructions: https://www.bioconductor.org/packages/release/bioc/html/dada2.html install: https://www.bioconductor.org/packages/release/bioc/html/phyloseq.html install development version package (submission CRAN progress):","code":"#If you installed with pip or pipx, or homebrew, run this command from a Terminal window which cutadapt cutadapt --version #Run commands from a Terminal window conda activate cutadapt which cutadapt cutadapt --version #Here we install demulticoder (instructions will be updated once available through CRAN) devtools::install_github(\"grunwaldlab/demulticoder\") library(\"demulticoder\")  #Let's make sure other packages are loaded: library(\"devtools\") library(\"dada2\") library(\"phyloseq\") library(\"metacoder\")"},{"path":"/index.html","id":"quick-start","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Quick start","title":"A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes","text":"1. Set-input directory files demonstrate use package, small test data set comes loaded package. data set used workflow example . Already loaded test data set directory following files: Files: S1_R1.fastq.gz, S1_R2.fastq.gz, S2_R1.fastq.gz, S2_R1.fastq.gz files must end either R1.fastq.gz , R2.fastq.gz sample must R1 R2 files. New row unique sample Samples entered twice samples contain two pooled metabolites, test data template New row unique barcode associated primer sequence Optional cutadapt DADA2 parameters UNITE fungal database (abridged version) completed See Documentation format databases input files. details step, check Getting Started tab package website 2. Prepare reads 3. Cut trim reads User must install cutadapt local machine append path executable. 4. Make ASV abundance matrix 5. Assign taxonomy 6. Convert ASV matrix taxmap phyloseq objects","code":"output<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"), # This allows us to use the test directory located within the package   output_directory = \"~/demulticoder_test\", # Change to you preferred location on your local computer (Example: \"~/demulticoder_test\")   overwrite_existing = TRUE) cut_trim(   output,   cutadapt_path=\"/usr/bin/cutadapt\", # Change to the location on your computer. (Example: \"/usr/bin/cutadapt\")   overwrite_existing = TRUE) make_asv_abund_matrix(   output,   overwrite_existing = TRUE) assign_tax(   output,   asv_abund_matrix,   overwrite_existing = TRUE) objs<-convert_asv_matrix_to_objs(output)"},{"path":"/index.html","id":"check-out-the-website-to-view-the-documentation-and-see-more-examples","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Check out the website to view the documentation and see more examples","title":"A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes","text":"information source code, check package repository: https://grunwaldlab.github.io/demulticoder/","code":""},{"path":"/index.html","id":"for-source-code","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"For source code:","title":"A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes","text":"https://github.com/grunwaldlab/demulticoder/","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Citation","title":"A Set of Tools for the Simultaneous Analysis of Multiplexed Metabarcodes","text":"package developed Martha Sudermann, Zachary Foster, Samantha Dawson, Hung Phan, Jeff Chang, Niklaus Grünwald associated manuscript currently review.","code":""},{"path":"/reference/add_pid_to_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Add PID and bootstrap values to tax result. — add_pid_to_tax","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"Add PID bootstrap values tax result.","code":""},{"path":"/reference/add_pid_to_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"","code":"add_pid_to_tax(tax_results, asv_pid)"},{"path":"/reference/add_pid_to_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"tax_results dataframe containing taxonomic assignments asv_pid Percent identity information ASV relative reference database sequence","code":""},{"path":"/reference/assignTax_as_char.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine taxonomic assignments and bootstrap values for each metabarcode into single falsification vector — assignTax_as_char","title":"Combine taxonomic assignments and bootstrap values for each metabarcode into single falsification vector — assignTax_as_char","text":"Combine taxonomic assignments bootstrap values metabarcode single falsification vector","code":""},{"path":"/reference/assignTax_as_char.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine taxonomic assignments and bootstrap values for each metabarcode into single falsification vector — assignTax_as_char","text":"","code":"assignTax_as_char(tax_results, temp_directory_path, metabarcode)"},{"path":"/reference/assignTax_as_char.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine taxonomic assignments and bootstrap values for each metabarcode into single falsification vector — assignTax_as_char","text":"tax_results dataframe containing taxonomic assignments","code":""},{"path":"/reference/assign_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy functions — assign_tax","title":"Assign taxonomy functions — assign_tax","text":"Assign taxonomy functions","code":""},{"path":"/reference/assign_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy functions — assign_tax","text":"","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files = FALSE,   overwrite_existing = FALSE,   db_rps10 = \"oomycetedb.fasta\",   db_its = \"fungidb.fasta\",   db_16S = \"bacteriadb.fasta\",   db_other1 = \"otherdb1.fasta\",   db_other2 = \"otherdb2.fasta\" )"},{"path":"/reference/assign_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy functions — assign_tax","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function asv_abund_matrix final abundance matrix containing amplified sequence variants retrieve_files Logical, TRUE/FALSE whether copy files temp directory output directory. Default FALSE. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. db_rps10 reference database rps10 metabarcode db_its reference database metabarcode db_16S SILVA 16S-rRNA reference database provided user db_other1 reference database metabarcode 1 (assumes format like SILVA DB entries) db_other2 reference database metabarcode 2 (assumes format like SILVA DB entries)","code":""},{"path":"/reference/assign_tax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign taxonomy functions — assign_tax","text":"Taxonomic assignments unique ASV sequence","code":""},{"path":"/reference/assign_tax.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Assign taxonomy functions — assign_tax","text":"point, 'dada2' function assignTaxonomy used assign taxonomy inferred ASVs.","code":""},{"path":"/reference/assign_tax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign taxonomy functions — assign_tax","text":"","code":"# \\donttest{ # Assign taxonomies to ASVs on by metabarcode analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   overwrite_existing = TRUE ) #> Existing files found in the output directory. Overwriting existing files. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpCSy1OS/demulticoder_run/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Running cutadapt 3.5 for its sequence data #> Read in 299 paired-sequences, output 163 (54.5%) filtered paired-sequences. #> Read in 235 paired-sequences, output 144 (61.3%) filtered paired-sequences.  #> Running cutadapt 3.5 for rps10 sequence data #> Read in 196 paired-sequences, output 145 (74%) filtered paired-sequences. #> Read in 253 paired-sequences, output 182 (71.9%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup, overwrite_existing = TRUE ) #> 80608 total bases in 307 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the  Forward  read of primer pair  its #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 163 reads in 84 unique sequences. #> Sample 2 - 144 reads in 96 unique sequences. #> 82114 total bases in 307 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the  Reverse  read of primer pair  its #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 163 reads in 128 unique sequences. #> Sample 2 - 144 reads in 119 unique sequences. #> 132 paired-reads (in 4 unique pairings) successfully merged out of 132 (in 4 pairings) input. #> 99 paired-reads (in 8 unique pairings) successfully merged out of 99 (in 8 pairings) input.  #> Identified 0 bimeras out of 12 input sequences.  #> 91897 total bases in 327 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the  Forward  read of primer pair  rps10 #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 145 reads in 107 unique sequences. #> Sample 2 - 182 reads in 133 unique sequences. #> 91567 total bases in 327 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the  Reverse  read of primer pair  rps10 #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 145 reads in 114 unique sequences. #> Sample 2 - 182 reads in 170 unique sequences. #> 145 paired-reads (in 2 unique pairings) successfully merged out of 145 (in 2 pairings) input. #> 181 paired-reads (in 3 unique pairings) successfully merged out of 181 (in 3 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/tmp/RtmpCSy1OS/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/RtmpCSy1OS/demulticoder_run/asvabund_matrixDADA2_rps10.RData\" #>  assign_tax( analysis_setup, asv_abund_matrix, retrieve_files=FALSE, overwrite_existing = TRUE ) #> Tracking read counts: #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its   299      163       146       141    132     132 #> 2             S2_its   235      144       113        99     99      99 #> Tracking read counts: #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10   196      145       145       145    145     145 #> 2           S2_rps10   253      182       181       181    181     181 # }"},{"path":"/reference/assign_taxonomyDada2.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy — assign_taxonomyDada2","title":"Assign taxonomy — assign_taxonomyDada2","text":"Assign taxonomy","code":""},{"path":"/reference/assign_taxonomyDada2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy — assign_taxonomyDada2","text":"","code":"assign_taxonomyDada2(   asv_abund_matrix,   temp_directory_path,   metabarcode = \"metabarcode\",   barcode_params )"},{"path":"/reference/assign_taxonomyDada2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy — assign_taxonomyDada2","text":"asv_abund_matrix final abundance matrix containing amplified sequence variants. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow metabarcode metabarcode used throughout workflow (applicable options: 'rps10', '', 'r16S', 'other1', other2')","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"Filter ASV abundance matrix convert taxmap phyloseq objects","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"convert_asv_matrix_to_objs(   analysis_setup,   min_read_depth = 0,   minimum_bootstrap = 0,   save_outputs = FALSE )"},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function min_read_depth ASV filter parameter. mean read depth across samples less threshold, ASV filtered. minimum_bootstrap Set threshold bootstrap support value taxonomic assignments. designated minimum bootstrap threshold, taxnomoic assignments set N/save_outputs Logical, indicating whether save resulting phyloseq taxmap objects. TRUE, objects saved; FALSE, available global environment. Default FALSE.","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"ASV matrix converted taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"# \\donttest{ # Convert final matrix to taxmap and phyloseq objects for downstream analysis steps analysis_setup <- prepare_reads( data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   overwrite_existing = TRUE ) #> Existing files found in the output directory. Overwriting existing files. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpCSy1OS/demulticoder_run/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Running cutadapt 3.5 for its sequence data #> Read in 299 paired-sequences, output 163 (54.5%) filtered paired-sequences. #> Read in 235 paired-sequences, output 144 (61.3%) filtered paired-sequences.  #> Running cutadapt 3.5 for rps10 sequence data #> Read in 196 paired-sequences, output 145 (74%) filtered paired-sequences. #> Read in 253 paired-sequences, output 182 (71.9%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup, overwrite_existing = TRUE ) #> 80608 total bases in 307 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the  Forward  read of primer pair  its #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 163 reads in 84 unique sequences. #> Sample 2 - 144 reads in 96 unique sequences. #> 82114 total bases in 307 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the  Reverse  read of primer pair  its #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 163 reads in 128 unique sequences. #> Sample 2 - 144 reads in 119 unique sequences. #> 132 paired-reads (in 4 unique pairings) successfully merged out of 132 (in 4 pairings) input. #> 99 paired-reads (in 8 unique pairings) successfully merged out of 99 (in 8 pairings) input.  #> Identified 0 bimeras out of 12 input sequences.  #> 91897 total bases in 327 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the  Forward  read of primer pair  rps10 #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 145 reads in 107 unique sequences. #> Sample 2 - 182 reads in 133 unique sequences. #> 91567 total bases in 327 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the  Reverse  read of primer pair  rps10 #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 145 reads in 114 unique sequences. #> Sample 2 - 182 reads in 170 unique sequences. #> 145 paired-reads (in 2 unique pairings) successfully merged out of 145 (in 2 pairings) input. #> 181 paired-reads (in 3 unique pairings) successfully merged out of 181 (in 3 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/tmp/RtmpCSy1OS/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/RtmpCSy1OS/demulticoder_run/asvabund_matrixDADA2_rps10.RData\" #>  assign_tax( analysis_setup, asv_abund_matrix, retrieve_files=FALSE, overwrite_existing=TRUE ) #> Tracking read counts: #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its   299      163       146       141    132     132 #> 2             S2_its   235      144       113        99     99      99 #> Tracking read counts: #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10   196      145       145       145    145     145 #> 2           S2_rps10   253      182       181       181    181     181 objs<-convert_asv_matrix_to_objs( analysis_setup ) #> Rows: 12 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: /tmp/RtmpCSy1OS/taxmap_obj_its.RData  #> Phyloseq object saved in: /tmp/RtmpCSy1OS/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 5 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: /tmp/RtmpCSy1OS/taxmap_obj_rps10.RData  #> Phyloseq object saved in: /tmp/RtmpCSy1OS/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # }"},{"path":"/reference/countOverlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Count overlap to see how well the reads were merged — countOverlap","title":"Count overlap to see how well the reads were merged — countOverlap","text":"Count overlap see well reads merged","code":""},{"path":"/reference/countOverlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count overlap to see how well the reads were merged — countOverlap","text":"","code":"countOverlap(data_tables, merged_reads, barcode, output_directory_path)"},{"path":"/reference/countOverlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count overlap to see how well the reads were merged — countOverlap","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences merged_reads Intermediate merged read RData file barcode metabarcode used throughout workflow (applicable options: 'rps10', '', 'r16S', 'other1', other2') output_directory_path path directory resulting files output","code":""},{"path":"/reference/countOverlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count overlap to see how well the reads were merged — countOverlap","text":"plot describing well reads merged information overlap reads","code":""},{"path":"/reference/createASVSequenceTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Make ASV sequence matrix — createASVSequenceTable","title":"Make ASV sequence matrix — createASVSequenceTable","text":"Make ASV sequence matrix","code":""},{"path":"/reference/createASVSequenceTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make ASV sequence matrix — createASVSequenceTable","text":"","code":"createASVSequenceTable(merged_reads, orderBy = \"abundance\", barcode_params)"},{"path":"/reference/createASVSequenceTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make ASV sequence matrix — createASVSequenceTable","text":"merged_reads Intermediate merged read RData file orderBy (Optional). character(1). Default \"abundance\". Specifies sequences (columns) returned table ordered (decreasing). Valid values: \"abundance\", \"nsamples\", NULL.","code":""},{"path":"/reference/createASVSequenceTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make ASV sequence matrix — createASVSequenceTable","text":"raw_seqtab","code":""},{"path":"/reference/cut_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Main command to trim primers using 'Cutadapt' and core 'dada2' functions — cut_trim","title":"Main command to trim primers using 'Cutadapt' and core 'dada2' functions — cut_trim","text":"Main command trim primers using 'Cutadapt' core 'dada2' functions","code":""},{"path":"/reference/cut_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main command to trim primers using 'Cutadapt' and core 'dada2' functions — cut_trim","text":"","code":"cut_trim(analysis_setup, cutadapt_path, overwrite_existing = FALSE)"},{"path":"/reference/cut_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main command to trim primers using 'Cutadapt' and core 'dada2' functions — cut_trim","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function cutadapt_path Path 'Cutadapt' program. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/cut_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main command to trim primers using 'Cutadapt' and core 'dada2' functions — cut_trim","text":"Trimmed reads, primer counts, quality plots, ASV matrix.","code":""},{"path":"/reference/cut_trim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Main command to trim primers using 'Cutadapt' and core 'dada2' functions — cut_trim","text":"samples comprised two different metabarcodes (like ITS1 rps10), reads also demultiplexed prior dada2-specific read trimming steps.","code":""},{"path":"/reference/cut_trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main command to trim primers using 'Cutadapt' and core 'dada2' functions — cut_trim","text":"","code":"# \\donttest{ # Remove remaining primers from raw reads, demultiplex pooled barcoded samples, # and then trim reads based on specific 'DADA2' parameters analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   overwrite_existing = TRUE ) #> Existing files found in the output directory. Overwriting existing files. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpCSy1OS/demulticoder_run/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Running cutadapt 3.5 for its sequence data #> Read in 299 paired-sequences, output 163 (54.5%) filtered paired-sequences. #> Read in 235 paired-sequences, output 144 (61.3%) filtered paired-sequences.  #> Running cutadapt 3.5 for rps10 sequence data #> Read in 196 paired-sequences, output 145 (74%) filtered paired-sequences. #> Read in 253 paired-sequences, output 182 (71.9%) filtered paired-sequences.  # }"},{"path":"/reference/filter_and_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for filterAndTrim function from 'dada2', to be used after primer trimming — filter_and_trim","title":"Wrapper function for filterAndTrim function from 'dada2', to be used after primer trimming — filter_and_trim","text":"Wrapper function filterAndTrim function 'dada2', used primer trimming","code":""},{"path":"/reference/filter_and_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for filterAndTrim function from 'dada2', to be used after primer trimming — filter_and_trim","text":"","code":"filter_and_trim(   output_directory_path,   temp_directory_path,   cutadapt_data_barcode,   barcode_params,   barcode )"},{"path":"/reference/filter_and_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for filterAndTrim function from 'dada2', to be used after primer trimming — filter_and_trim","text":"output_directory_path path directory resulting files output cutadapt_data_barcode Metabarcode-specific FASTQ read files trimmed primers","code":""},{"path":"/reference/filter_and_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for filterAndTrim function from 'dada2', to be used after primer trimming — filter_and_trim","text":"Filtered trimmed reads","code":""},{"path":"/reference/format_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Format ASV abundance matrix — format_abund_matrix","title":"Format ASV abundance matrix — format_abund_matrix","text":"Format ASV abundance matrix","code":""},{"path":"/reference/format_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format ASV abundance matrix — format_abund_matrix","text":"","code":"format_abund_matrix(   data_tables,   asv_abund_matrix,   seq_tax_asv,   output_directory_path,   metabarcode )"},{"path":"/reference/format_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format ASV abundance matrix — format_abund_matrix","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences asv_abund_matrix final abundance matrix containing amplified sequence variants seq_tax_asv amplified sequence variants matrix taxonomic information","code":""},{"path":"/reference/format_database.html","id":null,"dir":"Reference","previous_headings":"","what":"General functions to format user-specified databases — format_database","title":"General functions to format user-specified databases — format_database","text":"General functions format user-specified databases","code":""},{"path":"/reference/format_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General functions to format user-specified databases — format_database","text":"","code":"format_database(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   metabarcode,   db_its,   db_rps10,   db_16S,   db_other1,   db_other2 )"},{"path":"/reference/format_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General functions to format user-specified databases — format_database","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow metabarcode metabarcode used throughout workflow (applicable options: 'rps10', '', 'r16S', 'other1', other2')","code":""},{"path":"/reference/format_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General functions to format user-specified databases — format_database","text":"Formatted database(s) specified metabarcode type(s)","code":""},{"path":"/reference/format_db_16S.html","id":null,"dir":"Reference","previous_headings":"","what":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"16S database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_16S.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"","code":"format_db_16S(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_16S )"},{"path":"/reference/format_db_16S.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow db_16S SILVA 16S rRNA reference database provided user","code":""},{"path":"/reference/format_db_16S.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"SILVA 16S rRNA database modified headers","code":""},{"path":"/reference/format_db_its.html","id":null,"dir":"Reference","previous_headings":"","what":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"","code":"format_db_its(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_its )"},{"path":"/reference/format_db_its.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow db_its UNITE reference database provided user","code":""},{"path":"/reference/format_db_its.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"UNITE database modified headers","code":""},{"path":"/reference/format_db_other1.html","id":null,"dir":"Reference","previous_headings":"","what":"An other, user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other1","title":"An other, user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other1","text":", user-specified database initially format specified dada2 header containing taxonomic levels (kingdom species, separated semi-colons)","code":""},{"path":"/reference/format_db_other1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An other, user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other1","text":"","code":"format_db_other1(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other1 )"},{"path":"/reference/format_db_other1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An other, user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other1","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow db_other1 reference database SILVA, UNITE, oomyceteDB (assumes format like SILVA DB entries)","code":""},{"path":"/reference/format_db_other1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An other, user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other1","text":"database modified headers","code":""},{"path":"/reference/format_db_other2.html","id":null,"dir":"Reference","previous_headings":"","what":"An second user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other2","title":"An second user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other2","text":"second user-specified database initially format specified dada2 header containing taxonomic levels (kingdom species, separated semi-colons)","code":""},{"path":"/reference/format_db_other2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An second user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other2","text":"","code":"format_db_other2(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other2 )"},{"path":"/reference/format_db_other2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An second user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other2","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow db_other2 second reference database SILVA, UNITE, oomyceteDB (assumes format like SILVA DB entries)","code":""},{"path":"/reference/format_db_other2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An second user-specified database that is initially in the format specified by dada2 with header containing taxonomic levels (kingdom down to species, separated by semi-colons) — format_db_other2","text":"database modified headers","code":""},{"path":"/reference/format_db_rps10.html","id":null,"dir":"Reference","previous_headings":"","what":"Create modified reference rps10 database for downstream analysis — format_db_rps10","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"Create modified reference rps10 database downstream analysis","code":""},{"path":"/reference/format_db_rps10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"","code":"format_db_rps10(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_rps10 )"},{"path":"/reference/format_db_rps10.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow db_rps10 oomyceteDB rps10 reference database provided user","code":""},{"path":"/reference/format_db_rps10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"oomyceteDB database modified headers","code":""},{"path":"/reference/get_fastq_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"Retrieve paths filtered trimmed Fastq files","code":""},{"path":"/reference/get_fastq_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"","code":"get_fastq_paths(data_tables, my_direction, my_primer_pair_id)"},{"path":"/reference/get_fastq_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences my_direction Whether primer forward reverse direction my_primer_pair_id specific metabarcode ID","code":""},{"path":"/reference/get_pids.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"Align ASV sequences reference sequences database get percent ID. Get percent identities.","code":""},{"path":"/reference/get_pids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"","code":"get_pids(tax_results, temp_directory_path, output_directory_path, db, locus)"},{"path":"/reference/get_pids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"tax_results data frame containing taxonomic assignments","code":""},{"path":"/reference/get_post_trim_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_post_trim_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"","code":"get_post_trim_hits(primer_data, cutadapt_data, output_directory_path)"},{"path":"/reference/get_post_trim_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"primer_data Primer data.frame created using orient_primers function parse information forward reverse primer sequences. cutadapt_data FASTQ read files trimmed primers output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_post_trim_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Table read counts across sample","code":""},{"path":"/reference/get_pre_primer_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"","code":"get_pre_primer_hits(primer_data, fastq_data, output_directory_path)"},{"path":"/reference/get_pre_primer_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"primer_data Primer data.frame created using orient_primers function parse information forward reverse primer sequences. fastq_data data.frame containing read file paths direction reads sample output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"number reads primer found number reads primer found","code":""},{"path":"/reference/get_read_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"Final inventory read counts step input removal chimeras. function deals one sample. TODO optimize one sample","code":""},{"path":"/reference/get_read_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"","code":"get_read_counts(   asv_abund_matrix,   temp_directory_path,   output_directory_path,   metabarcode )"},{"path":"/reference/get_read_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"asv_abund_matrix final abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/get_ref_seq.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"Align ASV sequences reference sequences database get percent ID. Start retrieving reference sequences.","code":""},{"path":"/reference/get_ref_seq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"","code":"get_ref_seq(tax_results, db)"},{"path":"/reference/get_ref_seq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"tax_results dataframe containing taxonomic assignments db reference database","code":""},{"path":"/reference/infer_asv_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to infer ASVs, for multiple loci — infer_asv_command","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"Function infer ASVs, multiple loci","code":""},{"path":"/reference/infer_asv_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"","code":"infer_asv_command(   output_directory_path,   temp_directory_path,   data_tables,   barcode_params,   barcode )"},{"path":"/reference/infer_asv_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"output_directory_path path directory resulting files output data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences","code":""},{"path":"/reference/infer_asvs.html","id":null,"dir":"Reference","previous_headings":"","what":"Core dada2 function to learn errors and infer ASVs — infer_asvs","title":"Core dada2 function to learn errors and infer ASVs — infer_asvs","text":"Core dada2 function learn errors infer ASVs","code":""},{"path":"/reference/infer_asvs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core dada2 function to learn errors and infer ASVs — infer_asvs","text":"","code":"infer_asvs(   data_tables,   my_direction,   my_primer_pair_id,   barcode_params,   output_directory_path )"},{"path":"/reference/infer_asvs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core dada2 function to learn errors and infer ASVs — infer_asvs","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences my_direction Whether primer forward reverse direction my_primer_pair_id specific metabarcode ID output_directory_path path directory resulting files output","code":""},{"path":"/reference/infer_asvs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core dada2 function to learn errors and infer ASVs — infer_asvs","text":"asv_data","code":""},{"path":"/reference/make_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"Quality filtering remove chimeras short sequences","code":""},{"path":"/reference/make_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"","code":"make_abund_matrix(   raw_seqtab,   temp_directory_path,   barcode_params = barcode_params,   barcode )"},{"path":"/reference/make_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"raw_seqtab RData file containing intermediate read data chimeras removed","code":""},{"path":"/reference/make_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"Make amplified sequence variant (ASV) abundance matrix input barcodes","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"","code":"make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE)"},{"path":"/reference/make_asv_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"ASV abundance matrix (asv_abund_matrix)","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"function processes data unique barcode separately, inferring ASVs, merging reads, creating ASV abundance matrix. , DADA2 core denoising alogrithm used infer ASVs.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"","code":"# \\donttest{ # The primary wrapper function for DADA2 ASV inference steps analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   overwrite_existing = TRUE ) #> Existing files found in the output directory. Overwriting existing files. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpCSy1OS/demulticoder_run/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Running cutadapt 3.5 for its sequence data #> Read in 299 paired-sequences, output 163 (54.5%) filtered paired-sequences. #> Read in 235 paired-sequences, output 144 (61.3%) filtered paired-sequences.  #> Running cutadapt 3.5 for rps10 sequence data #> Read in 196 paired-sequences, output 145 (74%) filtered paired-sequences. #> Read in 253 paired-sequences, output 182 (71.9%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup, overwrite_existing = TRUE ) #> 80608 total bases in 307 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the  Forward  read of primer pair  its #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 163 reads in 84 unique sequences. #> Sample 2 - 144 reads in 96 unique sequences. #> 82114 total bases in 307 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the  Reverse  read of primer pair  its #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 163 reads in 128 unique sequences. #> Sample 2 - 144 reads in 119 unique sequences. #> 132 paired-reads (in 4 unique pairings) successfully merged out of 132 (in 4 pairings) input. #> 99 paired-reads (in 8 unique pairings) successfully merged out of 99 (in 8 pairings) input.  #> Identified 0 bimeras out of 12 input sequences.  #> 91897 total bases in 327 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the  Forward  read of primer pair  rps10 #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 145 reads in 107 unique sequences. #> Sample 2 - 182 reads in 133 unique sequences. #> 91567 total bases in 327 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the  Reverse  read of primer pair  rps10 #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 145 reads in 114 unique sequences. #> Sample 2 - 182 reads in 170 unique sequences. #> 145 paired-reads (in 2 unique pairings) successfully merged out of 145 (in 2 pairings) input. #> 181 paired-reads (in 3 unique pairings) successfully merged out of 181 (in 3 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/tmp/RtmpCSy1OS/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/RtmpCSy1OS/demulticoder_run/asvabund_matrixDADA2_rps10.RData\" #>  # }"},{"path":"/reference/make_cutadapt_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare for primmer trimming with 'Cutadapt'. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","title":"Prepare for primmer trimming with 'Cutadapt'. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Prepare primmer trimming 'Cutadapt'. Make new sub-directories specify paths trimmed untrimmed reads","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare for primmer trimming with 'Cutadapt'. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"","code":"make_cutadapt_tibble(fastq_data, metadata_primer_data, temp_directory_path)"},{"path":"/reference/make_cutadapt_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare for primmer trimming with 'Cutadapt'. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"fastq_data data.frame containing read file paths direction reads sample metadata_primer_data data.frame combining metadata primer data temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare for primmer trimming with 'Cutadapt'. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Returns larger data.frame containing paths temporary read directories, used input running 'Cutadapt'","code":""},{"path":"/reference/make_seqhist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"Plots histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/make_seqhist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"","code":"make_seqhist(asv_abund_matrix, output_directory_path)"},{"path":"/reference/make_seqhist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"asv_abund_matrix final abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/make_seqhist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/merge_reads_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge forward and reverse reads — merge_reads_command","title":"Merge forward and reverse reads — merge_reads_command","text":"Merge forward reverse reads","code":""},{"path":"/reference/merge_reads_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge forward and reverse reads — merge_reads_command","text":"","code":"merge_reads_command(   output_directory_path,   temp_directory_path,   barcode_params,   barcode )"},{"path":"/reference/merge_reads_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge forward and reverse reads — merge_reads_command","text":"output_directory_path path directory resulting files output","code":""},{"path":"/reference/merge_reads_command.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge forward and reverse reads — merge_reads_command","text":"merged_reads Intermediate merged read RData file","code":""},{"path":"/reference/orient_primers.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data.frame — orient_primers","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data.frame — orient_primers","text":"Take user's forward reverse sequences creates complement, reverse, reverse complement primers one data.frame","code":""},{"path":"/reference/orient_primers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data.frame — orient_primers","text":"","code":"orient_primers(primers_params_path)"},{"path":"/reference/orient_primers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data.frame — orient_primers","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/orient_primers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data.frame — orient_primers","text":"data.frame oriented primer information.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Wrapper script plotQualityProfile trim steps primer removal.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"","code":"plot_post_trim_qc(   cutadapt_data,   output_directory_path,   n = 5e+05,   barcode_params )"},{"path":"/reference/plot_post_trim_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"cutadapt_data FASTQ read files trimmed primers output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Quality profiles reads primer trimming","code":""},{"path":"/reference/plot_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for plotQualityProfile function — plot_qc","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Wrapper function plotQualityProfile function","code":""},{"path":"/reference/plot_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"","code":"plot_qc(cutadapt_data, output_directory_path, n = 5e+05, barcode_params)"},{"path":"/reference/plot_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"cutadapt_data FASTQ read files trimmed primers output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"'dada2' wrapper function making quality profiles sample","code":""},{"path":"/reference/prep_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare final ASV abundance matrix — prep_abund_matrix","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"Prepare final ASV abundance matrix","code":""},{"path":"/reference/prep_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"","code":"prep_abund_matrix(cutadapt_data, asv_abund_matrix, data_tables, metabarcode)"},{"path":"/reference/prep_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"asv_abund_matrix final abundance matrix containing amplified sequence variants metabarcode metabarcode used throughout workflow (applicable options: 'rps10', '', 'r16S', 'other1', other2')","code":""},{"path":"/reference/prepare_metadata_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"Read metadata file user combine reformat , given primer data. Included larger function prepare_reads.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"","code":"prepare_metadata_table(metadata_file_path, primer_data)"},{"path":"/reference/prepare_metadata_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"metadata_file_path path metadata file. primer_data Primer data.frame created using orient_primers function parse information forward reverse primer sequences.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"data.frame containing merged metadata primer data called metadata_primer_data.","code":""},{"path":"/reference/prepare_reads.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare reads for primer trimming using 'Cutadapt' — prepare_reads","title":"Prepare reads for primer trimming using 'Cutadapt' — prepare_reads","text":"Prepare reads primer trimming using 'Cutadapt'","code":""},{"path":"/reference/prepare_reads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare reads for primer trimming using 'Cutadapt' — prepare_reads","text":"","code":"prepare_reads(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\",   overwrite_existing = FALSE )"},{"path":"/reference/prepare_reads.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare reads for primer trimming using 'Cutadapt' — prepare_reads","text":"data_directory Directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. user can provide helpful ID, whether date specific name run. Default \"demulticoder_run\" overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/prepare_reads.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare reads for primer trimming using 'Cutadapt' — prepare_reads","text":"list containing data tables, including metadata, primer sequences search based orientation, paths trimming reads, user-defined parameters subsequent steps.","code":""},{"path":"/reference/prepare_reads.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare reads for primer trimming using 'Cutadapt' — prepare_reads","text":"","code":"# \\donttest{ # Pre-filter raw reads and parse metadata and primer_information to prepare # for primer trimming and filter analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   overwrite_existing = TRUE ) #> Existing files found in the output directory. Overwriting existing files. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 25 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (18): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpCSy1OS/demulticoder_run/prefiltered_sequences  # }"},{"path":"/reference/primer_check.html","id":null,"dir":"Reference","previous_headings":"","what":"Matching Order Primer Check — primer_check","title":"Matching Order Primer Check — primer_check","text":"Matching Order Primer Check","code":""},{"path":"/reference/primer_check.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matching Order Primer Check — primer_check","text":"","code":"primer_check(fastq_data)"},{"path":"/reference/primer_check.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matching Order Primer Check — primer_check","text":"fastq_data data.frame containing read file paths direction reads sample","code":""},{"path":"/reference/primer_check.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matching Order Primer Check — primer_check","text":"None","code":""},{"path":"/reference/process_single_barcode.html","id":null,"dir":"Reference","previous_headings":"","what":"Run 'dada2' taxonomy functions for single metabarcode — process_single_barcode","title":"Run 'dada2' taxonomy functions for single metabarcode — process_single_barcode","text":"Run 'dada2' taxonomy functions single metabarcode","code":""},{"path":"/reference/process_single_barcode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run 'dada2' taxonomy functions for single metabarcode — process_single_barcode","text":"","code":"process_single_barcode(   data_tables,   temp_directory_path,   output_directory_path,   asv_abund_matrix,   metabarcode = metabarcode,   barcode_params )"},{"path":"/reference/process_single_barcode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run 'dada2' taxonomy functions for single metabarcode — process_single_barcode","text":"data_tables data tables containing paths read files, metadata, metabarcode information associated primer sequences asv_abund_matrix final abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/read_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"Takes in the FASTQ files from the user and creates a data.frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","title":"Takes in the FASTQ files from the user and creates a data.frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"Takes FASTQ files user creates data.frame paths files created used future. Included larger 'read_prefilt_fastq' function.","code":""},{"path":"/reference/read_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Takes in the FASTQ files from the user and creates a data.frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"","code":"read_fastq(data_directory_path, temp_directory_path)"},{"path":"/reference/read_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Takes in the FASTQ files from the user and creates a data.frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data_directory_path path directory containing raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow.","code":""},{"path":"/reference/read_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Takes in the FASTQ files from the user and creates a data.frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data.frame FASTQ file paths, primer orientations sequences, parsed sample names.","code":""},{"path":"/reference/read_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"Take user's DADA2 parameters make dataframe downstream steps","code":""},{"path":"/reference/read_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"","code":"read_parameters(primers_params_path)"},{"path":"/reference/read_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/read_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"data frame information DADA2 parameters.","code":""},{"path":"/reference/read_parameters_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's 'dada2' parameters and make a dataframe for downstream steps — read_parameters_table","title":"Take in user's 'dada2' parameters and make a dataframe for downstream steps — read_parameters_table","text":"Take user's 'dada2' parameters make dataframe downstream steps","code":""},{"path":"/reference/read_parameters_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's 'dada2' parameters and make a dataframe for downstream steps — read_parameters_table","text":"","code":"read_parameters_table(primers_params_path)"},{"path":"/reference/read_parameters_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's 'dada2' parameters and make a dataframe for downstream steps — read_parameters_table","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/read_parameters_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's 'dada2' parameters and make a dataframe for downstream steps — read_parameters_table","text":"data.frame information 'dada2' parameters.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with 'Cutadapt'. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with 'Cutadapt'. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"function calling read_fastq, primer_check, remove_ns functions. process edit FASTQ make ready trimming primers 'Cutadapt'. Part larger 'prepare_reads' function.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with 'Cutadapt'. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"","code":"read_prefilt_fastq(   data_directory_path = data_directory_path,   multithread,   temp_directory_path )"},{"path":"/reference/read_prefilt_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with 'Cutadapt'. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"data_directory_path path directory containing raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with 'Cutadapt'. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"Returns filtered reads Ns","code":""},{"path":"/reference/remove_ns.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for core 'dada2' filter and trim function for first filtering step — remove_ns","title":"Wrapper function for core 'dada2' filter and trim function for first filtering step — remove_ns","text":"Wrapper function core 'dada2' filter trim function first filtering step","code":""},{"path":"/reference/remove_ns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for core 'dada2' filter and trim function for first filtering step — remove_ns","text":"","code":"remove_ns(fastq_data, multithread, temp_directory_path)"},{"path":"/reference/remove_ns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for core 'dada2' filter and trim function for first filtering step — remove_ns","text":"fastq_data data.frame containing read file paths direction reads sample multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/remove_ns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for core 'dada2' filter and trim function for first filtering step — remove_ns","text":"Return prefiltered reads Ns","code":""},{"path":"/reference/run_cutadapt.html","id":null,"dir":"Reference","previous_headings":"","what":"Core function for running 'Cutadapt' — run_cutadapt","title":"Core function for running 'Cutadapt' — run_cutadapt","text":"Core function running 'Cutadapt'","code":""},{"path":"/reference/run_cutadapt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core function for running 'Cutadapt' — run_cutadapt","text":"","code":"run_cutadapt(   cutadapt_path,   cutadapt_data_barcode,   barcode_params,   minCutadaptlength )"},{"path":"/reference/run_cutadapt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core function for running 'Cutadapt' — run_cutadapt","text":"cutadapt_path path 'Cutadapt' program. minCutadaptlength Read lengths lower threshold discarded. Default 0.","code":""},{"path":"/reference/run_cutadapt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core function for running 'Cutadapt' — run_cutadapt","text":"Trimmed read.","code":""},{"path":"/reference/setup_directories.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up directory paths for subsequent analyses — setup_directories","title":"Set up directory paths for subsequent analyses — setup_directories","text":"function sets directory paths subsequent analyses. checks whether specified output directories exist creates . function also provides paths primer metadata files within data directory.","code":""},{"path":"/reference/setup_directories.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up directory paths for subsequent analyses — setup_directories","text":"","code":"setup_directories(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\" )"},{"path":"/reference/setup_directories.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up directory paths for subsequent analyses — setup_directories","text":"data_directory Directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory path outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. user can provide helpful ID, whether date specific name run. Default \"demulticoder_run\".","code":""},{"path":"/reference/setup_directories.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up directory paths for subsequent analyses — setup_directories","text":"list paths data, output, temporary directories, primer, metadata files.","code":""},{"path":"/news/index.html","id":"demulticoder-011","dir":"Changelog","previous_headings":"","what":"demulticoder 0.1.1","title":"demulticoder 0.1.1","text":"Minor clean-documentation describing key functions example vignettes Initial CRAN submission","code":""}]
