[{"path":"/articles/DADA2_16S_mothur_validation.html","id":"demonstration-of-how-to-use-demulticoder-on-a-dataset-that-is-actually-three-separate-datasets-rps10-its-and-16s-at-once","dir":"Articles","previous_headings":"","what":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","title":"16S Mothur SOP Validation","text":"Input metadata primerinfo_params files data folder required columns first sample names, second primer name/barcode used. subsequent columns user-specific columns downstream steps metadata.csv file included necessary second file name barcode selected, primer sequences, optional DADA2 parameter options. referenced DADA2 tutorial select proper parameter options. Note, primers already trimmed reads, just certain, included Earth Microbiome primers described , primer sequences still found within small number reads. primerinfo_params.csv","code":""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-1-remove-ns-and-create-directory-structure-for-downstream-steps","dir":"Articles","previous_headings":"","what":"Step 1-Remove N’s and create directory structure for downstream steps","title":"16S Mothur SOP Validation","text":"","code":"options(mc.cores = 1)  outputs<-prepare_reads(   data_directory = \"~/benchmark_demulticoder/mothur_16S_sop/data\",    output_directory = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs\",   tempdir_path = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp\",   tempdir_id = \"temp_files\",   overwrite_existing = FALSE) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE. #> Rows: 1 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 1 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 20 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, When #> dbl (1): Day #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Warning in get_read_names(paired_file_paths[1]) == #> get_read_names(paired_file_paths[2]): longer object length is not a multiple of #> shorter object length #> Rows: 8 Columns: 43 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, orientation, sequence #> dbl (40): F3D0_R1, F3D0_R2, F3D1_R1, F3D1_R2, F3D141_R1, F3D141_R2, F3D142_R... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-2-run-cutadapt-to-remove-primers-and-then-trim-reads-with-dada2-filterandtrim-function","dir":"Articles","previous_headings":"","what":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","title":"16S Mothur SOP Validation","text":"","code":"cut_trim(   outputs,   cutadapt_path=\"/opt/homebrew/bin/cutadapt\",   overwrite_existing = FALSE) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-3-core-asv-inference-step","dir":"Articles","previous_headings":"","what":"Step 3-Core ASV inference step","title":"16S Mothur SOP Validation","text":"","code":"make_asv_abund_matrix(   outputs,   overwrite_existing = FALSE) #> Existing files found. The following ASV abundance matrices are saved in the tempdir path: #> /Users/masudermann/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/asvabund_matrixDADA2_r16S.RData #> [[1]] #> NULL"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-4-assign-taxonomy-step","dir":"Articles","previous_headings":"","what":"Step 4-Assign taxonomy step","title":"16S Mothur SOP Validation","text":"","code":"assign_tax(   outputs,   asv_abund_matrix,   db_16S=\"silva_nr99_v138.2_toSpecies_trainset.fa.gz\",   retrieve_files=FALSE,   overwrite_existing=FALSE) #> Existing files found. Specify overwrite=TRUE, to rerun analysis"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-5-convert-asv-matrix-to-taxmap-and-phyloseq-objects-with-one-function","dir":"Articles","previous_headings":"","what":"Step 5-convert asv matrix to taxmap and phyloseq objects with one function","title":"16S Mothur SOP Validation","text":"","code":"objs<-convert_asv_matrix_to_objs(outputs) #> Rows: 232 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (20): F3D0_r16S, F3D1_r16S, F3D141_r16S, F3D142_r16S, F3D143_r16S, F3D14... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For r16S dataset  #> Taxmap object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/taxmap_obj_r16S.RData  #> Phyloseq object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/phylo_obj_r16S.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-6-evaluate-accuracy-using-mock-community-as-shown-in-dada2-tutorial","dir":"Articles","previous_headings":"","what":"Step 6-evaluate accuracy using mock community, as shown in dada2 tutorial","title":"16S Mothur SOP Validation","text":"looking mock community sample, able extract 20 bacterial sequences 0% mismatch, matched described previously.","code":"track_reads_demulticoder<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/track_reads_r16S.csv\", row.names = 1) summary(track_reads_demulticoder) #>      input          filtered       denoisedF       denoisedR     #>  Min.   : 3158   Min.   : 2914   Min.   : 2799   Min.   : 2830   #>  1st Qu.: 4944   1st Qu.: 4498   1st Qu.: 4409   1st Qu.: 4424   #>  Median : 5878   Median : 5381   Median : 5278   Median : 5298   #>  Mean   : 7571   Mean   : 6982   Mean   : 6852   Mean   : 6890   #>  3rd Qu.: 7783   3rd Qu.: 7176   3rd Qu.: 7040   3rd Qu.: 7058   #>  Max.   :19489   Max.   :18075   Max.   :17907   Max.   :17939   #>      merged         nonchim      #>  Min.   : 2553   Min.   : 2519   #>  1st Qu.: 4194   1st Qu.: 4132   #>  Median : 5006   Median : 4940   #>  Mean   : 6444   Mean   : 6212   #>  3rd Qu.: 6621   3rd Qu.: 6566   #>  Max.   :17431   Max.   :16835  tax_matrix<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/final_asv_abundance_matrix_r16S.csv\")  unqs.mock <- tax_matrix[, c(2, which(colnames(tax_matrix) == \"Mock_r16S\"))]  unqs.mock <- unqs.mock[unqs.mock$Mock_r16S != 0,]  cat(\"DADA2 inferred\", nrow(unqs.mock), \"sample sequences present in the Mock community.\\n\") #> DADA2 inferred 20 sample sequences present in the Mock community.  mock.ref <- dada2::getSequences(file.path(\"~/benchmark_demulticoder/mothur_16S_sop/data\", \"HMP_MOCK.v35.fasta\")) match.ref <- sum(sapply(unqs.mock$sequence, function(x) any(grepl(x, mock.ref)))) cat(\"Of those,\", sum(match.ref), \"were exact matches to the expected reference sequences.\\n\") #> Of those, 20 were exact matches to the expected reference sequences."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-7-follow-up-work-using-phyloseq-to-do-side-by-side-comparison-with-dada2-example-and-to-examine-alpha-diversity-results","dir":"Articles","previous_headings":"","what":"Step 7-Follow-up work using phyloseq to do side-by-side comparison with dada2 example and to examine alpha diversity results","title":"16S Mothur SOP Validation","text":"","code":"objs$phyloseq_r16S <- phyloseq::prune_samples(phyloseq::sample_names(objs$phyloseq_r16S) != \"Mock_r16S\", objs$phyloseq_r16S) # Remove mock sample phyloseq::plot_richness(objs$phyloseq_r16S, x=\"Day\", measures=c(\"Shannon\", \"Simpson\"), color=\"When\") #> Warning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have #> any singletons. This is highly suspicious. Results of richness #> estimates (for example) are probably unreliable, or wrong, if you have already #> trimmed low-abundance taxa from the data. #>  #> We recommended that you find the un-trimmed data and retry."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-8-examine-ordination-plots-as-additional-point-of-comparison-with-dada2-tutorial","dir":"Articles","previous_headings":"","what":"Step 8-Examine ordination plots as additional point of comparison with DADA2 tutorial","title":"16S Mothur SOP Validation","text":"","code":"# Transform data to proportions as appropriate for Bray-Curtis distances ps.prop <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(otu) otu/sum(otu)) ord.nmds.bray <- phyloseq::ordinate(ps.prop, method=\"NMDS\", distance=\"bray\") #> Run 0 stress 0.0808378  #> Run 1 stress 0.1334385  #> Run 2 stress 0.0809639  #> ... Procrustes: rmse 0.009219366  max resid 0.02824555  #> Run 3 stress 0.09466618  #> Run 4 stress 0.3744204  #> Run 5 stress 0.09061366  #> Run 6 stress 0.0808378  #> ... Procrustes: rmse 2.017452e-06  max resid 4.726435e-06  #> ... Similar to previous best #> Run 7 stress 0.08635462  #> Run 8 stress 0.0808378  #> ... Procrustes: rmse 6.420168e-06  max resid 1.573833e-05  #> ... Similar to previous best #> Run 9 stress 0.08096389  #> ... Procrustes: rmse 0.00914913  max resid 0.02801434  #> Run 10 stress 0.09061366  #> Run 11 stress 0.09061366  #> Run 12 stress 0.08096392  #> ... Procrustes: rmse 0.009240612  max resid 0.02831497  #> Run 13 stress 0.08635462  #> Run 14 stress 0.08635462  #> Run 15 stress 0.0808378  #> ... Procrustes: rmse 1.076163e-06  max resid 3.172951e-06  #> ... Similar to previous best #> Run 16 stress 0.09466618  #> Run 17 stress 0.08635462  #> Run 18 stress 0.0809639  #> ... Procrustes: rmse 0.009215314  max resid 0.02823148  #> Run 19 stress 0.0809639  #> ... Procrustes: rmse 0.009213734  max resid 0.02822652  #> Run 20 stress 0.09466621  #> *** Best solution repeated 3 times phyloseq::plot_ordination(ps.prop, ord.nmds.bray, color=\"When\", title=\"Bray NMDS\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-9-lets-look-at-what-the-top-20-taxa-are-in-the-early-vs--late-samples-time-points-as-shown-in-the-dada2-tutorial","dir":"Articles","previous_headings":"","what":"Step 9-Let’s look at what the top 20 taxa are in the early vs. late samples time points, as shown in the dada2 tutorial","title":"16S Mothur SOP Validation","text":"","code":"top20 <- names(sort(phyloseq::taxa_sums(objs$phyloseq_r16S), decreasing=TRUE))[1:20] ps.top20 <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(OTU) OTU/sum(OTU)) ps.top20 <- phyloseq::prune_taxa(top20, ps.top20) phyloseq::plot_bar(ps.top20, x=\"Day\", fill=\"Family\") + ggplot2::facet_wrap(~When, scales=\"free_x\")"},{"path":"/articles/Documentation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Documentation","text":"documentation provides comprehensive information use Demulticoder R package processing analyzing metabarcode sequencing data. covers input file requirements, parameter settings, key parameters.","code":""},{"path":"/articles/Documentation.html","id":"quick-start-guide","dir":"Articles","previous_headings":"","what":"Quick Start Guide","title":"Documentation","text":"Prepare input files (metadata.csv, primerinfo_params.csv, unformatted reference databases, PE Illumina read files). Place input files single directory. Ensure file names comply specified format. Run pipeline default settings adjust parameters needed.","code":""},{"path":[]},{"path":"/articles/Documentation.html","id":"directory-structure","dir":"Articles","previous_headings":"Input Files","what":"Directory Structure","title":"Documentation","text":"Place input files single directory. directory contain following files: metadata.csv primerinfo_params.csv PE Illumina read files Unformatted reference databases","code":""},{"path":[]},{"path":"/articles/Documentation.html","id":"read-name-format","dir":"Articles","previous_headings":"Input Files > File Naming Conventions","what":"Read Name Format","title":"Documentation","text":"avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz","code":""},{"path":"/articles/Documentation.html","id":"metadata-csv","dir":"Articles","previous_headings":"Input Files","what":"metadata.csv","title":"Documentation","text":"metadata.csv file contains information samples primers (associated barcodes) used experiment. following two required columns: sample_name: Identifier sample (e.g., S1, S2) primer_name: Name primer used (e.g., rps10, , r16S, other1, oteher2) Please add associated metadata file two required columns. can used downstream exploratory diversity analyses, sample data incorporated final phyloseq taxmap objects. Example file (optional third column):","code":"sample_name,primer_name,organism S1,rps10,Cry S2,rps10,Cin S1,its,Cry S2,its,Cin"},{"path":"/articles/Documentation.html","id":"primerinfo_params-csv","dir":"Articles","previous_headings":"Input Files","what":"primerinfo_params.csv","title":"Documentation","text":"primerinfo_params.csv file contains information primer sequences used experiment, along optional additional parameters part DADA2 pipeline. anything specified, default values used. Required columns: primer_name: Name primer/barcode (e.g., , rps10) forward: Forward primer sequence reverse: Reverse primer sequence DADA2 filterAndTrim function parameters: already_trimmed: Boolean indicating primers already trimmed (TRUE/FALSE) (default: FALSE) minCutadaptlength: Cutadapt parameter-Minimum length Cutadapt trimming (default: 0) multithread: Boolean multithreading (TRUE/FALSE) (default: FALSE) verbose: Boolean verbose output (TRUE/FALSE) (default: FALSE) maxN: Maximum number N bases allowed (default: 0) maxEE_forward: Maximum expected errors forward reads (default: Inf) maxEE_reverse: Maximum expected errors reverse reads (default: Inf) truncLen_forward: Truncation length forward reads (default: 0) truncLen_reverse: Truncation length reverse reads (default: 0) truncQ: Truncation quality threshold (default: 2) minLen: Minimum length reads processing (default: 20) maxLen: Maximum length reads processing (default: Inf) minQ: Minimum quality score (default: 0) trimLeft: Number bases trim start reads (default: 0) trimRight: Number bases trim end reads (default: 0) rm.lowcomplex: Boolean removing low complexity sequences (default: TRUE) DADA2 learnErrors function parameters: nbases: Number bases use error rate learning (default: 1e+08) randomize: Randomize reads error rate learning (default: FALSE) MAX_CONSIST: Maximum number self-consistency iterations (default: 10) OMEGA_C: Convergence threshold error rates (default: 0) qualityType: Quality score type (“Auto”, “FastqQuality”, “ShortRead”) (default: “Auto”) DADA2 plot errors parameters: err_out: Return error rates used inference (default: TRUE) err_in: Use input error rates instead learning (default: FALSE) nominalQ: Use nominal Q-scores (default: FALSE) obs: Return observed error rates (default: TRUE) DADA2 dada function parameters: OMP: Use OpenMP multi-threading available (default: TRUE) n: Number reads use error rate estimation (default: 1e+05) id.sep: Character separating sample ID sequence name (default: “\\s”) orient.fwd: NULL TRUE/FALSE orient sequences (default: NULL) pool: Pool samples error rate estimation (default: FALSE) selfConsist: Perform self-consistency iterations (default: FALSE) DADA2 mergePairs function parameters: minOverlap: Minimum overlap merging paired-end reads (default: 12) maxMismatch: Maximum mismatches allowed overlap region (default: 0) DADA2 removeBimeraDenovo function parameters: method: Method sample inference (“consensus” “pooled”) (default: “consensus”) parameters include CSV input file: min_asv_length: Minimum length Amplicon Sequence Variants (ASVs) core dada ASV inference steps (default=0) Example file (select optional columns forward reverse primer sequence columns):","code":"primer_name,forward,reverse,already_trimmed,minCutadaptlength,multithread,verbose,maxN,maxEE_forward,maxEE_reverse,truncLen_forward,truncLen_reverse,truncQ,minLen,maxLen,minQ,trimLeft,trimRight,rm.lowcomplex,minOverlap,maxMismatch,min_asv_length rps10,GTTGGTTAGAGYARAAGACT,ATRYYTAGAAAGAYTYGAACT,FALSE,100,TRUE,FALSE,1.00E+05,5,5,0,0,5,150,Inf,0,0,0,0,15,0,50 its,CTTGGTCATTTAGAGGAAGTAA,GCTGCGTTCTTCATCGATGC,FALSE,50,TRUE,FALSE,1.00E+05,5,5,0,0,5,50,Inf,0,0,0,0,15,0,50"},{"path":"/articles/Documentation.html","id":"reference-database","dir":"Articles","previous_headings":"Input Files","what":"Reference Database","title":"Documentation","text":"Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function. now, package compatible following databases: oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ UNITE fungal database https://unite.ut.ee/repository.php two reference databases. user need reformat headers exactly outlined DADA2 database format, similar SILVA database format. user can specify path database input file. database fasta format.","code":""},{"path":"/articles/Documentation.html","id":"faq","dir":"Articles","previous_headings":"","what":"FAQ","title":"Documentation","text":"progress","code":""},{"path":"/articles/Documentation.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Documentation","text":"progress","code":""},{"path":"/articles/Getting_started.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before You Start","title":"Getting Started","text":"following example, demonstrate key package functionality using subset reads two samples containing pooled ITS1 fungal rps10 oomycete amplicons. databases used assign taxonomy step also abridged versions full UNITE oomyceteDB databases. can follow along test data associated CSV input files loaded package. Additional examples also available website. Please note, speed, test dataset comprised randomly subset reads samples (S1 S2), due database size, full UNITE database included package, also smaller subset larger database. need prepare raw read files fill metadata.csv primerinfo_params.csv templates.","code":""},{"path":"/articles/Getting_started.html","id":"format-of-the-pe-amplicon-files","dir":"Articles","previous_headings":"","what":"Format of the PE amplicon files","title":"Getting Started","text":"package takes forward reverse Illumina short read sequence data. avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz.","code":""},{"path":"/articles/Getting_started.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"","what":"Format of metadata file (metadata.csv)","title":"Getting Started","text":"format CSV file simple. template . two necessary columns (names) : sample_name column primer_info column additional metadata pasted two columns. can referenced later analysis steps save step loading metadata later. S1 S2 come rhododendron rhizobiome dataset random subset reads. notice S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled reads (rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. Example using test dataset:","code":""},{"path":"/articles/Getting_started.html","id":"format-of-primer-and-parameters-file-primerinfo_parms-csv","dir":"Articles","previous_headings":"","what":"Format of primer and parameters file (primerinfo_parms.csv)","title":"Getting Started","text":"DADA2 Primer sequence information user-defined parameters placed primerinfo_params.csv. simplify functions called, user provide parameters within input file. recommend using template linked . Remember add additional optional DADA2 parameters want use. required columns user must fill : 1.primer_name compatible options currently (add cells written crucial database formatting step): rps10, , r16S, other1, other2 2.forward-forward sequence 3.reverse-reverse sequence Example template ‘primerinfo_params.csv’ info parameter specifics, see Documentation tab.","code":""},{"path":"/articles/Getting_started.html","id":"reference-database-format","dir":"Articles","previous_headings":"","what":"Reference Database Format","title":"Getting Started","text":"now, package compatible following databases: oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ UNITE fungal database https://unite.ut.ee/repository.php user can select two databases, first need reformat headers exactly like SILVA database. See ‘Documentation’ tab. Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function","code":""},{"path":"/articles/Getting_started.html","id":"additional-notes","dir":"Articles","previous_headings":"","what":"Additional Notes","title":"Getting Started","text":"Computer specifications may limiting factor. using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases comprised randomly subset reads. following example, run personal computer least 16 GB RAM. Users need upload databases input data folder. computer crashes taxonomic assignment step, need switch computer sufficient memory. Please also ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/Getting_started.html","id":"loading-the-package","dir":"Articles","previous_headings":"","what":"Loading the Package","title":"Getting Started","text":"now, package loaded retrieving GitHub. Eventually, package uploaded CRAN Bioconductor.","code":"#devtools::install_github(\"grunwaldlab/demulticoder\") devtools::load_all(\"~/demulticoder\") library(\"demulticoder\") library(\"metacoder\") library(\"dplyr\")"},{"path":"/articles/Getting_started.html","id":"reorganize-data-tables-and-set-up-data-directory-structure","dir":"Articles","previous_headings":"","what":"Reorganize data tables and set-up data directory structure","title":"Getting Started","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    tempdir_path = \"~/temp_test_dataset\",   tempdir_id = \"test_dataset\",   overwrite_existing = TRUE)"},{"path":"/articles/Getting_started.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"","what":"Remove primers with Cutadapt","title":"Getting Started","text":"running Cutadapt, please ensure installed .","code":"demulticoder::cut_trim(   analysis_setup,   cutadapt_path = \"/opt/homebrew/bin/cutadapt\",   #cutadapt_path = \"/usr/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 4.1 for its sequence data #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 4.1 for rps10 sequence data #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences."},{"path":"/articles/Getting_started.html","id":"asv-inference-step","dir":"Articles","previous_headings":"","what":"ASV inference step","title":"Getting Started","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = TRUE) #> 710847 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 654 unique sequences. #> Sample 2 - 1215 reads in 610 unique sequences. #> 724232 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input. #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input. #> Identified 0 bimeras out of 5 input sequences. #> $its #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/Getting_started.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"","what":"Taxonomic assignment step","title":"Getting Started","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing = TRUE) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/Getting_started.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Getting Started","text":"","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, minimum_bootstrap = 0, save_outputs = TRUE) #> Rows: 38 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 5 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":[]},{"path":"/articles/Getting_started.html","id":"objects-can-now-be-used-for-downstream-data-analysis","dir":"Articles","previous_headings":"","what":"Objects can now be used for downstream data analysis","title":"Getting Started","text":"make heattrees using taxmap object. First make heat tree -barcoded samples  Now make heat tree rps10-barcoded samples  can also variety analyses, convert phyloseq object demonstrate make stacked bar plot relative abundance taxa sample -barcoded samples  Finally,demonstrate make stacked bar plot relative abundance taxa sample rps10-barcoded samples","code":"objs$taxmap_its %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") objs$taxmap_rps10 %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated.  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martha . Sudermann. Author, maintainer. Zachary S. L Foster. Author. Samantha Dawson. Author. Hung Phan. Author. Jeff H. Chang. Author. Niklaus Grünwald. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sudermann M, Foster Z, Dawson S, Phan H, H. Chang J, Grünwald N (2025). demulticoder: R Package Integrated Analysis Multiplex Metabarcodes. R package version 0.0.0.9000, https://grunwaldlab.github.io/demulticoder/.","code":"@Manual{,   title = {demulticoder: An R Package for the Integrated Analysis of Multiplex Metabarcodes},   author = {Martha A. Sudermann and Zachary S. L Foster and Samantha Dawson and Hung Phan and Jeff {H. Chang} and Niklaus Grünwald},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://grunwaldlab.github.io/demulticoder/}, }"},{"path":"/index.html","id":"demulticoder-r-package","dir":"","previous_headings":"","what":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package actively development. message removed, use caution. Additional testing, documentation, examples progress.","code":""},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"demulticoder package Cutadapt DADA2 wrapper package metabarcodng analyses. main commands outputs intuitive comprehensive, helps account complex iterative nature metabarcoding analyses. brief schematic general workflow:","code":""},{"path":"/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key features","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"ability analysis either demultiplexed pooled amplicons within samples Amplicons multiple datasets trimmed primers, filtered, denoised, merged, given taxonomic assignments one go (different parameters dataset desired) package handles just 16S datasets using default UNITE fungal Silva 16S databases also oomycete rps10 analyses using oomycetedb (https://oomycetedb.org), two custom databases (provided formatted described : https://benjjneb.github.io/dada2/training.html).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"install development version package:","code":"devtools::install_github(\"grunwaldlab/demulticoder\")"},{"path":"/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick start","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"1. Set-input directory files installing package, make data directory add following files: - PE short read amplicon data. files must end either *_R1.fastq.gz* , *_R2.fastq.gz* sample must R1 R2 files. metadata.csv file (unique row sample, samples entered twice contain pooled amplicons, example template) primerinfo_params.csv file (new row unique barcode associated primer sequences, also optional Cutadapt, DADA2 filtering parameters can added adjusted) 2. Prepare reads 3. Cut trim reads 4. Make ASV abundance matrix 5. Assign taxonomy 6. Convert ASV matrix taxmap phyloseq objects","code":"output<-prepare_reads(   data_directory = \"<DATADIR>\",   output_directory = \"<OUTDIR>\") cut_trim(   output,   cutadapt_path=\"<CUTADAPTPATH>\") make_asv_abund_matrix(   output) assign_tax(   output,   asv_abund_matrix) objs<-convert_asv_matrix_to_objs(output)"},{"path":"/index.html","id":"check-out-the-associated-github-repository-to-view-source-code","dir":"","previous_headings":"","what":"Check out the associated Github repository to view source code","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"information, see source code, submit issue, check :https://github.com/grunwaldlab/demulticoder/","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package developed Martha Sudermann, Zachary Foster, Samantha Dawson, Hung Phan, Jeff Chang, Niklaus Grünwald Stay tuned associated manuscript.","code":""},{"path":"/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"demulticoder logo created BioRender.com","code":""},{"path":"/reference/add_pid_to_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Add PID and bootstrap values to tax result. — add_pid_to_tax","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"Add PID bootstrap values tax result.","code":""},{"path":"/reference/add_pid_to_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"","code":"add_pid_to_tax(tax_results, asv_pid)"},{"path":"/reference/add_pid_to_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"tax_results dataframe containing taxonomic assignments asv_pid Percent identity information ASV relative reference database sequence","code":""},{"path":"/reference/assignTax_as_char.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"Combine taxonomic assignments bootstrap values locus single falsification vector","code":""},{"path":"/reference/assignTax_as_char.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"","code":"assignTax_as_char(tax_results, temp_directory_path, locus)"},{"path":"/reference/assignTax_as_char.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"tax_results dataframe containing taxonomic assignments","code":""},{"path":"/reference/assign_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy functions — assign_tax","title":"Assign taxonomy functions — assign_tax","text":"Assign taxonomy functions","code":""},{"path":"/reference/assign_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy functions — assign_tax","text":"","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   retrieve_files = FALSE,   overwrite_existing = FALSE,   db_rps10 = \"oomycetedb.fasta\",   db_its = \"fungidb.fasta\",   db_16S = \"bacteriadb.fasta\",   db_other1 = \"otherdb1.fasta\",   db_other2 = \"otherdb2.fasta\" )"},{"path":"/reference/assign_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy functions — assign_tax","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function asv_abund_matrix ASV abundance matrix. tryRC Whether try reverse complementing sequences taxonomic assignment verbose Logical, indicating whether display verbose output multithread Logical, indicating whether use multithreading retrieve_files Specify TRUE/FALSE whether copy files temp directory output directory overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. db_rps10 reference database rps10 locus db_its reference database locus db_16S reference database 16S locus db_other1 reference database different locus 1 (assumes format like SILVA DB entries) db_other2 reference database different locus 2 (assumes format like SILVA DB entries)","code":""},{"path":"/reference/assign_tax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign taxonomy functions — assign_tax","text":"Taxonomic assignments unique ASV sequence","code":""},{"path":"/reference/assign_tax.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Assign taxonomy functions — assign_tax","text":"point DADA2 assignTaxonomy used assign taxonomy inferred ASVs.","code":""},{"path":"/reference/assign_tax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign taxonomy functions — assign_tax","text":"","code":"# Assign taxonomies to ASVs on a per barcode basis analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\", overwrite_existing = TRUE ) #> Running Cutadapt 4.1 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 4.1 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> 710847 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 654 unique sequences. #> Sample 2 - 1215 reads in 610 unique sequences. #> 724232 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input.  #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values.  #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/asvabund_matrixDADA2_rps10.RData\" #>  assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing = TRUE ) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/reference/assign_taxonomyDada2.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy — assign_taxonomyDada2","title":"Assign taxonomy — assign_taxonomyDada2","text":"Assign taxonomy","code":""},{"path":"/reference/assign_taxonomyDada2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy — assign_taxonomyDada2","text":"","code":"assign_taxonomyDada2(   asv_abund_matrix,   temp_directory_path,   minBoot = 0,   tryRC = FALSE,   verbose = FALSE,   multithread = TRUE,   locus = \"barcode\" )"},{"path":"/reference/assign_taxonomyDada2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy — assign_taxonomyDada2","text":"asv_abund_matrix ASV abundance matrix temp_directory_path temporary directory path minBoot Minimum bootstrap value taxonomy assignment (default 0) tryRC Try reverse complement (default FALSE) verbose Print additional information (default FALSE) multithread Use multiple threads (default TRUE) locus locus taxonomy assignment (e.g., rps10, other1, other2)","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"Filter ASV abundance matrix convert taxmap phyloseq objects","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"convert_asv_matrix_to_objs(   analysis_setup,   min_read_depth = 0,   minimum_bootstrap = 0,   save_outputs = FALSE )"},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function min_read_depth ASV filter parameter. mean read depth across samples less threshold, ASV filtered. minimum_bootstrap Threshold bootstrap support value taxonomic assignments. designated minimum bootstrap threshold, taxnomoic assignments set N/","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"ASV matrix converted taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"# Convert final matrix to taxmap and phyloseq objects for downstream analysis steps analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\", overwrite_existing = TRUE ) #> Running Cutadapt 4.1 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 4.1 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> 710847 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 654 unique sequences. #> Sample 2 - 1215 reads in 610 unique sequences. #> 724232 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input.  #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values.  #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/asvabund_matrixDADA2_rps10.RData\" #>  assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing=TRUE ) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503 objs<-convert_asv_matrix_to_objs( analysis_setup ) #> Rows: 38 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/taxmap_obj_its.RData  #> Phyloseq object saved in: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 5 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/taxmap_obj_rps10.RData  #> Phyloseq object saved in: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/reference/countOverlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Count overlap to see how well the reads were merged — countOverlap","title":"Count overlap to see how well the reads were merged — countOverlap","text":"Count overlap see well reads merged","code":""},{"path":"/reference/countOverlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count overlap to see how well the reads were merged — countOverlap","text":"","code":"countOverlap(data_tables, merged_reads, barcode, output_directory_path)"},{"path":"/reference/countOverlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count overlap to see how well the reads were merged — countOverlap","text":"data_tables data tables containing paths read files, metadata, primer sequences merged_reads Intermediate merged read R data file barcode barcode used analysis output_directory_path path directory resulting files output","code":""},{"path":"/reference/countOverlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count overlap to see how well the reads were merged — countOverlap","text":"plot describing well reads merged information overlap reads","code":""},{"path":"/reference/createASVSequenceTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Make ASV sequence matrix — createASVSequenceTable","title":"Make ASV sequence matrix — createASVSequenceTable","text":"Make ASV sequence matrix","code":""},{"path":"/reference/createASVSequenceTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make ASV sequence matrix — createASVSequenceTable","text":"","code":"createASVSequenceTable(merged_reads, orderBy = \"abundance\")"},{"path":"/reference/createASVSequenceTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make ASV sequence matrix — createASVSequenceTable","text":"merged_reads Intermediate merged read R data file orderBy (Optional). character(1). Default \"abundance\". Specifies sequences (columns) returned table ordered (decreasing). Valid values: \"abundance\", \"nsamples\", NULL.","code":""},{"path":"/reference/createASVSequenceTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make ASV sequence matrix — createASVSequenceTable","text":"raw_seqtab","code":""},{"path":"/reference/cut_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"Main command trim primers using Cutadapt core DADA2 functions","code":""},{"path":"/reference/cut_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"","code":"cut_trim(analysis_setup, cutadapt_path, overwrite_existing = FALSE)"},{"path":"/reference/cut_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function cutadapt_path Path Cutadapt program. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/cut_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"Trimmed reads, primer counts, quality plots, ASV matrix.","code":""},{"path":"/reference/cut_trim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"samples comprised two different barcodes (like ITS1 rps10), reads also demultiplexed prior DADA2 trimming steps.","code":""},{"path":"/reference/cut_trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"","code":"# Remove remaining primers from raw reads, demultiplex pooled barcoded samples,  # and then trim reads based on specific DADA2 parameters analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = TRUE ) #> Running Cutadapt 4.1 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 4.1 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences."},{"path":"/reference/filter_and_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Wrapper function filterAndTrim function DADA2, used primer trimming","code":""},{"path":"/reference/filter_and_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"","code":"filter_and_trim(   output_directory_path,   temp_directory_path,   cutadapt_data_barcode,   barcode_params,   barcode )"},{"path":"/reference/filter_and_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"output_directory_path path directory resulting files output cutadapt_data_barcode directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/filter_and_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Filtered trimmed reads","code":""},{"path":"/reference/format_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Format ASV abundance matrix — format_abund_matrix","title":"Format ASV abundance matrix — format_abund_matrix","text":"Format ASV abundance matrix","code":""},{"path":"/reference/format_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format ASV abundance matrix — format_abund_matrix","text":"","code":"format_abund_matrix(   data_tables,   asv_abund_matrix,   seq_tax_asv,   output_directory_path,   locus )"},{"path":"/reference/format_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format ASV abundance matrix — format_abund_matrix","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants seq_tax_asv amplified sequence variants matrix taxonomic information","code":""},{"path":"/reference/format_database.html","id":null,"dir":"Reference","previous_headings":"","what":"General functions to format user-specified databases — format_database","title":"General functions to format user-specified databases — format_database","text":"General functions format user-specified databases","code":""},{"path":"/reference/format_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General functions to format user-specified databases — format_database","text":"","code":"format_database(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   barcode,   db_its,   db_rps10,   db_16S,   db_other1,   db_other2 )"},{"path":"/reference/format_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General functions to format user-specified databases — format_database","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files barcode barcode database formatted","code":""},{"path":"/reference/format_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General functions to format user-specified databases — format_database","text":"formatted database based specified barcode type","code":""},{"path":"/reference/format_db_16S.html","id":null,"dir":"Reference","previous_headings":"","what":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"16S database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_16S.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"","code":"format_db_16S(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_16S )"},{"path":"/reference/format_db_16S.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_16S name database","code":""},{"path":"/reference/format_db_16S.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"16S database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":null,"dir":"Reference","previous_headings":"","what":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"","code":"format_db_its(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_its )"},{"path":"/reference/format_db_its.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_its name database","code":""},{"path":"/reference/format_db_its.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other1.html","id":null,"dir":"Reference","previous_headings":"","what":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":", user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"","code":"format_db_other1(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other1 )"},{"path":"/reference/format_db_other1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other1 name database","code":""},{"path":"/reference/format_db_other1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other2.html","id":null,"dir":"Reference","previous_headings":"","what":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"second user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"","code":"format_db_other2(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other2 )"},{"path":"/reference/format_db_other2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other2 name database","code":""},{"path":"/reference/format_db_other2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_rps10.html","id":null,"dir":"Reference","previous_headings":"","what":"Create modified reference rps10 database for downstream analysis — format_db_rps10","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"Create modified reference rps10 database downstream analysis","code":""},{"path":"/reference/format_db_rps10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"","code":"format_db_rps10(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_rps10 )"},{"path":"/reference/format_db_rps10.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_rps10 name database","code":""},{"path":"/reference/format_db_rps10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"rps10 database modified headers output reference_databases folder.","code":""},{"path":"/reference/get_fastq_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"Retrieve paths filtered trimmed Fastq files","code":""},{"path":"/reference/get_fastq_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"","code":"get_fastq_paths(data_tables, my_direction, my_primer_pair_id)"},{"path":"/reference/get_fastq_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Whether primer forward reverse direction my_primer_pair_id specific barcode id cutadapt_data directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/get_pids.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"Align ASV sequences reference sequences database get percent ID. Get percent identities.","code":""},{"path":"/reference/get_pids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"","code":"get_pids(tax_results, temp_directory_path, output_directory_path, db, locus)"},{"path":"/reference/get_pids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"tax_results data frame containing taxonomic assignments","code":""},{"path":"/reference/get_post_trim_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_post_trim_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"","code":"get_post_trim_hits(primer_data, cutadapt_data, output_directory_path)"},{"path":"/reference/get_post_trim_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"primer_data primer data frame created orient_primers function cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_post_trim_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Table read counts across sample","code":""},{"path":"/reference/get_pre_primer_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"","code":"get_pre_primer_hits(primer_data, fastq_data, output_directory_path)"},{"path":"/reference/get_pre_primer_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"primer_data primer data data frame created orient_primers function fastq_data data frame FASTQ file paths, direction sequences, names sequences output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"number reads primer found number reads primer found","code":""},{"path":"/reference/get_read_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"Final inventory read counts step input removal chimeras. function deals one sample. TODO optimize one sample","code":""},{"path":"/reference/get_read_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"","code":"get_read_counts(   asv_abund_matrix,   temp_directory_path,   output_directory_path,   locus )"},{"path":"/reference/get_read_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"asv_abund_matrix abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/get_ref_seq.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"Align ASV sequences reference sequences database get percent ID. Start retrieving reference sequences.","code":""},{"path":"/reference/get_ref_seq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"","code":"get_ref_seq(tax_results, db)"},{"path":"/reference/get_ref_seq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"tax_results dataframe containing taxonomic assignments db reference database","code":""},{"path":"/reference/infer_asv_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to infer ASVs, for multiple loci — infer_asv_command","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"Function infer ASVs, multiple loci","code":""},{"path":"/reference/infer_asv_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"","code":"infer_asv_command(   output_directory_path,   temp_directory_path,   data_tables,   barcode_params,   barcode )"},{"path":"/reference/infer_asv_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"output_directory_path path directory resulting files output data_tables data tables containing paths read files, metadata, primer sequences denoised_data_path Path saved intermediate denoised data","code":""},{"path":"/reference/infer_asvs.html","id":null,"dir":"Reference","previous_headings":"","what":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"Core DADA2 function learn errors infer ASVs","code":""},{"path":"/reference/infer_asvs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"","code":"infer_asvs(   data_tables,   my_direction,   my_primer_pair_id,   barcode_params,   output_directory_path )"},{"path":"/reference/infer_asvs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Location read files metadata file my_primer_pair_id specific barcode id output_directory_path path directory containing fastq, metadata, primerinfo_params files","code":""},{"path":"/reference/infer_asvs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"asv_data","code":""},{"path":"/reference/make_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"Quality filtering remove chimeras short sequences","code":""},{"path":"/reference/make_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"","code":"make_abund_matrix(   raw_seqtab,   temp_directory_path,   barcode_params = barcode_params,   barcode )"},{"path":"/reference/make_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"raw_seqtab R data file raw sequence data prior removal chimeras","code":""},{"path":"/reference/make_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"Make amplified sequence variant (ASV) abundance matrix input barcodes","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"","code":"make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE)"},{"path":"/reference/make_asv_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"ASV abundance matrix (asv_abund_matrix)","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"function processes data unique barcode separately, inferring ASVs, merging reads, creating ASV abundance matrix. , DADA2 core denoising alogrithm used infer ASVs.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"","code":"# The primary wrapper function for DADA2 ASV inference steps analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\", overwrite_existing = TRUE ) #> Running Cutadapt 4.1 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 4.1 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> 710847 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 654 unique sequences. #> Sample 2 - 1215 reads in 610 unique sequences. #> 724232 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input.  #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values.  #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/asvabund_matrixDADA2_rps10.RData\" #>"},{"path":"/reference/make_cutadapt_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Prepare primmer trimming Cutaapt. Make new sub-directories specify paths trimmed untrimmed reads","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"","code":"make_cutadapt_tibble(fastq_data, metadata, temp_directory_path)"},{"path":"/reference/make_cutadapt_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"fastq_data path FASTQ files analysis metadata, primer_info files metadata Loaded metadata pairing user's metadata file primer data temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Returns larger data frame containing paths temporary read directories, used input running Cutadapt","code":""},{"path":"/reference/make_seqhist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"Plots histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/make_seqhist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"","code":"make_seqhist(asv_abund_matrix, output_directory_path)"},{"path":"/reference/make_seqhist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_seqhist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/merge_reads_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge forward and reverse reads — merge_reads_command","title":"Merge forward and reverse reads — merge_reads_command","text":"Merge forward reverse reads","code":""},{"path":"/reference/merge_reads_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge forward and reverse reads — merge_reads_command","text":"","code":"merge_reads_command(   output_directory_path,   temp_directory_path,   barcode_params,   barcode )"},{"path":"/reference/merge_reads_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge forward and reverse reads — merge_reads_command","text":"output_directory_path path directory resulting files output merged_read_data_path Path R data file containing merged read data","code":""},{"path":"/reference/merge_reads_command.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge forward and reverse reads — merge_reads_command","text":"merged_reads Intermediate merged read R data file","code":""},{"path":"/reference/orient_primers.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"Take user's forward reverse sequences creates complement, reverse, reverse complement primers one data frame","code":""},{"path":"/reference/orient_primers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"","code":"orient_primers(primers_params_path)"},{"path":"/reference/orient_primers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/orient_primers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"data frame oriented primer information.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Wrapper script plotQualityProfile trim steps primer removal.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"","code":"plot_post_trim_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_post_trim_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Quality profiles reads primer trimming","code":""},{"path":"/reference/plot_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for plotQualityProfile function — plot_qc","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Wrapper function plotQualityProfile function","code":""},{"path":"/reference/plot_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"","code":"plot_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Dada2 wrapper function making quality profiles sample","code":""},{"path":"/reference/prep_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare final ASV abundance matrix — prep_abund_matrix","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"Prepare final ASV abundance matrix","code":""},{"path":"/reference/prep_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"","code":"prep_abund_matrix(cutadapt_data, asv_abund_matrix, data_tables, locus)"},{"path":"/reference/prep_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix locus barcode selected analysis directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/prepare_metadata_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"Read metadata file user combine reformat , given primer data. Included larger function prepare_reads.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"","code":"prepare_metadata_table(metadata_file_path, primer_data)"},{"path":"/reference/prepare_metadata_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"primer_data data frame oriented primer information returned orient_primers function. metadata_path path metadata file.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"dataframe containing merged metadata primer data.","code":""},{"path":"/reference/prepare_reads.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare reads for primer trimming using Cutadapt — prepare_reads","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"Prepare reads primer trimming using Cutadapt","code":""},{"path":"/reference/prepare_reads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"prepare_reads(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\",   overwrite_existing = FALSE )"},{"path":"/reference/prepare_reads.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. multithread Logical, indicating whether use multithreading certain operations. Default FALSE.","code":""},{"path":"/reference/prepare_reads.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"list containing data tables, including metadata, primer sequences search based orientation, paths trimming reads, user-defined parameters subsequent steps.","code":""},{"path":"/reference/prepare_reads.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"# Pre-filter raw reads and parse metadata and primer_information to prepare  # for primer trimming and filter analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpKJNKmR/demulticoder_run_temp/prefiltered_sequences"},{"path":"/reference/primer_check.html","id":null,"dir":"Reference","previous_headings":"","what":"Matching Order Primer Check — primer_check","title":"Matching Order Primer Check — primer_check","text":"Matching Order Primer Check","code":""},{"path":"/reference/primer_check.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matching Order Primer Check — primer_check","text":"","code":"primer_check(fastq_data)"},{"path":"/reference/primer_check.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matching Order Primer Check — primer_check","text":"fastq_data data frame FASTQ file paths, direction sequences, names sequences","code":""},{"path":"/reference/primer_check.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matching Order Primer Check — primer_check","text":"None","code":""},{"path":"/reference/process_single_barcode.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"Process information ASV abundance matrix run DADA2 single barcode","code":""},{"path":"/reference/process_single_barcode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"","code":"process_single_barcode(   data_tables,   temp_directory_path,   output_directory_path,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   locus = barcode )"},{"path":"/reference/process_single_barcode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"data_tables data tables containing paths read files, metadata, primer sequences temp_directory_path temporary directory path asv_abund_matrix abundance matrix containing amplified sequence variants tryRC Try reverse complement (default FALSE) verbose Print additional information (default FALSE) multithread Use multiple threads (default TRUE) locus locus taxonomy assignment (e.g., rps10, other1, other2)","code":""},{"path":"/reference/read_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"Takes FASTQ files user creates data frame paths files created used future. Included larger 'read_prefilt_fastq' function.","code":""},{"path":"/reference/read_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"","code":"read_fastq(data_directory_path, temp_directory_path)"},{"path":"/reference/read_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data_directory_path path directory containing FASTQ, metadata, primer_info files temp_directory_path User-defined temporary directory place reads throughout workflow.","code":""},{"path":"/reference/read_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data frame FASTQ file paths, primer orientations sequences, parsed sample names","code":""},{"path":"/reference/read_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"Take user's DADA2 parameters make dataframe downstream steps","code":""},{"path":"/reference/read_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"","code":"read_parameters(primers_params_path)"},{"path":"/reference/read_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/read_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"data frame information DADA2 parameters.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"function calling read_fastq, primer_check, remove_ns functions. process edit FASTQ make ready trimming primers Cutadapt. Part larger 'prepare_reads' function.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"","code":"read_prefilt_fastq(   data_directory_path = data_directory_path,   multithread,   temp_directory_path )"},{"path":"/reference/read_prefilt_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"data_directory_path path directory containing FASTQ, metadata.csv, primerinfo_params.csv files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"Returns filtered reads Ns","code":""},{"path":"/reference/remove_ns.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Wrapper function core DADA2 filter trim function first filtering step","code":""},{"path":"/reference/remove_ns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"","code":"remove_ns(fastq_data, multithread, temp_directory_path)"},{"path":"/reference/remove_ns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"fastq_data data frame fastq file paths, direction sequences, names sequences metadata, primer_info files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow metadata metadata containing concatenated metadata primer data","code":""},{"path":"/reference/remove_ns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Return prefiltered reads Ns","code":""},{"path":"/reference/run_cutadapt.html","id":null,"dir":"Reference","previous_headings":"","what":"Core function for running cutadapt — run_cutadapt","title":"Core function for running cutadapt — run_cutadapt","text":"Core function running cutadapt","code":""},{"path":"/reference/run_cutadapt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core function for running cutadapt — run_cutadapt","text":"","code":"run_cutadapt(   cutadapt_path,   cutadapt_data_barcode,   barcode_params,   minCutadaptlength )"},{"path":"/reference/run_cutadapt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core function for running cutadapt — run_cutadapt","text":"cutadapt_path path cutadapt program. minCutadaptlength Read lengths lower threshold discarded. Default 50. cutadapt_data Directory_data folder trimmed filtered reads sample.","code":""},{"path":"/reference/run_cutadapt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core function for running cutadapt — run_cutadapt","text":"Trimmed read.","code":""},{"path":"/reference/setup_directories.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up directory paths for subsequent analyses — setup_directories","title":"Set up directory paths for subsequent analyses — setup_directories","text":"function sets directory paths subsequent analyses. checks whether specified output directories exist creates . function also provides paths primer metadata files within data directory.","code":""},{"path":"/reference/setup_directories.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up directory paths for subsequent analyses — setup_directories","text":"","code":"setup_directories(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\" )"},{"path":"/reference/setup_directories.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up directory paths for subsequent analyses — setup_directories","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run.","code":""},{"path":"/reference/setup_directories.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up directory paths for subsequent analyses — setup_directories","text":"list paths data, output, temporary directories, primer, metadata files.","code":""}]
