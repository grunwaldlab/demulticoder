[{"path":"/articles/01_getting_started.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before You Start","title":"Getting Started","text":"following example, demonstrate key package functionality using subset reads two samples containing pooled ITS1 fungal rps10 oomycete amplicons. databases used assign taxonomy step also abridged versions full UNITE oomyceteDB databases. can follow along test data associated CSV input files loaded package. Additional examples also available website. Please note, speed, test dataset comprised randomly subset reads samples (S1 S2), due database size, full UNITE database included package, also smaller subset larger database. need prepare raw read files fill metadata.csv primerinfo_params.csv templates.","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-the-pe-amplicon-files","dir":"Articles","previous_headings":"","what":"Format of the PE amplicon files","title":"Getting Started","text":"package takes foward reverse Illumina short read sequence data. avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz. Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz error R1 R2 directly preceding ‘.fastq.gz’ suffix.","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"","what":"Format of metadata file (metadata.csv)","title":"Getting Started","text":"format CSV file simple. template . two necessary columns (names) : sample_name column primer_info column additional metadata pasted two columns. can referenced later analysis steps save step loading metadata later. S1 S2 come rhododendron rhizobiome dataset random subset reads. notice S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled reads (rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. Example using test dataset:","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-primer-and-parameters-file-primerinfo_parms-csv","dir":"Articles","previous_headings":"","what":"Format of primer and parameters file (primerinfo_parms.csv)","title":"Getting Started","text":"DADA2 Primer sequence information user-defined parameters placed primerinfo_params.csv. simplify functions called, user provide parameters within input file. recommend using template linked required columns user must fill : 1.primer_name (rps10, , another barcode) 2.forward-forward sequence 3.reverse-reverse sequence user doesn’t add info subsequent columns, series default parameters used. Example template ‘primerinfo_params.csv’ TODO-table parameters, functions associated , default parameters, whether Cutadapt, DADA2, package specific parameters.","code":""},{"path":"/articles/01_getting_started.html","id":"reference-database-format","dir":"Articles","previous_headings":"","what":"Reference Database Format","title":"Getting Started","text":"now, package compatible following databases: oomycetedb : http://www.oomycetedb.org/ SILVA 16S database species assignments: https://zenodo.org/records/4587955/files/silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1 UNITE fungal database https://unite.ut.ee/repository.php user can select one database (now), first need reformat headers exactly like UNITE fungal database specifications. Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function","code":""},{"path":"/articles/01_getting_started.html","id":"additional-notes","dir":"Articles","previous_headings":"","what":"Additional Notes","title":"Getting Started","text":"Computer specifications may limiting factor– using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases package randomly subsetted demonstration purposes. Users need upload databases input data folder. computer crashes taxonomic assignment step, please switch computing cluster. Please also ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/01_getting_started.html","id":"loading-the-package","dir":"Articles","previous_headings":"","what":"Loading the Package","title":"Getting Started","text":"now, package loaded retrieving GitHub. Eventually, package uploaded CRAN Bioconductor.","code":"#devtools::install_github(\"grunwaldlab/demulticoder\") devtools::load_all(\"~/demulticoder\") library(demulticoder)"},{"path":"/articles/01_getting_started.html","id":"reorganize-data-tables-and-set-up-data-directory-structure","dir":"Articles","previous_headings":"","what":"Reorganize data tables and set-up data directory structure","title":"Getting Started","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    overwrite_existing=TRUE)"},{"path":"/articles/01_getting_started.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"","what":"Remove primers with Cutadapt","title":"Getting Started","text":"running Cutadapt, please ensure installed .","code":"demulticoder::cut_trim(   analysis_setup,   cutadapt_path=\"/usr/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 3.5 for its sequence data  #> Running Cutadapt 3.5 for rps10 sequence data"},{"path":"/articles/01_getting_started.html","id":"asv-inference-step","dir":"Articles","previous_headings":"","what":"ASV inference step","title":"Getting Started","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = FALSE) #> Warning in make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE): #> No existing files found. The 'make_asv_abund_matrix' function will run. #> 711900 total bases in 2698 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1481 reads in 660 unique sequences. #> Sample 2 - 1217 reads in 614 unique sequences. #> 725393 total bases in 2698 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1481 reads in 1021 unique sequences. #> Sample 2 - 1217 reads in 815 unique sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821851 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> $its #> [1] \"/tmp/Rtmpm5j0zR/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/Rtmpm5j0zR/demulticoder_run/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/01_getting_started.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"","what":"Taxonomic assignment step","title":"Getting Started","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing=FALSE) #> Warning in assign_tax(analysis_setup, asv_abund_matrix, retrieve_files = TRUE, #> : No existing files found. The analysis will be run. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1481      1427      1433   1316    1316 #> 2             S2_its  1996     1217      1145      1124   1065    1065 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/01_getting_started.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Getting Started","text":"","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, save_outputs=TRUE) #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":[]},{"path":"/articles/01_getting_started.html","id":"objects-can-now-be-used-for-downstream-data-analysis","dir":"Articles","previous_headings":"","what":"Objects can now be used for downstream data analysis","title":"Getting Started","text":"make heattrees using taxmap object. First make heat tree -barcoded samples  Now make heat tree rps10-barcoded samples  can also variety analyses, convert phyloseq object demonstrate make stacked bar plot relative abundance taxa sample -barcoded samples  Finally,demonstrate make stacked bar plot relative abundance taxa sample rps10-barcoded samples","code":"metacoder::heat_tree(objs$taxmap_its,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\") metacoder::heat_tree(objs$taxmap_rps10,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\") data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated.  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/articles/02_mixed_barcode_example.html","id":"demonstration-of-how-to-use-demulticoder-on-a-dataset-that-is-actually-three-separate-datasets-rps10-its-and-16s-at-once","dir":"Articles","previous_headings":"","what":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","title":"Mixed barcode example","text":"Input metadata primerinfo_params files data folder TODO show example add commentary","code":""},{"path":"/articles/02_mixed_barcode_example.html","id":"step-1-remove-ns-and-create-directory-structure-for-downstream-steps","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 1-Remove N’s and create directory structure for downstream steps","title":"Mixed barcode example","text":"","code":"outputs<-prepare_reads(   data_directory = \"~/demulticoder_benchmarking/demulticoder_combined_analysis_dads/data\",    output_directory = \"~/output_multiple_datasets\",    overwrite_existing = TRUE) #> Rows: 3 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 3 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 72 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (5): sample_name, primer_name, incubation, soil, treatment #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/Rtmp7bWhXN/demulticoder_run/prefiltered_sequences"},{"path":"/articles/02_mixed_barcode_example.html","id":"step-2-run-cutadapt-to-remove-primers-and-then-trim-reads-with-dada2-filterandtrim-function","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","title":"Mixed barcode example","text":"","code":"cut_trim(   outputs,   cutadapt_path=\"/usr/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 3.5 for its sequence data  #> Running Cutadapt 3.5 for rps10 sequence data  #> Running Cutadapt 3.5 for sixteenS sequence data  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/1_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/1_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/1_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/1_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/1_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/1_3_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/2_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/2_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/2_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/2_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/2_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/2_3_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/3_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/3_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/3_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/3_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/3_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/3_3_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/4_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/4_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/4_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/4_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/4_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/4_3_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/5_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/5_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/5_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/5_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/5_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/5_3_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/6_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/6_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/6_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/6_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/6_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/6_3_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/7_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/7_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/7_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/7_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/7_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/7_3_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/8_1_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/8_1_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/8_2_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/8_2_01_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/8_3_01_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /tmp/Rtmp7bWhXN/demulticoder_run/trimmed_sequences/8_3_01_R2_sixteenS.fastq.gz"},{"path":"/articles/02_mixed_barcode_example.html","id":"step-3-core-asv-inference-step","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 3-Core ASV inference step","title":"Mixed barcode example","text":"","code":"make_asv_abund_matrix(   outputs,   overwrite_existing = TRUE) #> 102635388 total bases in 409159 reads from 13 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ............. #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #>    selfConsist step 7 #>    selfConsist step 8 #>    selfConsist step 9 #> Convergence after  9  rounds. #> Error rate plot for the Forward read of primer pair sixteenS #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 31044 reads in 16523 unique sequences. #> Sample 2 - 33424 reads in 17777 unique sequences. #> Sample 3 - 31303 reads in 17153 unique sequences. #> Sample 4 - 32148 reads in 15953 unique sequences. #> Sample 5 - 33116 reads in 17060 unique sequences. #> Sample 6 - 32784 reads in 17064 unique sequences. #> Sample 7 - 28387 reads in 14682 unique sequences. #> Sample 8 - 30735 reads in 16838 unique sequences. #> Sample 9 - 28056 reads in 14315 unique sequences. #> Sample 10 - 27224 reads in 13751 unique sequences. #> Sample 11 - 29113 reads in 15066 unique sequences. #> Sample 12 - 26465 reads in 13934 unique sequences. #> Sample 13 - 45360 reads in 23666 unique sequences. #> Sample 14 - 36081 reads in 19771 unique sequences. #> Sample 15 - 25836 reads in 13766 unique sequences. #> Sample 16 - 34122 reads in 17632 unique sequences. #> Sample 17 - 29597 reads in 14830 unique sequences. #> Sample 18 - 32748 reads in 16553 unique sequences. #> Sample 19 - 28625 reads in 15077 unique sequences. #> Sample 20 - 33205 reads in 16854 unique sequences. #> Sample 21 - 53629 reads in 28116 unique sequences. #> Sample 22 - 27085 reads in 14055 unique sequences. #> Sample 23 - 30555 reads in 14959 unique sequences. #> Sample 24 - 28050 reads in 14411 unique sequences. #> 102636881 total bases in 409159 reads from 13 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ............. #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #> Convergence after  6  rounds. #> Error rate plot for the Reverse read of primer pair sixteenS #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 31044 reads in 20471 unique sequences. #> Sample 2 - 33424 reads in 22029 unique sequences. #> Sample 3 - 31303 reads in 21177 unique sequences. #> Sample 4 - 32148 reads in 20174 unique sequences. #> Sample 5 - 33116 reads in 21307 unique sequences. #> Sample 6 - 32784 reads in 21009 unique sequences. #> Sample 7 - 28387 reads in 18556 unique sequences. #> Sample 8 - 30735 reads in 20617 unique sequences. #> Sample 9 - 28056 reads in 18116 unique sequences. #> Sample 10 - 27224 reads in 16921 unique sequences. #> Sample 11 - 29113 reads in 18995 unique sequences. #> Sample 12 - 26465 reads in 17308 unique sequences. #> Sample 13 - 45360 reads in 30076 unique sequences. #> Sample 14 - 36081 reads in 23820 unique sequences. #> Sample 15 - 25836 reads in 17177 unique sequences. #> Sample 16 - 34122 reads in 21692 unique sequences. #> Sample 17 - 29597 reads in 18744 unique sequences. #> Sample 18 - 32748 reads in 20631 unique sequences. #> Sample 19 - 28625 reads in 18661 unique sequences. #> Sample 20 - 33205 reads in 21617 unique sequences. #> Sample 21 - 53629 reads in 34768 unique sequences. #> Sample 22 - 27085 reads in 17426 unique sequences. #> Sample 23 - 30555 reads in 18972 unique sequences. #> Sample 24 - 28050 reads in 18055 unique sequences. #> 24283 paired-reads (in 567 unique pairings) successfully merged out of 27568 (in 1190 pairings) input. #> Duplicate sequences in merged output. #> 25891 paired-reads (in 606 unique pairings) successfully merged out of 29763 (in 1318 pairings) input. #> Duplicate sequences in merged output. #> 24499 paired-reads (in 552 unique pairings) successfully merged out of 27853 (in 1152 pairings) input. #> Duplicate sequences in merged output. #> 25319 paired-reads (in 600 unique pairings) successfully merged out of 28432 (in 1189 pairings) input. #> Duplicate sequences in merged output. #> 25772 paired-reads (in 586 unique pairings) successfully merged out of 29240 (in 1193 pairings) input. #> Duplicate sequences in merged output. #> 25716 paired-reads (in 589 unique pairings) successfully merged out of 28997 (in 1179 pairings) input. #> Duplicate sequences in merged output. #> 21943 paired-reads (in 526 unique pairings) successfully merged out of 25354 (in 1111 pairings) input. #> Duplicate sequences in merged output. #> 23701 paired-reads (in 539 unique pairings) successfully merged out of 27355 (in 1159 pairings) input. #> Duplicate sequences in merged output. #> 22343 paired-reads (in 545 unique pairings) successfully merged out of 25084 (in 1045 pairings) input. #> Duplicate sequences in merged output. #> 21642 paired-reads (in 563 unique pairings) successfully merged out of 23960 (in 1016 pairings) input. #> Duplicate sequences in merged output. #> 22393 paired-reads (in 566 unique pairings) successfully merged out of 25431 (in 1134 pairings) input. #> Duplicate sequences in merged output. #> 20426 paired-reads (in 475 unique pairings) successfully merged out of 22945 (in 958 pairings) input. #> Duplicate sequences in merged output. #> 36361 paired-reads (in 722 unique pairings) successfully merged out of 40899 (in 1578 pairings) input. #> Duplicate sequences in merged output. #> 28632 paired-reads (in 635 unique pairings) successfully merged out of 32430 (in 1325 pairings) input. #> Duplicate sequences in merged output. #> 19868 paired-reads (in 490 unique pairings) successfully merged out of 22923 (in 1000 pairings) input. #> Duplicate sequences in merged output. #> 27071 paired-reads (in 672 unique pairings) successfully merged out of 30161 (in 1233 pairings) input. #> Duplicate sequences in merged output. #> 23064 paired-reads (in 624 unique pairings) successfully merged out of 25935 (in 1207 pairings) input. #> Duplicate sequences in merged output. #> 25878 paired-reads (in 616 unique pairings) successfully merged out of 28967 (in 1247 pairings) input. #> Duplicate sequences in merged output. #> 22253 paired-reads (in 541 unique pairings) successfully merged out of 25462 (in 1109 pairings) input. #> Duplicate sequences in merged output. #> 25999 paired-reads (in 577 unique pairings) successfully merged out of 29892 (in 1261 pairings) input. #> Duplicate sequences in merged output. #> 43357 paired-reads (in 818 unique pairings) successfully merged out of 48347 (in 1869 pairings) input. #> Duplicate sequences in merged output. #> 21016 paired-reads (in 555 unique pairings) successfully merged out of 23680 (in 1112 pairings) input. #> Duplicate sequences in merged output. #> 24130 paired-reads (in 624 unique pairings) successfully merged out of 27041 (in 1198 pairings) input. #> Duplicate sequences in merged output. #> 21746 paired-reads (in 577 unique pairings) successfully merged out of 24561 (in 1142 pairings) input. #> Duplicate sequences in merged output. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Identified 99 bimeras out of 4529 input sequences. #> 106034910 total bases in 466183 reads from 9 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ......... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #> Convergence after  5  rounds. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 78090 reads in 14475 unique sequences. #> Sample 2 - 45955 reads in 9604 unique sequences. #> Sample 3 - 42450 reads in 9362 unique sequences. #> Sample 4 - 47513 reads in 13905 unique sequences. #> Sample 5 - 37652 reads in 12257 unique sequences. #> Sample 6 - 50196 reads in 15866 unique sequences. #> Sample 7 - 33726 reads in 7781 unique sequences. #> Sample 8 - 48490 reads in 10219 unique sequences. #> Sample 9 - 82111 reads in 14763 unique sequences. #> Sample 10 - 65012 reads in 18955 unique sequences. #> Sample 11 - 50432 reads in 13490 unique sequences. #> Sample 12 - 44904 reads in 13902 unique sequences. #> Sample 13 - 35611 reads in 7806 unique sequences. #> Sample 14 - 29981 reads in 7103 unique sequences. #> Sample 15 - 35426 reads in 8032 unique sequences. #> Sample 16 - 40535 reads in 13049 unique sequences. #> Sample 17 - 37788 reads in 11717 unique sequences. #> Sample 18 - 25282 reads in 8508 unique sequences. #> Sample 19 - 24576 reads in 5694 unique sequences. #> Sample 20 - 31335 reads in 7645 unique sequences. #> Sample 21 - 37061 reads in 7258 unique sequences. #> Sample 22 - 30407 reads in 9859 unique sequences. #> Sample 23 - 42100 reads in 11928 unique sequences. #> Sample 24 - 57989 reads in 16385 unique sequences. #> 105499828 total bases in 466183 reads from 9 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ......... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #> Convergence after  4  rounds. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 78090 reads in 18561 unique sequences. #> Sample 2 - 45955 reads in 11830 unique sequences. #> Sample 3 - 42450 reads in 12227 unique sequences. #> Sample 4 - 47513 reads in 12720 unique sequences. #> Sample 5 - 37652 reads in 15089 unique sequences. #> Sample 6 - 50196 reads in 19051 unique sequences. #> Sample 7 - 33726 reads in 12311 unique sequences. #> Sample 8 - 48490 reads in 17792 unique sequences. #> Sample 9 - 82111 reads in 19363 unique sequences. #> Sample 10 - 65012 reads in 16307 unique sequences. #> Sample 11 - 50432 reads in 12229 unique sequences. #> Sample 12 - 44904 reads in 12850 unique sequences. #> Sample 13 - 35611 reads in 9856 unique sequences. #> Sample 14 - 29981 reads in 8435 unique sequences. #> Sample 15 - 35426 reads in 10431 unique sequences. #> Sample 16 - 40535 reads in 10489 unique sequences. #> Sample 17 - 37788 reads in 18061 unique sequences. #> Sample 18 - 25282 reads in 11321 unique sequences. #> Sample 19 - 24576 reads in 13200 unique sequences. #> Sample 20 - 31335 reads in 13584 unique sequences. #> Sample 21 - 37061 reads in 10363 unique sequences. #> Sample 22 - 30407 reads in 9253 unique sequences. #> Sample 23 - 42100 reads in 10947 unique sequences. #> Sample 24 - 57989 reads in 13634 unique sequences. #> 76532 paired-reads (in 235 unique pairings) successfully merged out of 77154 (in 268 pairings) input. #> Duplicate sequences in merged output. #> 44826 paired-reads (in 182 unique pairings) successfully merged out of 45262 (in 211 pairings) input. #> Duplicate sequences in merged output. #> 41241 paired-reads (in 177 unique pairings) successfully merged out of 41588 (in 200 pairings) input. #> Duplicate sequences in merged output. #> 46443 paired-reads (in 134 unique pairings) successfully merged out of 46791 (in 153 pairings) input. #> Duplicate sequences in merged output. #> 36763 paired-reads (in 118 unique pairings) successfully merged out of 36988 (in 133 pairings) input. #> Duplicate sequences in merged output. #> 49327 paired-reads (in 138 unique pairings) successfully merged out of 49592 (in 153 pairings) input. #> Duplicate sequences in merged output. #> 32710 paired-reads (in 136 unique pairings) successfully merged out of 33009 (in 158 pairings) input. #> Duplicate sequences in merged output. #> 47112 paired-reads (in 167 unique pairings) successfully merged out of 47654 (in 200 pairings) input. #> Duplicate sequences in merged output. #> 80706 paired-reads (in 239 unique pairings) successfully merged out of 81276 (in 280 pairings) input. #> Duplicate sequences in merged output. #> 63826 paired-reads (in 177 unique pairings) successfully merged out of 64176 (in 202 pairings) input. #> Duplicate sequences in merged output. #> 49621 paired-reads (in 117 unique pairings) successfully merged out of 49737 (in 131 pairings) input. #> Duplicate sequences in merged output. #> 43962 paired-reads (in 136 unique pairings) successfully merged out of 44349 (in 154 pairings) input. #> Duplicate sequences in merged output. #> 34455 paired-reads (in 154 unique pairings) successfully merged out of 34905 (in 181 pairings) input. #> Duplicate sequences in merged output. #> 28736 paired-reads (in 155 unique pairings) successfully merged out of 29390 (in 181 pairings) input. #> Duplicate sequences in merged output. #> 33960 paired-reads (in 163 unique pairings) successfully merged out of 34687 (in 188 pairings) input. #> Duplicate sequences in merged output. #> 39776 paired-reads (in 129 unique pairings) successfully merged out of 39921 (in 142 pairings) input. #> Duplicate sequences in merged output. #> 36836 paired-reads (in 104 unique pairings) successfully merged out of 36988 (in 113 pairings) input. #> Duplicate sequences in merged output. #> 24650 paired-reads (in 88 unique pairings) successfully merged out of 24794 (in 96 pairings) input. #> Duplicate sequences in merged output. #> 23672 paired-reads (in 102 unique pairings) successfully merged out of 23918 (in 118 pairings) input. #> Duplicate sequences in merged output. #> 30099 paired-reads (in 103 unique pairings) successfully merged out of 30492 (in 123 pairings) input. #> Duplicate sequences in merged output. #> 35951 paired-reads (in 148 unique pairings) successfully merged out of 36386 (in 166 pairings) input. #> Duplicate sequences in merged output. #> 29757 paired-reads (in 117 unique pairings) successfully merged out of 29919 (in 126 pairings) input. #> Duplicate sequences in merged output. #> 41280 paired-reads (in 143 unique pairings) successfully merged out of 41588 (in 162 pairings) input. #> Duplicate sequences in merged output. #> 56900 paired-reads (in 156 unique pairings) successfully merged out of 57408 (in 180 pairings) input. #> Duplicate sequences in merged output. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Identified 28 bimeras out of 869 input sequences. #> 126787839 total bases in 451263 reads from 6 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ...... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #> Convergence after  5  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Sample 1 - 20082 reads in 15404 unique sequences. #> Sample 2 - 81794 reads in 50235 unique sequences. #> Sample 3 - 86486 reads in 57261 unique sequences. #> Sample 4 - 64939 reads in 48091 unique sequences. #> Sample 5 - 79738 reads in 50805 unique sequences. #> Sample 6 - 118224 reads in 69385 unique sequences. #> Sample 7 - 19427 reads in 13201 unique sequences. #> Sample 8 - 73752 reads in 47699 unique sequences. #> Sample 9 - 23829 reads in 16766 unique sequences. #> Sample 10 - 69370 reads in 41438 unique sequences. #> Sample 11 - 134589 reads in 79909 unique sequences. #> Sample 12 - 88315 reads in 55387 unique sequences. #> Sample 13 - 43607 reads in 37646 unique sequences. #> Sample 14 - 68342 reads in 51601 unique sequences. #> Sample 15 - 14251 reads in 12014 unique sequences. #> Sample 16 - 10633 reads in 9740 unique sequences. #> Sample 17 - 99725 reads in 58227 unique sequences. #> Sample 18 - 82697 reads in 49568 unique sequences. #> Sample 19 - 58679 reads in 39009 unique sequences. #> Sample 20 - 58661 reads in 40824 unique sequences. #> Sample 21 - 46627 reads in 39317 unique sequences. #> Sample 22 - 60360 reads in 46852 unique sequences. #> Sample 23 - 14006 reads in 11814 unique sequences. #> Sample 24 - 22369 reads in 18243 unique sequences. #> 126347798 total bases in 451263 reads from 6 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ...... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #>    selfConsist step 7 #> Convergence after  7  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Sample 1 - 20082 reads in 14654 unique sequences. #> Sample 2 - 81794 reads in 56653 unique sequences. #> Sample 3 - 86486 reads in 61939 unique sequences. #> Sample 4 - 64939 reads in 41617 unique sequences. #> Sample 5 - 79738 reads in 48181 unique sequences. #> Sample 6 - 118224 reads in 71942 unique sequences. #> Sample 7 - 19427 reads in 14744 unique sequences. #> Sample 8 - 73752 reads in 53141 unique sequences. #> Sample 9 - 23829 reads in 17238 unique sequences. #> Sample 10 - 69370 reads in 41800 unique sequences. #> Sample 11 - 134589 reads in 73468 unique sequences. #> Sample 12 - 88315 reads in 51668 unique sequences. #> Sample 13 - 43607 reads in 29142 unique sequences. #> Sample 14 - 68342 reads in 42353 unique sequences. #> Sample 15 - 14251 reads in 10551 unique sequences. #> Sample 16 - 10633 reads in 6488 unique sequences. #> Sample 17 - 99725 reads in 56068 unique sequences. #> Sample 18 - 82697 reads in 47196 unique sequences. #> Sample 19 - 58679 reads in 43743 unique sequences. #> Sample 20 - 58661 reads in 40142 unique sequences. #> Sample 21 - 46627 reads in 33507 unique sequences. #> Sample 22 - 60360 reads in 31906 unique sequences. #> Sample 23 - 14006 reads in 8796 unique sequences. #> Sample 24 - 22369 reads in 13421 unique sequences. #> 19680 paired-reads (in 36 unique pairings) successfully merged out of 19998 (in 91 pairings) input. #> Duplicate sequences in merged output. #> 77741 paired-reads (in 72 unique pairings) successfully merged out of 81624 (in 162 pairings) input. #> Duplicate sequences in merged output. #> 81094 paired-reads (in 72 unique pairings) successfully merged out of 86377 (in 191 pairings) input. #> Duplicate sequences in merged output. #> 60693 paired-reads (in 192 unique pairings) successfully merged out of 64495 (in 586 pairings) input. #> Duplicate sequences in merged output. #> 75768 paired-reads (in 125 unique pairings) successfully merged out of 79396 (in 371 pairings) input. #> Duplicate sequences in merged output. #> 113422 paired-reads (in 172 unique pairings) successfully merged out of 117935 (in 471 pairings) input. #> Duplicate sequences in merged output. #> 18957 paired-reads (in 42 unique pairings) successfully merged out of 19137 (in 78 pairings) input. #> Duplicate sequences in merged output. #> 70795 paired-reads (in 79 unique pairings) successfully merged out of 73418 (in 146 pairings) input. #> Duplicate sequences in merged output. #> 23206 paired-reads (in 26 unique pairings) successfully merged out of 23760 (in 54 pairings) input. #> Duplicate sequences in merged output. #> 66712 paired-reads (in 81 unique pairings) successfully merged out of 69287 (in 241 pairings) input. #> Duplicate sequences in merged output. #> 128424 paired-reads (in 143 unique pairings) successfully merged out of 134179 (in 349 pairings) input. #> Duplicate sequences in merged output. #> 85426 paired-reads (in 85 unique pairings) successfully merged out of 88207 (in 264 pairings) input. #> Duplicate sequences in merged output. #> 41456 paired-reads (in 37 unique pairings) successfully merged out of 43409 (in 115 pairings) input. #> Duplicate sequences in merged output. #> 65184 paired-reads (in 84 unique pairings) successfully merged out of 68075 (in 205 pairings) input. #> Duplicate sequences in merged output. #> 13965 paired-reads (in 31 unique pairings) successfully merged out of 14174 (in 66 pairings) input. #> Duplicate sequences in merged output. #> 9229 paired-reads (in 16 unique pairings) successfully merged out of 10463 (in 76 pairings) input. #> Duplicate sequences in merged output. #> 96759 paired-reads (in 63 unique pairings) successfully merged out of 99648 (in 198 pairings) input. #> Duplicate sequences in merged output. #> 78760 paired-reads (in 80 unique pairings) successfully merged out of 82349 (in 244 pairings) input. #> Duplicate sequences in merged output. #> 52961 paired-reads (in 106 unique pairings) successfully merged out of 58370 (in 267 pairings) input. #> Duplicate sequences in merged output. #> 52492 paired-reads (in 81 unique pairings) successfully merged out of 58507 (in 209 pairings) input. #> Duplicate sequences in merged output. #> 44849 paired-reads (in 60 unique pairings) successfully merged out of 45994 (in 190 pairings) input. #> Duplicate sequences in merged output. #> 58198 paired-reads (in 55 unique pairings) successfully merged out of 60194 (in 175 pairings) input. #> Duplicate sequences in merged output. #> 13004 paired-reads (in 19 unique pairings) successfully merged out of 13817 (in 65 pairings) input. #> 21168 paired-reads (in 38 unique pairings) successfully merged out of 22205 (in 167 pairings) input. #> Duplicate sequences in merged output. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Identified 548 bimeras out of 732 input sequences. #> $sixteenS #> [1] \"/tmp/Rtmp7bWhXN/demulticoder_run/asvabund_matrixDADA2_sixteenS.RData\" #>  #> $its #> [1] \"/tmp/Rtmp7bWhXN/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/Rtmp7bWhXN/demulticoder_run/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/02_mixed_barcode_example.html","id":"step-4-assign-taxonomy-step","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 4-Assign taxonomy step","title":"Mixed barcode example","text":"","code":"assign_tax(   outputs,   asv_abund_matrix,   db_its = \"sh_general_release_dynamic_all_25.07.2023.fasta\",   db_rps10 = \"oomycetedb.fasta\",   db_16s=\"silva_nr99_v138.1_wSpecies_train_set.fa\",   retrieve_files=TRUE,   overwrite_existing=FALSE) #> Warning in assign_tax(outputs, asv_abund_matrix, db_its = #> \"sh_general_release_dynamic_all_25.07.2023.fasta\", : No existing files found. #> The analysis will be run. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #>    samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1     1_1_01_sixteenS 40437    31044     28831     28734  24283   24083 #> 2     1_2_01_sixteenS 44735    33424     31233     30898  25891   25478 #> 3     1_3_01_sixteenS 40114    31303     28921     29135  24499   24173 #> 4     2_1_01_sixteenS 39984    32148     29635     29600  25319   24777 #> 5     2_2_01_sixteenS 43128    33116     30520     30402  25772   25191 #> 6     2_3_01_sixteenS 43158    32784     30284     30135  25716   25190 #> 7     3_1_01_sixteenS 37316    28387     26469     26388  21943   21860 #> 8     3_2_01_sixteenS 40927    30735     28505     28481  23701   23565 #> 9     3_3_01_sixteenS 37114    28056     26174     26167  22343   22274 #> 10    4_1_01_sixteenS 34714    27224     25138     24949  21642   21498 #> 11    4_2_01_sixteenS 38032    29113     26796     26533  22393   22228 #> 12    4_3_01_sixteenS 33827    26465     24219     23954  20426   20196 #> 13    5_1_01_sixteenS 59058    45360     42428     42464  36361   35713 #> 14    5_2_01_sixteenS 46740    36081     33668     33625  28632   28372 #> 15    5_3_01_sixteenS 33537    25836     23919     23907  19868   19749 #> 16    6_1_01_sixteenS 44580    34122     31504     31387  27071   26675 #> 17    6_2_01_sixteenS 38109    29597     27310     27077  23064   22872 #> 18    6_3_01_sixteenS 43078    32748     30319     30200  25878   25608 #> 19    7_1_01_sixteenS 37803    28625     26505     26610  22253   22200 #> 20    7_2_01_sixteenS 44298    33205     30947     31107  25999   25751 #> 21    7_3_01_sixteenS 69977    53629     50285     50177  43357   42594 #> 22    8_1_01_sixteenS 35717    27085     24927     24699  21016   20953 #> 23    8_2_01_sixteenS 39233    30555     28274     28117  24130   23890 #> 24    8_3_01_sixteenS 36831    28050     25774     25665  21746   21521 #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #>          samplename_barcode  input filtered denoisedF denoisedR merged nonchim #> 1  Dung1_1_S221_L001_02_its 107502    78090     77364     77495  76532   76477 #> 2  Dung1_2_S222_L001_02_its  61061    45955     45480     45404  44826   44775 #> 3  Dung1_3_S223_L001_02_its  61239    42450     41879     41813  41241   41212 #> 4  Dung2_1_S224_L001_02_its  63088    47513     47053     46978  46443   46146 #> 5  Dung2_2_S225_L001_02_its  58055    37652     37262     37094  36763   36571 #> 6  Dung2_3_S226_L001_02_its  80809    50196     49877     49673  49327   49046 #> 7  Dung3_1_S227_L001_02_its  52495    33726     33220     33165  32710   32681 #> 8  Dung3_2_S228_L001_02_its  81501    48490     48003     47784  47112   47069 #> 9  Dung3_3_S229_L001_02_its 109828    82111     81638     81435  80706   80549 #> 10 Dung4_1_S230_L001_02_its  89140    65012     64527     64413  63826   63378 #> 11 Dung4_2_S231_L001_02_its  63345    50432     49931     49931  49621   49267 #> 12 Dung4_3_S232_L001_02_its  59459    44904     44543     44470  43962   43633 #> 13 Dung5_1_S233_L001_02_its  46202    35611     35150     35057  34455   34400 #> 14 Dung5_2_S234_L001_02_its  39533    29981     29579     29519  28736   28722 #> 15 Dung5_3_S235_L001_02_its  56309    35426     34922     34913  33960   33933 #> 16 Dung6_1_S236_L001_02_its  54091    40535     40136     40108  39776   39536 #> 17 Dung6_2_S237_L001_02_its  83307    37788     37413     37100  36836   36644 #> 18 Dung6_3_S238_L001_02_its  49829    25282     24998     24834  24650   24515 #> 19 Dung7_1_S239_L001_02_its  57541    24576     24220     23984  23672   23652 #> 20 Dung7_2_S240_L001_02_its  54654    31335     30784     30639  30099   30043 #> 21 Dung7_3_S241_L001_02_its  49902    37061     36566     36567  35951   35934 #> 22 Dung8_1_S242_L001_02_its  42973    30407     30060     30025  29757   29561 #> 23 Dung8_2_S243_L001_02_its  58646    42100     41795     41701  41280   41090 #> 24 Dung8_3_S244_L001_02_its  77154    57989     57602     57554  56900   56623 #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #>    samplename_barcode  input filtered denoisedF denoisedR merged nonchim #> 1  Dung1_1_S222_rps10  31012    20082     19999     20028  19680   19259 #> 2  Dung1_2_S223_rps10 117900    81794     81719     81690  77741   75601 #> 3  Dung1_3_S224_rps10 132855    86486     86438     86422  81094   78814 #> 4  Dung2_1_S225_rps10  96066    64939     64784     64631  60693   48037 #> 5  Dung2_2_S226_rps10 111356    79738     79519     79577  75768   64714 #> 6  Dung2_3_S227_rps10 159936   118224    117976    118170 113422  100408 #> 7  Dung3_1_S228_rps10  28596    19427     19389     19156  18957   18414 #> 8  Dung3_2_S229_rps10 110053    73752     73617     73544  70795   66612 #> 9  Dung3_3_S230_rps10  35693    23829     23784     23793  23206   22878 #> 10 Dung4_1_S231_rps10  94160    69370     69320     69327  66712   63797 #> 11 Dung4_2_S232_rps10 181941   134589    134530    134227 128424  114055 #> 12 Dung4_3_S233_rps10 120075    88315     88238     88236  85426   79149 #> 13 Dung5_1_S234_rps10  89019    43607     43478     43503  41456   38404 #> 14 Dung5_2_S235_rps10 104996    68342     68208     68192  65184   58908 #> 15 Dung5_3_S236_rps10  25501    14251     14203     14187  13965   13760 #> 16 Dung6_1_S237_rps10  24523    10633     10478     10606   9229    8739 #> 17 Dung6_2_S238_rps10 147048    99725     99704     99664  96759   94361 #> 18 Dung6_3_S239_rps10 114844    82697     82620     82402  78760   74217 #> 19 Dung7_1_S240_rps10  86521    58679     58432     58604  52961   49033 #> 20 Dung7_2_S241_rps10  86204    58661     58550     58608  52492   48440 #> 21 Dung7_3_S242_rps10  90932    46627     46444     46136  44849   43436 #> 22 Dung8_1_S243_rps10  95078    60360     60213     60329  58198   54921 #> 23 Dung8_2_S244_rps10  23896    14006     13876     13927  13004   12520 #> 24 Dung8_3_S245_rps10  36871    22369     22225     22326  21168   20016"},{"path":"/articles/02_mixed_barcode_example.html","id":"step-5-convert-asv-matrix-to-taxmap-and-phyloseq-objects-with-one-function","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 5-convert asv matrix to taxmap and phyloseq objects with one function","title":"Mixed barcode example","text":"","code":"objs<-convert_asv_matrix_to_objs(outputs, save_outputs=TRUE, overwrite_existing = TRUE) #> Rows: 841 Columns: 27 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (24): Dung1_1_S221_L001_02_its, Dung1_2_S222_L001_02_its, Dung1_3_S223_L... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: ~/output_multiple_datasets/obj_dada_its.RData  #> Phyloseq object saved in: ~/output_multiple_datasets/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 184 Columns: 27 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (24): Dung1_1_S222_rps10, Dung1_2_S223_rps10, Dung1_3_S224_rps10, Dung2_... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: ~/output_multiple_datasets/obj_dada_rps10.RData  #> Phyloseq object saved in: ~/output_multiple_datasets/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 4430 Columns: 27 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (24): 1_1_01_sixteenS, 1_2_01_sixteenS, 1_3_01_sixteenS, 2_1_01_sixteenS... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For sixteenS dataset  #> Taxmap object saved in: ~/output_multiple_datasets/obj_dada_sixteenS.RData  #> Phyloseq object saved in: ~/output_multiple_datasets/phylo_obj_sixteenS.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.1.2 (2021-11-01) #>  os       Pop!_OS 22.04 LTS #>  system   x86_64, linux-gnu #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Los_Angeles #>  date     2024-07-16 #>  pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  ! package              * version    date (UTC) lib source #>    ade4                   1.7-22     2023-02-06 [2] CRAN (R 4.1.2) #>    ape                    5.7-1      2023-03-13 [2] CRAN (R 4.1.2) #>    Biobase                2.54.0     2021-10-26 [2] Bioconductor #>    BiocGenerics           0.40.0     2021-10-26 [2] Bioconductor #>    BiocParallel           1.28.3     2021-12-09 [2] Bioconductor #>    biomformat             1.22.0     2021-10-26 [2] Bioconductor #>    Biostrings             2.62.0     2021-10-26 [2] Bioconductor #>    bit                    4.0.5      2022-11-15 [2] CRAN (R 4.1.2) #>    bit64                  4.0.5      2020-08-30 [4] CRAN (R 4.0.2) #>    bitops                 1.0-7      2021-04-24 [4] CRAN (R 4.1.1) #>    bslib                  0.7.0      2024-03-29 [2] CRAN (R 4.1.2) #>    cachem                 1.0.8      2023-05-01 [2] CRAN (R 4.1.2) #>    cli                    3.6.2      2023-12-11 [2] CRAN (R 4.1.2) #>    cluster                2.1.2      2021-04-17 [5] CRAN (R 4.1.1) #>    codetools              0.2-18     2020-11-04 [5] CRAN (R 4.0.3) #>    colorspace             2.1-0      2023-01-23 [2] CRAN (R 4.1.2) #>    crayon                 1.5.2      2022-09-29 [2] CRAN (R 4.1.2) #>    dada2                  1.30.0     2024-01-20 [2] bioc_xgit (@ec87892) #>    data.table             1.15.4     2024-03-30 [2] CRAN (R 4.1.2) #>    DBI                    1.2.2      2024-02-16 [2] CRAN (R 4.1.2) #>    DelayedArray           0.20.0     2021-10-26 [2] Bioconductor #>    deldir                 2.0-4      2024-02-28 [2] CRAN (R 4.1.2) #>  P demulticoder         * 0.0.0.9000 2024-04-08 [?] Github (grunwaldlab/demulticoder@2684e6b) #>    desc                   1.4.3      2023-12-10 [2] CRAN (R 4.1.2) #>    devtools               2.4.3      2021-11-30 [4] CRAN (R 4.1.2) #>    digest                 0.6.35     2024-03-11 [2] CRAN (R 4.1.2) #>    dplyr                  1.1.4      2023-11-17 [2] CRAN (R 4.1.2) #>    ellipsis               0.3.2      2021-04-29 [2] CRAN (R 4.1.2) #>    evaluate               0.23       2023-11-01 [2] CRAN (R 4.1.2) #>    fansi                  1.0.6      2023-12-08 [2] CRAN (R 4.1.2) #>    farver                 2.1.1      2022-07-06 [2] CRAN (R 4.1.2) #>    fastmap                1.1.1      2023-02-24 [2] CRAN (R 4.1.2) #>    foreach                1.5.2      2022-02-02 [4] CRAN (R 4.1.2) #>    fs                     1.6.3      2023-07-20 [2] CRAN (R 4.1.2) #>    furrr                  0.3.1      2022-08-15 [2] CRAN (R 4.1.2) #>    future                 1.33.2     2024-03-26 [2] CRAN (R 4.1.2) #>    generics               0.1.3      2022-07-05 [2] CRAN (R 4.1.2) #>    GenomeInfoDb           1.30.1     2022-01-30 [2] Bioconductor #>    GenomeInfoDbData       1.2.7      2024-01-20 [2] Bioconductor #>    GenomicAlignments      1.30.0     2021-10-26 [2] Bioconductor #>    GenomicRanges          1.46.1     2021-11-18 [2] Bioconductor #>    ggplot2                3.5.0      2024-02-23 [2] CRAN (R 4.1.2) #>    globals                0.16.3     2024-03-08 [2] CRAN (R 4.1.2) #>    glue                   1.7.0      2024-01-09 [2] CRAN (R 4.1.2) #>    gtable                 0.3.4      2023-08-21 [2] CRAN (R 4.1.2) #>    highr                  0.10       2022-12-22 [2] CRAN (R 4.1.2) #>    hms                    1.1.3      2023-03-21 [2] CRAN (R 4.1.2) #>    htmltools              0.5.8.1    2024-04-04 [2] CRAN (R 4.1.2) #>    hwriter                1.3.2.1    2022-04-08 [2] CRAN (R 4.1.2) #>    igraph                 2.0.3      2024-03-13 [2] CRAN (R 4.1.2) #>    interp                 1.1-6      2024-01-26 [2] CRAN (R 4.1.2) #>    IRanges                2.28.0     2021-10-26 [2] Bioconductor #>    iterators              1.0.14     2022-02-05 [4] CRAN (R 4.1.2) #>    jpeg                   0.1-10     2022-11-29 [2] CRAN (R 4.1.2) #>    jquerylib              0.1.4      2021-04-26 [2] CRAN (R 4.1.2) #>    jsonlite               1.8.8      2023-12-04 [2] CRAN (R 4.1.2) #>    knitr                  1.46       2024-04-06 [2] CRAN (R 4.1.2) #>    labeling               0.4.3      2023-08-29 [2] CRAN (R 4.1.2) #>    lattice                0.20-45    2021-09-22 [5] CRAN (R 4.1.1) #>    latticeExtra           0.6-30     2022-07-04 [2] CRAN (R 4.1.2) #>    lazyeval               0.2.2      2019-03-15 [4] CRAN (R 4.0.1) #>    lifecycle              1.0.4      2023-11-07 [2] CRAN (R 4.1.2) #>    listenv                0.9.1      2024-01-29 [2] CRAN (R 4.1.2) #>    magrittr               2.0.3      2022-03-30 [2] CRAN (R 4.1.2) #>    MASS                   7.3-55     2022-01-13 [5] CRAN (R 4.1.2) #>    Matrix                 1.4-0      2021-12-08 [5] CRAN (R 4.1.2) #>    MatrixGenerics         1.6.0      2021-10-26 [2] Bioconductor #>    matrixStats            1.2.0      2023-12-11 [2] CRAN (R 4.1.2) #>    memoise                2.0.1      2021-11-26 [2] CRAN (R 4.1.2) #>    metacoder              0.3.7      2024-02-20 [2] CRAN (R 4.1.2) #>    mgcv                   1.8-39     2022-02-24 [5] CRAN (R 4.1.2) #>    multtest               2.50.0     2021-10-26 [2] Bioconductor #>    munsell                0.5.1      2024-04-01 [2] CRAN (R 4.1.2) #>    nlme                   3.1-155    2022-01-13 [5] CRAN (R 4.1.2) #>    parallelly             1.37.1     2024-02-29 [2] CRAN (R 4.1.2) #>    permute                0.9-7      2022-01-27 [2] CRAN (R 4.1.2) #>    phyloseq               1.46.0     2024-01-20 [2] bioc_xgit (@7320133) #>    pillar                 1.9.0      2023-03-22 [2] CRAN (R 4.1.2) #>    pkgbuild               1.4.4      2024-03-17 [2] CRAN (R 4.1.2) #>    pkgconfig              2.0.3      2019-09-22 [2] CRAN (R 4.1.2) #>    pkgdown                2.0.7      2022-12-14 [2] CRAN (R 4.1.2) #>    pkgload                1.3.4      2024-01-16 [2] CRAN (R 4.1.2) #>    plyr                   1.8.9      2023-10-02 [2] CRAN (R 4.1.2) #>    png                    0.1-8      2022-11-29 [2] CRAN (R 4.1.2) #>    purrr                * 1.0.2      2023-08-10 [2] CRAN (R 4.1.2) #>    R6                     2.5.1      2021-08-19 [2] CRAN (R 4.1.2) #>    ragg                   1.2.1      2021-12-06 [4] CRAN (R 4.1.2) #>    RColorBrewer           1.1-3      2022-04-03 [2] CRAN (R 4.1.2) #>    Rcpp                   1.0.12     2024-01-09 [2] CRAN (R 4.1.2) #>    RcppParallel           5.1.7      2023-02-27 [2] CRAN (R 4.1.2) #>    RCurl                  1.98-1.14  2024-01-09 [2] CRAN (R 4.1.2) #>    readr                  2.1.5      2024-01-10 [2] CRAN (R 4.1.2) #>    remotes                2.5.0      2024-03-17 [2] CRAN (R 4.1.2) #>    reshape2               1.4.4      2020-04-09 [4] CRAN (R 4.0.1) #>    rhdf5                  2.38.1     2022-03-10 [2] Bioconductor #>    rhdf5filters           1.6.0      2021-10-26 [2] Bioconductor #>    Rhdf5lib               1.16.0     2021-10-26 [2] Bioconductor #>    rlang                  1.1.3      2024-01-10 [2] CRAN (R 4.1.2) #>    rmarkdown              2.26       2024-03-05 [2] CRAN (R 4.1.2) #>    rprojroot              2.0.4      2023-11-05 [2] CRAN (R 4.1.2) #>    Rsamtools              2.10.0     2021-10-26 [2] Bioconductor #>    rstudioapi             0.16.0     2024-03-24 [2] CRAN (R 4.1.2) #>    S4Vectors              0.32.4     2022-03-24 [2] Bioconductor #>    sass                   0.4.9      2024-03-15 [2] CRAN (R 4.1.2) #>    scales                 1.3.0      2023-11-28 [2] CRAN (R 4.1.2) #>    sessioninfo            1.2.2      2021-12-06 [2] CRAN (R 4.1.2) #>    ShortRead              1.60.0     2024-01-20 [2] bioc_xgit (@4304db4) #>    stringi                1.8.3      2023-12-11 [2] CRAN (R 4.1.2) #>    stringr                1.5.1      2023-11-14 [2] CRAN (R 4.1.2) #>    SummarizedExperiment   1.24.0     2021-10-26 [2] Bioconductor #>    survival               3.2-13     2021-08-24 [5] CRAN (R 4.1.1) #>    systemfonts            1.0.4      2022-02-11 [4] CRAN (R 4.1.2) #>    textshaping            0.3.6      2021-10-13 [4] CRAN (R 4.1.1) #>    tibble                 3.2.1      2023-03-20 [2] CRAN (R 4.1.2) #>    tidyr                  1.3.1      2024-01-24 [2] CRAN (R 4.1.2) #>    tidyselect             1.2.1      2024-03-11 [2] CRAN (R 4.1.2) #>    tzdb                   0.4.0      2023-05-12 [2] CRAN (R 4.1.2) #>    usethis                2.1.5      2021-12-09 [4] CRAN (R 4.1.2) #>    utf8                   1.2.4      2023-10-22 [2] CRAN (R 4.1.2) #>    vctrs                  0.6.5      2023-12-01 [2] CRAN (R 4.1.2) #>    vegan                  2.6-4      2022-10-11 [2] CRAN (R 4.1.2) #>    viridisLite            0.4.2      2023-05-02 [2] CRAN (R 4.1.2) #>    vroom                  1.6.5      2023-12-05 [2] CRAN (R 4.1.2) #>    withr                  3.0.0      2024-01-16 [2] CRAN (R 4.1.2) #>    xfun                   0.43       2024-03-25 [2] CRAN (R 4.1.2) #>    XVector                0.34.0     2021-10-26 [2] Bioconductor #>    yaml                   2.3.8      2023-12-11 [2] CRAN (R 4.1.2) #>    zlibbioc               1.40.0     2021-10-26 [2] Bioconductor #>  #>  [1] /tmp/Rtmp6UyrH0/temp_libpath28a0b146ec3112 #>  [2] /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1 #>  [3] /usr/local/lib/R/site-library #>  [4] /usr/lib/R/site-library #>  [5] /usr/lib/R/library #>  #>  P ── Loaded and on-disk path mismatch. #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"/articles/03_pooled_amplicon_example.html","id":"demonstration-of-how-to-use-demulticoder-on-a-dataset-that-has-pooled-amplicoins-rps10-and-its","dir":"Articles","previous_headings":"","what":"Demonstration of how to use demulticoder on a dataset that has pooled amplicoins (RPS10 and ITS)","title":"Pooled amplicon example","text":"TODO add instruction summary Input metadata primerinfo_params files data folder","code":""},{"path":"/articles/03_pooled_amplicon_example.html","id":"step-1-remove-ns-and-create-directory-structure-for-downstream-steps","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that has pooled amplicoins (RPS10 and ITS)","what":"Step 1-Remove N’s and create directory structure for downstream steps","title":"Pooled amplicon example","text":"","code":"outputs<-prepare_reads(   data_directory = \"~/demulticoder_benchmarking/demulticoder_combined_analysis_rhode_short/data\",    output_directory = \"~/output_pooled_amplicons\",    overwrite_existing = TRUE) #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 10 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (5): sample_name, primer_name, well, organism, sample_type #> dbl (3): plate, path_conc, experiment #> lgl (2): flooded, is_ambiguous #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Warning in get_read_names(paired_file_paths[1]) == #> get_read_names(paired_file_paths[2]): longer object length is not a multiple of #> shorter object length #> Creating output directory: /tmp/RtmpGXuOFp/demulticoder_run/prefiltered_sequences"},{"path":"/articles/03_pooled_amplicon_example.html","id":"step-2-run-cutadapt-to-remove-primers-and-then-trim-reads-with-dada2-filterandtrim-function","dir":"Articles","previous_headings":"","what":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","title":"Pooled amplicon example","text":"","code":"cut_trim(   outputs,   cutadapt_path=\"/usr/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 3.5 for its sequence data  #> Running Cutadapt 3.5 for rps10 sequence data"},{"path":"/articles/03_pooled_amplicon_example.html","id":"step-3-core-asv-inference-step","dir":"Articles","previous_headings":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","what":"Step 3-Core ASV inference step","title":"Pooled amplicon example","text":"","code":"make_asv_abund_matrix(   outputs,   overwrite_existing = TRUE) #> 13541233 total bases in 51241 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #> Convergence after  4  rounds. #> Error rate plot for the Forward read of primer pair its #> Sample 1 - 28895 reads in 8966 unique sequences. #> Sample 2 - 22346 reads in 8138 unique sequences. #> 13799140 total bases in 51241 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #> Convergence after  4  rounds. #> Error rate plot for the Reverse read of primer pair its #> Sample 1 - 28895 reads in 16283 unique sequences. #> Sample 2 - 22346 reads in 11924 unique sequences. #> 27737 paired-reads (in 90 unique pairings) successfully merged out of 28547 (in 118 pairings) input. #> Duplicate sequences in merged output. #> 21588 paired-reads (in 107 unique pairings) successfully merged out of 21929 (in 127 pairings) input. #> Duplicate sequences in merged output. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 159 input sequences. #> 15502045 total bases in 55165 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #> Convergence after  4  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 29365 reads in 13593 unique sequences. #> Sample 2 - 25800 reads in 12573 unique sequences. #> 15446994 total bases in 55165 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #> Convergence after  4  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 29365 reads in 16488 unique sequences. #> Sample 2 - 25800 reads in 18716 unique sequences. #> 29310 paired-reads (in 5 unique pairings) successfully merged out of 29339 (in 8 pairings) input. #> 25720 paired-reads (in 6 unique pairings) successfully merged out of 25757 (in 14 pairings) input. #> Duplicate sequences in merged output. #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 7 input sequences. #> $its #> [1] \"/tmp/RtmpGXuOFp/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/RtmpGXuOFp/demulticoder_run/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/03_pooled_amplicon_example.html","id":"step-4-assign-taxonomy-step","dir":"Articles","previous_headings":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","what":"Step 4-Assign taxonomy step","title":"Pooled amplicon example","text":"","code":"assign_tax(   outputs,   asv_abund_matrix,   db_its = \"sh_general_release_dynamic_all_25.07.2023.fasta\",   db_rps10 = \"oomycetedb.fasta\",   retrieve_files=TRUE,   overwrite_existing=FALSE) #> Warning in assign_tax(outputs, asv_abund_matrix, db_its = #> \"sh_general_release_dynamic_all_25.07.2023.fasta\", : No existing files found. #> The analysis will be run. #> Duplicate sequences detected and merged. #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its 50668    28895     28701     28594  27737   27737 #> 2             S2_its 35377    22346     22111     21992  21588   21588 #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10 36752    29365     29351     29346  29310   29310 #> 2           S2_rps10 35888    25800     25785     25766  25720   25720"},{"path":"/articles/03_pooled_amplicon_example.html","id":"step-5-convert-asv-matrix-to-taxmap-and-phyloseq-objects-with-one-function","dir":"Articles","previous_headings":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","what":"Step 5-convert asv matrix to taxmap and phyloseq objects with one function","title":"Pooled amplicon example","text":"TODO add final quick analysis","code":"objs<-convert_asv_matrix_to_objs(outputs, save_outputs=TRUE, overwrite_existing = TRUE) #> Rows: 159 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: ~/output_pooled_amplicons/obj_dada_its.RData  #> Phyloseq object saved in: ~/output_pooled_amplicons/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 7 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: ~/output_pooled_amplicons/obj_dada_rps10.RData  #> Phyloseq object saved in: ~/output_pooled_amplicons/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.1.2 (2021-11-01) #>  os       Pop!_OS 22.04 LTS #>  system   x86_64, linux-gnu #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Los_Angeles #>  date     2024-07-16 #>  pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  ! package              * version    date (UTC) lib source #>    ade4                   1.7-22     2023-02-06 [2] CRAN (R 4.1.2) #>    ape                    5.7-1      2023-03-13 [2] CRAN (R 4.1.2) #>    Biobase                2.54.0     2021-10-26 [2] Bioconductor #>    BiocGenerics           0.40.0     2021-10-26 [2] Bioconductor #>    BiocParallel           1.28.3     2021-12-09 [2] Bioconductor #>    biomformat             1.22.0     2021-10-26 [2] Bioconductor #>    Biostrings             2.62.0     2021-10-26 [2] Bioconductor #>    bit                    4.0.5      2022-11-15 [2] CRAN (R 4.1.2) #>    bit64                  4.0.5      2020-08-30 [4] CRAN (R 4.0.2) #>    bitops                 1.0-7      2021-04-24 [4] CRAN (R 4.1.1) #>    bslib                  0.7.0      2024-03-29 [2] CRAN (R 4.1.2) #>    cachem                 1.0.8      2023-05-01 [2] CRAN (R 4.1.2) #>    cli                    3.6.2      2023-12-11 [2] CRAN (R 4.1.2) #>    cluster                2.1.2      2021-04-17 [5] CRAN (R 4.1.1) #>    codetools              0.2-18     2020-11-04 [5] CRAN (R 4.0.3) #>    colorspace             2.1-0      2023-01-23 [2] CRAN (R 4.1.2) #>    crayon                 1.5.2      2022-09-29 [2] CRAN (R 4.1.2) #>    dada2                  1.30.0     2024-01-20 [2] bioc_xgit (@ec87892) #>    data.table             1.15.4     2024-03-30 [2] CRAN (R 4.1.2) #>    DBI                    1.2.2      2024-02-16 [2] CRAN (R 4.1.2) #>    DelayedArray           0.20.0     2021-10-26 [2] Bioconductor #>    deldir                 2.0-4      2024-02-28 [2] CRAN (R 4.1.2) #>  P demulticoder         * 0.0.0.9000 2024-04-08 [?] Github (grunwaldlab/demulticoder@2684e6b) #>    desc                   1.4.3      2023-12-10 [2] CRAN (R 4.1.2) #>    devtools               2.4.3      2021-11-30 [4] CRAN (R 4.1.2) #>    digest                 0.6.35     2024-03-11 [2] CRAN (R 4.1.2) #>    dplyr                  1.1.4      2023-11-17 [2] CRAN (R 4.1.2) #>    ellipsis               0.3.2      2021-04-29 [2] CRAN (R 4.1.2) #>    evaluate               0.23       2023-11-01 [2] CRAN (R 4.1.2) #>    fansi                  1.0.6      2023-12-08 [2] CRAN (R 4.1.2) #>    farver                 2.1.1      2022-07-06 [2] CRAN (R 4.1.2) #>    fastmap                1.1.1      2023-02-24 [2] CRAN (R 4.1.2) #>    foreach                1.5.2      2022-02-02 [4] CRAN (R 4.1.2) #>    fs                     1.6.3      2023-07-20 [2] CRAN (R 4.1.2) #>    furrr                  0.3.1      2022-08-15 [2] CRAN (R 4.1.2) #>    future                 1.33.2     2024-03-26 [2] CRAN (R 4.1.2) #>    generics               0.1.3      2022-07-05 [2] CRAN (R 4.1.2) #>    GenomeInfoDb           1.30.1     2022-01-30 [2] Bioconductor #>    GenomeInfoDbData       1.2.7      2024-01-20 [2] Bioconductor #>    GenomicAlignments      1.30.0     2021-10-26 [2] Bioconductor #>    GenomicRanges          1.46.1     2021-11-18 [2] Bioconductor #>    ggplot2                3.5.0      2024-02-23 [2] CRAN (R 4.1.2) #>    globals                0.16.3     2024-03-08 [2] CRAN (R 4.1.2) #>    glue                   1.7.0      2024-01-09 [2] CRAN (R 4.1.2) #>    gtable                 0.3.4      2023-08-21 [2] CRAN (R 4.1.2) #>    highr                  0.10       2022-12-22 [2] CRAN (R 4.1.2) #>    hms                    1.1.3      2023-03-21 [2] CRAN (R 4.1.2) #>    htmltools              0.5.8.1    2024-04-04 [2] CRAN (R 4.1.2) #>    hwriter                1.3.2.1    2022-04-08 [2] CRAN (R 4.1.2) #>    igraph                 2.0.3      2024-03-13 [2] CRAN (R 4.1.2) #>    interp                 1.1-6      2024-01-26 [2] CRAN (R 4.1.2) #>    IRanges                2.28.0     2021-10-26 [2] Bioconductor #>    iterators              1.0.14     2022-02-05 [4] CRAN (R 4.1.2) #>    jpeg                   0.1-10     2022-11-29 [2] CRAN (R 4.1.2) #>    jquerylib              0.1.4      2021-04-26 [2] CRAN (R 4.1.2) #>    jsonlite               1.8.8      2023-12-04 [2] CRAN (R 4.1.2) #>    knitr                  1.46       2024-04-06 [2] CRAN (R 4.1.2) #>    labeling               0.4.3      2023-08-29 [2] CRAN (R 4.1.2) #>    lattice                0.20-45    2021-09-22 [5] CRAN (R 4.1.1) #>    latticeExtra           0.6-30     2022-07-04 [2] CRAN (R 4.1.2) #>    lazyeval               0.2.2      2019-03-15 [4] CRAN (R 4.0.1) #>    lifecycle              1.0.4      2023-11-07 [2] CRAN (R 4.1.2) #>    listenv                0.9.1      2024-01-29 [2] CRAN (R 4.1.2) #>    magrittr               2.0.3      2022-03-30 [2] CRAN (R 4.1.2) #>    MASS                   7.3-55     2022-01-13 [5] CRAN (R 4.1.2) #>    Matrix                 1.4-0      2021-12-08 [5] CRAN (R 4.1.2) #>    MatrixGenerics         1.6.0      2021-10-26 [2] Bioconductor #>    matrixStats            1.2.0      2023-12-11 [2] CRAN (R 4.1.2) #>    memoise                2.0.1      2021-11-26 [2] CRAN (R 4.1.2) #>    metacoder              0.3.7      2024-02-20 [2] CRAN (R 4.1.2) #>    mgcv                   1.8-39     2022-02-24 [5] CRAN (R 4.1.2) #>    multtest               2.50.0     2021-10-26 [2] Bioconductor #>    munsell                0.5.1      2024-04-01 [2] CRAN (R 4.1.2) #>    nlme                   3.1-155    2022-01-13 [5] CRAN (R 4.1.2) #>    parallelly             1.37.1     2024-02-29 [2] CRAN (R 4.1.2) #>    permute                0.9-7      2022-01-27 [2] CRAN (R 4.1.2) #>    phyloseq               1.46.0     2024-01-20 [2] bioc_xgit (@7320133) #>    pillar                 1.9.0      2023-03-22 [2] CRAN (R 4.1.2) #>    pkgbuild               1.4.4      2024-03-17 [2] CRAN (R 4.1.2) #>    pkgconfig              2.0.3      2019-09-22 [2] CRAN (R 4.1.2) #>    pkgdown                2.0.7      2022-12-14 [2] CRAN (R 4.1.2) #>    pkgload                1.3.4      2024-01-16 [2] CRAN (R 4.1.2) #>    plyr                   1.8.9      2023-10-02 [2] CRAN (R 4.1.2) #>    png                    0.1-8      2022-11-29 [2] CRAN (R 4.1.2) #>    purrr                * 1.0.2      2023-08-10 [2] CRAN (R 4.1.2) #>    R6                     2.5.1      2021-08-19 [2] CRAN (R 4.1.2) #>    ragg                   1.2.1      2021-12-06 [4] CRAN (R 4.1.2) #>    RColorBrewer           1.1-3      2022-04-03 [2] CRAN (R 4.1.2) #>    Rcpp                   1.0.12     2024-01-09 [2] CRAN (R 4.1.2) #>    RcppParallel           5.1.7      2023-02-27 [2] CRAN (R 4.1.2) #>    RCurl                  1.98-1.14  2024-01-09 [2] CRAN (R 4.1.2) #>    readr                  2.1.5      2024-01-10 [2] CRAN (R 4.1.2) #>    remotes                2.5.0      2024-03-17 [2] CRAN (R 4.1.2) #>    reshape2               1.4.4      2020-04-09 [4] CRAN (R 4.0.1) #>    rhdf5                  2.38.1     2022-03-10 [2] Bioconductor #>    rhdf5filters           1.6.0      2021-10-26 [2] Bioconductor #>    Rhdf5lib               1.16.0     2021-10-26 [2] Bioconductor #>    rlang                  1.1.3      2024-01-10 [2] CRAN (R 4.1.2) #>    rmarkdown              2.26       2024-03-05 [2] CRAN (R 4.1.2) #>    rprojroot              2.0.4      2023-11-05 [2] CRAN (R 4.1.2) #>    Rsamtools              2.10.0     2021-10-26 [2] Bioconductor #>    rstudioapi             0.16.0     2024-03-24 [2] CRAN (R 4.1.2) #>    S4Vectors              0.32.4     2022-03-24 [2] Bioconductor #>    sass                   0.4.9      2024-03-15 [2] CRAN (R 4.1.2) #>    scales                 1.3.0      2023-11-28 [2] CRAN (R 4.1.2) #>    sessioninfo            1.2.2      2021-12-06 [2] CRAN (R 4.1.2) #>    ShortRead              1.60.0     2024-01-20 [2] bioc_xgit (@4304db4) #>    stringi                1.8.3      2023-12-11 [2] CRAN (R 4.1.2) #>    stringr                1.5.1      2023-11-14 [2] CRAN (R 4.1.2) #>    SummarizedExperiment   1.24.0     2021-10-26 [2] Bioconductor #>    survival               3.2-13     2021-08-24 [5] CRAN (R 4.1.1) #>    systemfonts            1.0.4      2022-02-11 [4] CRAN (R 4.1.2) #>    textshaping            0.3.6      2021-10-13 [4] CRAN (R 4.1.1) #>    tibble                 3.2.1      2023-03-20 [2] CRAN (R 4.1.2) #>    tidyr                  1.3.1      2024-01-24 [2] CRAN (R 4.1.2) #>    tidyselect             1.2.1      2024-03-11 [2] CRAN (R 4.1.2) #>    tzdb                   0.4.0      2023-05-12 [2] CRAN (R 4.1.2) #>    usethis                2.1.5      2021-12-09 [4] CRAN (R 4.1.2) #>    utf8                   1.2.4      2023-10-22 [2] CRAN (R 4.1.2) #>    vctrs                  0.6.5      2023-12-01 [2] CRAN (R 4.1.2) #>    vegan                  2.6-4      2022-10-11 [2] CRAN (R 4.1.2) #>    viridisLite            0.4.2      2023-05-02 [2] CRAN (R 4.1.2) #>    vroom                  1.6.5      2023-12-05 [2] CRAN (R 4.1.2) #>    withr                  3.0.0      2024-01-16 [2] CRAN (R 4.1.2) #>    xfun                   0.43       2024-03-25 [2] CRAN (R 4.1.2) #>    XVector                0.34.0     2021-10-26 [2] Bioconductor #>    yaml                   2.3.8      2023-12-11 [2] CRAN (R 4.1.2) #>    zlibbioc               1.40.0     2021-10-26 [2] Bioconductor #>  #>  [1] /tmp/Rtmp6UyrH0/temp_libpath28a0b146ec3112 #>  [2] /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1 #>  [3] /usr/local/lib/R/site-library #>  [4] /usr/lib/R/site-library #>  [5] /usr/lib/R/library #>  #>  P ── Loaded and on-disk path mismatch. #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"/articles/Package_Workflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Package Workflow","text":"demulticoder package Cutadapt DADA2 wrapper package metabarcoding analyses. package designed users varying experience metabarcoding analysis. Outputs intuitive comprehensive account iterative nature metabarcodeing analyses.","code":""},{"path":"/articles/Package_Workflow.html","id":"key-features","dir":"Articles","previous_headings":"Introduction","what":"Key features","title":"Package Workflow","text":"ability analysis either demultiplexed pooled amplicons within sample Multiple datasets can trimmed primers, filtered, denoised, merged, assigned taxonomy one go package handles just 16S datasets using default UNITE fungal Silva 16S databases also oomycete rps10 analyses using oomycetedb (oomycetedb.org), two custom databases (provided formatted described : https://benjjneb.github.io/dada2/training.html). package developed Martha Sudermann, Zachary Foster, Niklaus Grunwald, Jeff Chang.","code":""},{"path":"/articles/Package_Workflow.html","id":"before-you-start","dir":"Articles","previous_headings":"Introduction","what":"Before You Start","title":"Package Workflow","text":"following example, demonstrate key package functionality using subset reads two samples containing pooled ITS1 fungal rps10 oomycete amplicons. can follow along test data associated CSV input files loaded package. Additional examples also available website. Please note, speed, test dataset comprised randomly subset reads samples (S1 S2), due database size, full UNITE database included package, also smaller subset larger database. need prepare raw read files fill metadata.csv primerinfo_params.csv templates.","code":""},{"path":"/articles/Package_Workflow.html","id":"format-of-the-raw-read-files","dir":"Articles","previous_headings":"Introduction","what":"Format of the Raw Read Files","title":"Package Workflow","text":"package takes foward reverse Illumina short read sequence data. avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. final characters .fastq.gz suffix MUST **_R1** **_R2**. Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible renamed : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz parsing functions error R1 R2 directly preceeding ‘.fastq.gz’ suffix.","code":""},{"path":"/articles/Package_Workflow.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"Introduction","what":"Format of metadata file (metadata.csv)","title":"Package Workflow","text":"format CSV file simple. two necessary columns (names) : sample_name column primer_info column additional metadata pasted two columns. can referenced later analysis steps save step loading metadata later. S1 S2 come rhododendron rhizobiome dataset random subset reads. notice S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled reads (rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. Example using test dataset:","code":""},{"path":"/articles/Package_Workflow.html","id":"primer-sequence-information-and-user-defined-parameters-are-placed-in-primerinfo_params-csv","dir":"Articles","previous_headings":"Introduction","what":"Primer sequence information and user-defined parameters are placed in primerinfo_params.csv","title":"Package Workflow","text":"simplify functions called, user provide parameters within input file. recommend using template provided documentation. required columns user must fill : 1.primer_name (rps10, , another barcode) 2.forward-forward sequence 3.reverse-reverse sequence 4.already_trimmed (TRUE/FALSE) (datasets require primers first removed reads. However, 16S datasets, protocols like Earth Microbiome Project followed, primers mostly removed demultiplexing barcoded samples following Illumina run). may primers still remain. already_trimmed flag specified, remaining primers removed reads primers located copied trimmed folder subsequent analyses. user doesn’t add info subsequent columns, default DADA2 parameters used. TODO-provide info different parameter descriptions functions associated . Example template ‘primerinfo_params.csv’","code":""},{"path":"/articles/Package_Workflow.html","id":"reference-database-format","dir":"Articles","previous_headings":"Introduction","what":"Reference Database Format","title":"Package Workflow","text":"now, package compatible following databases: oomycetedb : http://www.oomycetedb.org/ SILVA 16S database species assignments: https://zenodo.org/records/4587955/files/silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1 UNITE fungal database https://unite.ut.ee/repository.php user can select one database (now), first need reformat headers exactly like UNITE fungal database specifications. Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function","code":""},{"path":"/articles/Package_Workflow.html","id":"additional-notes","dir":"Articles","previous_headings":"Introduction","what":"Additional Notes","title":"Package Workflow","text":"Computer specifications may limiting factor– using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases package randomly subsetted demonstration purposes. Users need upload databases input data folder. computer crashes taxonomic assignment step, please switch computing cluster. Please also ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/Package_Workflow.html","id":"loading-the-package","dir":"Articles","previous_headings":"Introduction","what":"Loading the Package","title":"Package Workflow","text":"now, package loaded retrieving GitHub. Eventually, package uploaded CRAN Bioconductor.","code":"#devtools::install_github(\"grunwaldlab/demulticoder\")  library(demulticoder)"},{"path":"/articles/Package_Workflow.html","id":"reorganize-data-tables","dir":"Articles","previous_headings":"Introduction","what":"Reorganize Data Tables","title":"Package Workflow","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    tempdir_id = \"test_dataset\",   overwrite_existing=TRUE)"},{"path":"/articles/Package_Workflow.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"Introduction","what":"Remove Primers with Cutadapt","title":"Package Workflow","text":"running Cutadapt, please ensure installed .","code":"demulticoder::cut_trim(   analysis_setup,   cutadapt_path=\"/opt/homebrew/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 4.1 for its sequence data  #> Running Cutadapt 4.1 for rps10 sequence data  #> Running Cutadapt 4.1 for sixteenS sequence data  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_1_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_1_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_2_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_2_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_3_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_3_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_1_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_1_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_2_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_2_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_3_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_3_R2_sixteenS.fastq.gz"},{"path":"/articles/Package_Workflow.html","id":"asv-inference-step","dir":"Articles","previous_headings":"Introduction","what":"ASV inference step","title":"Package Workflow","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = TRUE) #> 48620525 total bases in 193819 reads from 6 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ...... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #>    selfConsist step 7 #>    selfConsist step 8 #>    selfConsist step 9 #> Convergence after  9  rounds. #> Error rate plot for the Forward read of primer pair sixteenS #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 31044 reads in 16523 unique sequences. #> Sample 2 - 33424 reads in 17777 unique sequences. #> Sample 3 - 31303 reads in 17153 unique sequences. #> Sample 4 - 32148 reads in 15953 unique sequences. #> Sample 5 - 33116 reads in 17060 unique sequences. #> Sample 6 - 32784 reads in 17064 unique sequences. #> 48621032 total bases in 193819 reads from 6 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ...... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #> Convergence after  6  rounds. #> Error rate plot for the Reverse read of primer pair sixteenS #> Sample 1 - 31044 reads in 20471 unique sequences. #> Sample 2 - 33424 reads in 22029 unique sequences. #> Sample 3 - 31303 reads in 21177 unique sequences. #> Sample 4 - 32148 reads in 20174 unique sequences. #> Sample 5 - 33116 reads in 21307 unique sequences. #> Sample 6 - 32784 reads in 21009 unique sequences. #> 713433 total bases in 2703 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1486 reads in 659 unique sequences. #> Sample 2 - 1217 reads in 611 unique sequences. #> 726880 total bases in 2703 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1486 reads in 1026 unique sequences. #> Sample 2 - 1217 reads in 815 unique sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> $sixteenS #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/asvabund_matrixDADA2_sixteenS.RData\" #>  #> $its #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/Package_Workflow.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"Introduction","what":"Taxonomic assignment step","title":"Package Workflow","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing=TRUE) #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1       1_1_sixteenS 40437    31044     28831     28737  24286   23913 #> 2       1_2_sixteenS 44735    33424     31246     30900  25900   25448 #> 3       1_3_sixteenS 40114    31303     28922     29133  24504   24172 #> 4       2_1_sixteenS 39984    32148     29659     29631  25377   24748 #> 5       2_2_sixteenS 43128    33116     30536     30410  25790   25130 #> 6       2_3_sixteenS 43158    32784     30300     30131  25743   25121 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1486      1427      1433   1316    1316 #> 2             S2_its  1996     1217      1145      1124   1065    1065 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/Package_Workflow.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"Introduction","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Package Workflow","text":"","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, save_outputs=TRUE, overwrite=TRUE) #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> For sixteenS dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_sixteenS.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_sixteenS.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":[]},{"path":"/articles/Package_Workflow.html","id":"now-we-demonstrate-how-to-make-a-heattree-using-taxmap-object--first-we-make-a-heat-tree-for-our-its-barcoded-samples-1","dir":"Articles","previous_headings":"Introduction","what":"Now we demonstrate how to make a heattree using taxmap object. First we make a heat tree for our ITS-barcoded samples","title":"Package Workflow","text":"","code":"metacoder::heat_tree(objs$taxmap_its,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\")"},{"path":"/articles/Package_Workflow.html","id":"now-we-demonstrate-how-to-make-a-heattree-using-taxmap-object--first-we-make-a-heat-tree-for-our-its-barcoded-sample","dir":"Articles","previous_headings":"Introduction","what":"Now we demonstrate how to make a heattree using taxmap object. First we make a heat tree for our ITS-barcoded sample","title":"Package Workflow","text":"","code":"metacoder::heat_tree(objs$taxmap_rps10,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\")"},{"path":"/articles/Package_Workflow.html","id":"we-can-also-do-a-variety-of-analyses-if-we-convert-to-phyloseq-object","dir":"Articles","previous_headings":"Introduction","what":"We can also do a variety of analyses, if we convert to phyloseq object","title":"Package Workflow","text":"","code":"data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated.  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martha . Sudermann. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sudermann M (2024). demulticoder: R Package Integrated Analysis Multiplex Metabarcodes. R package version 0.0.0.9000, https://grunwaldlab.github.io/demulticoder/.","code":"@Manual{,   title = {demulticoder: An R Package for the Integrated Analysis of Multiplex Metabarcodes},   author = {Martha A. Sudermann},   year = {2024},   note = {R package version 0.0.0.9000},   url = {https://grunwaldlab.github.io/demulticoder/}, }"},{"path":"/index.html","id":"demulticoder-r-package","dir":"","previous_headings":"","what":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package actively development. message removed, use caution. Additional testing, documentation, examples progress.","code":""},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"demulticoder package Cutadapt DADA2 wrapper package metabarcodng analyses. main commands outputs intuitive comprehensive, helps account complex iterative nature metabarcoding analyses. brief schematic general workflow:","code":""},{"path":"/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key features","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"ability analysis either demultiplexed pooled amplicons within samples Amplicons multiple datasets trimmed primers, filtered, denoised, merged, given taxonomic assignments one go (different parameters dataset desired) package handles just 16S datasets using default UNITE fungal Silva 16S databases also oomycete rps10 analyses using oomycetedb (https://oomycetedb.org), two custom databases (provided formatted described : https://benjjneb.github.io/dada2/training.html).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"install development version package:","code":"devtools::install_github(\"grunwaldlab/demulticoder\")"},{"path":"/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick start","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"1. Set-input directory files installing package, make data directory add following files: - PE short read amplicon data. files must end either *_R1.fastq.gz* , *_R2.fastq.gz* sample must R1 R2 files. metadata.csv file (unique row sample, samples entered twice contain pooled amplicions, example template) primerinfo_params.csv file (new row unique barcode associated primer sequences, also optional Cutadapt, DADA2 filtering parameters can added adjusted) 2. Prepare reads 3. Cut trim reads 4. Make ASV abundance matrix 5. Assign taxonomy 6. Convert ASV matrix taxmap phyloseq objects","code":"output<-prepare_reads(   data_directory = \"<DATADIR>\",   output_directory = \"<OUTDIR>\") cut_trim(   output,   cutadapt_path=\"<CUTADAPTPATH>\") make_asv_abund_matrix(   output) assign_tax(   output,   asv_abund_matrix) objs<-convert_asv_matrix_to_objs(output)"},{"path":"/index.html","id":"check-out-the-website-to-view-the-documentation-and-see-more-examples","dir":"","previous_headings":"","what":"Check out the website to view the documentation and see more examples","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"information, key functions, inputs, example vignettes, check documentation : https://grunwaldlab.github.io/demulticoder","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package developed Martha Sudermann, Zachary Foster, Samantha Dawson, Hung Phan, Niklaus Grnwald, Jeff Chang. Stay tuned associated manuscript.","code":""},{"path":"/reference/add_pid_to_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Add PID and bootstrap values to tax result. — add_pid_to_tax","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"Add PID bootstrap values tax result.","code":""},{"path":"/reference/add_pid_to_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"","code":"add_pid_to_tax(tax_results, asv_pid)"},{"path":"/reference/add_pid_to_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"tax_results dataframe containing taxonomic assignments asv_pid Percent identity information ASV relative reference database sequence","code":""},{"path":"/reference/assignTax_as_char.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine taxonomic assignments and bootstrap values for each locus into single\nfalsification vector — assignTax_as_char","title":"Combine taxonomic assignments and bootstrap values for each locus into single\nfalsification vector — assignTax_as_char","text":"Combine taxonomic assignments bootstrap values locus single falsification vector","code":""},{"path":"/reference/assignTax_as_char.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine taxonomic assignments and bootstrap values for each locus into single\nfalsification vector — assignTax_as_char","text":"","code":"assignTax_as_char(tax_results, temp_directory_path, locus)"},{"path":"/reference/assignTax_as_char.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine taxonomic assignments and bootstrap values for each locus into single\nfalsification vector — assignTax_as_char","text":"tax_results dataframe containing taxonomic assignments","code":""},{"path":"/reference/assign_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy functions — assign_tax","title":"Assign taxonomy functions — assign_tax","text":"Assign taxonomy functions","code":""},{"path":"/reference/assign_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy functions — assign_tax","text":"","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   retrieve_files = FALSE,   overwrite_existing = FALSE,   db_rps10 = \"oomycetedb.fasta\",   db_its = \"fungidb.fasta\",   db_16s = \"bacteriadb.fasta\",   db_other1 = \"otherdb1.fasta\",   db_other2 = \"otherdb2.fasta\" )"},{"path":"/reference/assign_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy functions — assign_tax","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function asv_abund_matrix ASV abundance matrix. tryRC Whether try reverse complementing sequences taxonomic assignment verbose Logical, indicating whether display verbose output multithread Logical, indicating whether use multithreading retrieve_files Specify TRUE/FALSE whether copy files temp directory output directory overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. db_rps10 reference database rps10 locus db_its reference database locus db_16s reference database 16S locus db_other1 reference database different locus 1 (assumes format like SILVA DB entries) db_other2 reference database different locus 2 (assumes format like SILVA DB entries)","code":""},{"path":"/reference/assign_tax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign taxonomy functions — assign_tax","text":"Taxonomic assignments unique ASV sequence","code":""},{"path":"/reference/assign_tax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign taxonomy functions — assign_tax","text":"","code":"# Assign taxonomies to ASVs on a per barcode basis analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Warning: Existing analysis output table not found. The 'prepare_reads' function was rerun #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences #> Cutadapt input #>   sample_name samplename_barcode primer_name well organism #> 1          S1             S1_its         its  A01      Cry #> 2          S1             S1_its         its  A01      Cry #> 3          S1           S1_rps10       rps10  A01      Cry #> 4          S1           S1_rps10       rps10  A01      Cry #> 5          S2             S2_its         its  B01      Cin #> 6          S2             S2_its         its  B01      Cin #> 7          S2           S2_rps10       rps10  B01      Cin #> 8          S2           S2_rps10       rps10  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 5 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 6 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 7   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 8   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 5 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 6 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 7   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 8   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc file_id direction #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R1   Forward #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R2   Reverse #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R1   Forward #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R2   Reverse #> 5  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R1   Forward #> 6  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R2   Reverse #> 7 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R1   Forward #> 8 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 5 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 6 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #> 7 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 8 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #>                                                                   trimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   untrimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   filtered_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_rps10.fastq.gz #> Primer input #>   primer_name                forward                f_compt #> 1         its CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT #> 2       rps10   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA #>                    f_rev                   f_rc               reverse #> 1 AATGAAGGAGATTTACTGGTTC TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC #> 2   TCAGAARAYGAGATTGGTTG   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT #>                 r_compt                 r_rev                  r_rc #> 1  CGACGCAAGAAGTAGCTACG  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 2 TAYRRATCTTTCTRARCTTGA TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> Fastq input #>   file_id sample_name direction #> 1   S1_R1          S1   Forward #> 2   S1_R2          S1   Reverse #> 3   S2_R1          S2   Forward #> 4   S2_R2          S2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> Parameters input #> # A tibble: 2 × 20 #>   primer_name already_trimmed minCutadaptlength multithread verbose  maxN #>   <chr>       <lgl>                       <dbl> <lgl>       <lgl>   <dbl> #> 1 rps10       FALSE                         150 TRUE        TRUE        0 #> 2 its         FALSE                          50 TRUE        TRUE        0 #> # ℹ 14 more variables: maxEE_forward <dbl>, maxEE_reverse <dbl>, #> #   truncLen_forward <dbl>, truncLen_reverse <dbl>, truncQ <dbl>, minLen <dbl>, #> #   maxLen <dbl>, minQ <dbl>, trimLeft <dbl>, trimRight <dbl>, #> #   rm.lowcomplex <dbl>, minOverlap <dbl>, maxMismatch <dbl>, #> #   min_asv_length <dbl> #> Metadata input  #>   samplename_barcode primer_name sample_name well organism #> 1             S1_its         its          S1  A01      Cry #> 3           S1_rps10       rps10          S1  A01      Cry #> 2             S2_its         its          S2  B01      Cin #> 4           S2_rps10       rps10          S2  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Error in list.files(path = directory_path, pattern = pattern, full.names = TRUE,     recursive = TRUE): invalid 'path' argument make_asv_abund_matrix( analysis_setup,  overwrite_existing = FALSE ) #> Error in list.files(directory_path_temp, pattern = files_to_check, full.names = TRUE): invalid 'path' argument assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing=FALSE ) #> Error in list.files(directory_path_temp, pattern = files_to_check, full.names = TRUE): invalid 'path' argument"},{"path":"/reference/assign_taxonomyDada2.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy — assign_taxonomyDada2","title":"Assign taxonomy — assign_taxonomyDada2","text":"Assign taxonomy","code":""},{"path":"/reference/assign_taxonomyDada2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy — assign_taxonomyDada2","text":"","code":"assign_taxonomyDada2(   asv_abund_matrix,   temp_directory_path,   minBoot = 0,   tryRC = FALSE,   verbose = FALSE,   multithread = TRUE,   locus = barcode )"},{"path":"/reference/assign_taxonomyDada2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy — assign_taxonomyDada2","text":"asv_abund_matrix ASV abundance matrix minBoot (Optional). Default 50.  minimum bootstrap confidence assigning taxonomic level. tryRC (Optional). Default FALSE.  TRUE, reverse-complement sequences used classification better match reference sequences forward sequence. verbose (Optional). Default FALSE. TRUE, print status standard output. multithread (Optional). Default FALSE. TRUE, multithreading enabled number available threads automatically determined.    integer provided, number threads use set passing argument setThreadOptions. ref_database reference database used taxonomic inference steps","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"Filter ASV abundance matrix convert taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"","code":"convert_asv_matrix_to_objs(   analysis_setup,   min_read_depth = 0,   minimum_bootstrap = 0,   save_outputs = FALSE,   overwrite_existing = FALSE )"},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function min_read_depth ASV filter parameter. mean read depth across samples less threshold, ASV filtered. reads less value across samples minimum_bootstrap Threshold bootstrap support value taxonomic assignments. designated minimum bootstrap threshold, taxnomoic assignments set N/save_outputs Logical, indicating whether save taxmap object. Default FALSE. overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"ASV matrix converted taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"","code":"# Convert final matrix to taxmap and phyloseq objects for downstream analysis steps analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a  new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Cutadapt input #>   sample_name samplename_barcode primer_name well organism #> 1          S1             S1_its         its  A01      Cry #> 2          S1             S1_its         its  A01      Cry #> 3          S1           S1_rps10       rps10  A01      Cry #> 4          S1           S1_rps10       rps10  A01      Cry #> 5          S2             S2_its         its  B01      Cin #> 6          S2             S2_its         its  B01      Cin #> 7          S2           S2_rps10       rps10  B01      Cin #> 8          S2           S2_rps10       rps10  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 5 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 6 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 7   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 8   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 5 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 6 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 7   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 8   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc file_id direction #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R1   Forward #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R2   Reverse #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R1   Forward #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R2   Reverse #> 5  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R1   Forward #> 6  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R2   Reverse #> 7 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R1   Forward #> 8 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 5 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 6 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #> 7 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 8 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #>                                                                   trimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   untrimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   filtered_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_rps10.fastq.gz #> Primer input #>   primer_name                forward                f_compt #> 1         its CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT #> 2       rps10   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA #>                    f_rev                   f_rc               reverse #> 1 AATGAAGGAGATTTACTGGTTC TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC #> 2   TCAGAARAYGAGATTGGTTG   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT #>                 r_compt                 r_rev                  r_rc #> 1  CGACGCAAGAAGTAGCTACG  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 2 TAYRRATCTTTCTRARCTTGA TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> Fastq input #>   file_id sample_name direction #> 1   S1_R1          S1   Forward #> 2   S1_R2          S1   Reverse #> 3   S2_R1          S2   Forward #> 4   S2_R2          S2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> Parameters input #> # A tibble: 2 × 20 #>   primer_name already_trimmed minCutadaptlength multithread verbose  maxN #>   <chr>       <lgl>                       <dbl> <lgl>       <lgl>   <dbl> #> 1 rps10       FALSE                         150 TRUE        TRUE        0 #> 2 its         FALSE                          50 TRUE        TRUE        0 #> # ℹ 14 more variables: maxEE_forward <dbl>, maxEE_reverse <dbl>, #> #   truncLen_forward <dbl>, truncLen_reverse <dbl>, truncQ <dbl>, minLen <dbl>, #> #   maxLen <dbl>, minQ <dbl>, trimLeft <dbl>, trimRight <dbl>, #> #   rm.lowcomplex <dbl>, minOverlap <dbl>, maxMismatch <dbl>, #> #   min_asv_length <dbl> #> Metadata input  #>   samplename_barcode primer_name sample_name well organism #> 1             S1_its         its          S1  A01      Cry #> 3           S1_rps10       rps10          S1  A01      Cry #> 2             S2_its         its          S2  B01      Cin #> 4           S2_rps10       rps10          S2  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Error in list.files(path = directory_path, pattern = pattern, full.names = TRUE,     recursive = TRUE): invalid 'path' argument make_asv_abund_matrix( analysis_setup,  overwrite_existing = FALSE ) #> Error in list.files(directory_path_temp, pattern = files_to_check, full.names = TRUE): invalid 'path' argument assign_tax( analysis_setup, asv_abund_matrix, retrieve_files=FALSE,  overwrite_existing=FALSE ) #> Error in list.files(directory_path_temp, pattern = files_to_check, full.names = TRUE): invalid 'path' argument objs<-convert_asv_matrix_to_objs( analysis_setup,  save_outputs=FALSE )"},{"path":"/reference/countOverlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Count overlap to see how well the reads were merged — countOverlap","title":"Count overlap to see how well the reads were merged — countOverlap","text":"Count overlap see well reads merged","code":""},{"path":"/reference/countOverlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count overlap to see how well the reads were merged — countOverlap","text":"","code":"countOverlap(data_tables, merged_reads, barcode, output_directory_path)"},{"path":"/reference/countOverlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count overlap to see how well the reads were merged — countOverlap","text":"data_tables data tables containing paths read files, metadata, primer sequences merged_reads Intermediate merged read R data file barcode barcode used analysis output_directory_path path directory resulting files output","code":""},{"path":"/reference/countOverlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count overlap to see how well the reads were merged — countOverlap","text":"plot describing well reads merged information overlap reads","code":""},{"path":"/reference/createASVSequenceTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Make ASV sequence matrix — createASVSequenceTable","title":"Make ASV sequence matrix — createASVSequenceTable","text":"Make ASV sequence matrix","code":""},{"path":"/reference/createASVSequenceTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make ASV sequence matrix — createASVSequenceTable","text":"","code":"createASVSequenceTable(merged_reads, orderBy = \"abundance\")"},{"path":"/reference/createASVSequenceTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make ASV sequence matrix — createASVSequenceTable","text":"merged_reads Intermediate merged read R data file orderBy (Optional). character(1). Default \"abundance\". Specifies sequences (columns) returned table ordered (decreasing). Valid values: \"abundance\", \"nsamples\", NULL.","code":""},{"path":"/reference/createASVSequenceTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make ASV sequence matrix — createASVSequenceTable","text":"raw_seqtab","code":""},{"path":"/reference/cut_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Main command to trim primers using Cutadapt and core DADA2 functions. If\nsamples contain pooled barcodes, reads will also be demultiplexed — cut_trim","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If\nsamples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"Main command trim primers using Cutadapt core DADA2 functions. samples contain pooled barcodes, reads also demultiplexed","code":""},{"path":"/reference/cut_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If\nsamples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"","code":"cut_trim(analysis_setup, cutadapt_path, overwrite_existing = FALSE)"},{"path":"/reference/cut_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If\nsamples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function cutadapt_path Path Cutadapt program. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/cut_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If\nsamples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"Trimmed reads, primer counts, quality plots, ASV matrix.","code":""},{"path":"/reference/cut_trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If\nsamples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"","code":"# Remove remaining primers from raw reads, demultiplex pooled barcoded samples,  # and then trim reads based on specific DADA2 parameters analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a  new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Cutadapt input #>   sample_name samplename_barcode primer_name well organism #> 1          S1             S1_its         its  A01      Cry #> 2          S1             S1_its         its  A01      Cry #> 3          S1           S1_rps10       rps10  A01      Cry #> 4          S1           S1_rps10       rps10  A01      Cry #> 5          S2             S2_its         its  B01      Cin #> 6          S2             S2_its         its  B01      Cin #> 7          S2           S2_rps10       rps10  B01      Cin #> 8          S2           S2_rps10       rps10  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 5 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 6 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 7   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 8   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 5 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 6 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 7   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 8   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc file_id direction #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R1   Forward #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R2   Reverse #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R1   Forward #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R2   Reverse #> 5  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R1   Forward #> 6  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R2   Reverse #> 7 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R1   Forward #> 8 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 5 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 6 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #> 7 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 8 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #>                                                                   trimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   untrimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   filtered_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_rps10.fastq.gz #> Primer input #>   primer_name                forward                f_compt #> 1         its CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT #> 2       rps10   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA #>                    f_rev                   f_rc               reverse #> 1 AATGAAGGAGATTTACTGGTTC TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC #> 2   TCAGAARAYGAGATTGGTTG   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT #>                 r_compt                 r_rev                  r_rc #> 1  CGACGCAAGAAGTAGCTACG  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 2 TAYRRATCTTTCTRARCTTGA TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> Fastq input #>   file_id sample_name direction #> 1   S1_R1          S1   Forward #> 2   S1_R2          S1   Reverse #> 3   S2_R1          S2   Forward #> 4   S2_R2          S2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> Parameters input #> # A tibble: 2 × 20 #>   primer_name already_trimmed minCutadaptlength multithread verbose  maxN #>   <chr>       <lgl>                       <dbl> <lgl>       <lgl>   <dbl> #> 1 rps10       FALSE                         150 TRUE        TRUE        0 #> 2 its         FALSE                          50 TRUE        TRUE        0 #> # ℹ 14 more variables: maxEE_forward <dbl>, maxEE_reverse <dbl>, #> #   truncLen_forward <dbl>, truncLen_reverse <dbl>, truncQ <dbl>, minLen <dbl>, #> #   maxLen <dbl>, minQ <dbl>, trimLeft <dbl>, trimRight <dbl>, #> #   rm.lowcomplex <dbl>, minOverlap <dbl>, maxMismatch <dbl>, #> #   min_asv_length <dbl> #> Metadata input  #>   samplename_barcode primer_name sample_name well organism #> 1             S1_its         its          S1  A01      Cry #> 3           S1_rps10       rps10          S1  A01      Cry #> 2             S2_its         its          S2  B01      Cin #> 4           S2_rps10       rps10          S2  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Error in list.files(path = directory_path, pattern = pattern, full.names = TRUE,     recursive = TRUE): invalid 'path' argument"},{"path":"/reference/filter_and_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for filterAndTrim function from DADA2, to be used after\nprimer trimming — filter_and_trim","title":"Wrapper function for filterAndTrim function from DADA2, to be used after\nprimer trimming — filter_and_trim","text":"Wrapper function filterAndTrim function DADA2, used primer trimming","code":""},{"path":"/reference/filter_and_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for filterAndTrim function from DADA2, to be used after\nprimer trimming — filter_and_trim","text":"","code":"filter_and_trim(   output_directory_path,   temp_directory_path,   cutadapt_data_barcode,   barcode_params,   barcode )"},{"path":"/reference/filter_and_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for filterAndTrim function from DADA2, to be used after\nprimer trimming — filter_and_trim","text":"output_directory_path path directory resulting files output cutadapt_data_barcode directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/filter_and_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for filterAndTrim function from DADA2, to be used after\nprimer trimming — filter_and_trim","text":"Filtered trimmed reads","code":""},{"path":"/reference/format_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Format ASV abundance matrix — format_abund_matrix","title":"Format ASV abundance matrix — format_abund_matrix","text":"Format ASV abundance matrix","code":""},{"path":"/reference/format_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format ASV abundance matrix — format_abund_matrix","text":"","code":"format_abund_matrix(   data_tables,   asv_abund_matrix,   seq_tax_asv,   output_directory_path,   locus )"},{"path":"/reference/format_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format ASV abundance matrix — format_abund_matrix","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants seq_tax_asv amplified sequence variants matrix taxonomic information","code":""},{"path":"/reference/format_database.html","id":null,"dir":"Reference","previous_headings":"","what":"General functions to format user-specified databases — format_database","title":"General functions to format user-specified databases — format_database","text":"General functions format user-specified databases","code":""},{"path":"/reference/format_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General functions to format user-specified databases — format_database","text":"","code":"format_database(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   barcode,   db_its,   db_rps10,   db_16s,   db_other1,   db_other2 )"},{"path":"/reference/format_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General functions to format user-specified databases — format_database","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files barcode barcode database formatted","code":""},{"path":"/reference/format_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General functions to format user-specified databases — format_database","text":"formatted database based specified barcode type","code":""},{"path":"/reference/format_db_16s.html","id":null,"dir":"Reference","previous_headings":"","what":"An 16s database that has modified headers and is output in the\nreference_databases folder — format_db_16s","title":"An 16s database that has modified headers and is output in the\nreference_databases folder — format_db_16s","text":"16s database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_16s.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An 16s database that has modified headers and is output in the\nreference_databases folder — format_db_16s","text":"","code":"format_db_16s(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_16s )"},{"path":"/reference/format_db_16s.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An 16s database that has modified headers and is output in the\nreference_databases folder — format_db_16s","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_16s name database","code":""},{"path":"/reference/format_db_16s.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An 16s database that has modified headers and is output in the\nreference_databases folder — format_db_16s","text":"16s database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":null,"dir":"Reference","previous_headings":"","what":"An ITS database that has modified headers and is output in the\nreference_databases folder — format_db_its","title":"An ITS database that has modified headers and is output in the\nreference_databases folder — format_db_its","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An ITS database that has modified headers and is output in the\nreference_databases folder — format_db_its","text":"","code":"format_db_its(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_its )"},{"path":"/reference/format_db_its.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An ITS database that has modified headers and is output in the\nreference_databases folder — format_db_its","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_its name database","code":""},{"path":"/reference/format_db_its.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An ITS database that has modified headers and is output in the\nreference_databases folder — format_db_its","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other1.html","id":null,"dir":"Reference","previous_headings":"","what":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":", user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"","code":"format_db_other1(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other1 )"},{"path":"/reference/format_db_other1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other1 name database","code":""},{"path":"/reference/format_db_other1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other2.html","id":null,"dir":"Reference","previous_headings":"","what":"An second user-specified database that is initially in the format specified\nby DADA2 with header simply taxonomic levels (kingdom down to species,\nseparated by semi-colons, ;) — format_db_other2","title":"An second user-specified database that is initially in the format specified\nby DADA2 with header simply taxonomic levels (kingdom down to species,\nseparated by semi-colons, ;) — format_db_other2","text":"second user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An second user-specified database that is initially in the format specified\nby DADA2 with header simply taxonomic levels (kingdom down to species,\nseparated by semi-colons, ;) — format_db_other2","text":"","code":"format_db_other2(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other2 )"},{"path":"/reference/format_db_other2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An second user-specified database that is initially in the format specified\nby DADA2 with header simply taxonomic levels (kingdom down to species,\nseparated by semi-colons, ;) — format_db_other2","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other2 name database","code":""},{"path":"/reference/format_db_other2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An second user-specified database that is initially in the format specified\nby DADA2 with header simply taxonomic levels (kingdom down to species,\nseparated by semi-colons, ;) — format_db_other2","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_rps10.html","id":null,"dir":"Reference","previous_headings":"","what":"Create modified reference rps10 database for downstream analysis — format_db_rps10","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"Create modified reference rps10 database downstream analysis","code":""},{"path":"/reference/format_db_rps10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"","code":"format_db_rps10(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_rps10 )"},{"path":"/reference/format_db_rps10.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_rps10 name database","code":""},{"path":"/reference/format_db_rps10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"rps10 database modified headers output reference_databases folder.","code":""},{"path":"/reference/get_fastq_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"Retrieve paths filtered trimmed Fastq files","code":""},{"path":"/reference/get_fastq_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"","code":"get_fastq_paths(data_tables, my_direction, my_primer_pair_id)"},{"path":"/reference/get_fastq_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Whether primer forward reverse direction my_primer_pair_id specific barcode id cutadapt_data directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/get_pids.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID.\nGet percent identities. — get_pids","title":"Align ASV sequences to reference sequences from database to get percent ID.\nGet percent identities. — get_pids","text":"Align ASV sequences reference sequences database get percent ID. Get percent identities.","code":""},{"path":"/reference/get_pids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID.\nGet percent identities. — get_pids","text":"","code":"get_pids(tax_results, temp_directory_path, output_directory_path, db, locus)"},{"path":"/reference/get_pids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID.\nGet percent identities. — get_pids","text":"tax_results data frame containing taxonomic assignments","code":""},{"path":"/reference/get_post_trim_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_post_trim_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"","code":"get_post_trim_hits(primer_data, cutadapt_data, output_directory_path)"},{"path":"/reference/get_post_trim_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"primer_data primer data frame created orient_primers function cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_post_trim_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Table read counts across sample","code":""},{"path":"/reference/get_pre_primer_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"","code":"get_pre_primer_hits(primer_data, fastq_data, output_directory_path)"},{"path":"/reference/get_pre_primer_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"primer_data primer data data frame created orient_primers function fastq_data data frame FASTQ file paths, direction sequences, names sequences output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"number reads primer found","code":""},{"path":"/reference/get_read_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"Final inventory read counts step input removal chimeras. function deals one sample. TODO optimize one sample","code":""},{"path":"/reference/get_read_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"","code":"get_read_counts(   asv_abund_matrix,   temp_directory_path,   output_directory_path,   locus )"},{"path":"/reference/get_read_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"asv_abund_matrix abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/get_ref_seq.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID.\nStart by retrieving reference sequences. — get_ref_seq","title":"Align ASV sequences to reference sequences from database to get percent ID.\nStart by retrieving reference sequences. — get_ref_seq","text":"Align ASV sequences reference sequences database get percent ID. Start retrieving reference sequences.","code":""},{"path":"/reference/get_ref_seq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID.\nStart by retrieving reference sequences. — get_ref_seq","text":"","code":"get_ref_seq(tax_results, db)"},{"path":"/reference/get_ref_seq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID.\nStart by retrieving reference sequences. — get_ref_seq","text":"tax_results dataframe containing taxonomic assignments db reference database","code":""},{"path":"/reference/infer_asv_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to infer ASVs, for multiple loci — infer_asv_command","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"Function infer ASVs, multiple loci","code":""},{"path":"/reference/infer_asv_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"","code":"infer_asv_command(   output_directory_path,   temp_directory_path,   data_tables,   barcode_params,   barcode )"},{"path":"/reference/infer_asv_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"output_directory_path path directory resulting files output data_tables data tables containing paths read files, metadata, primer sequences denoised_data_path Path saved intermediate denoised data","code":""},{"path":"/reference/infer_asvs.html","id":null,"dir":"Reference","previous_headings":"","what":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"Core DADA2 function learn errors infer ASVs","code":""},{"path":"/reference/infer_asvs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"","code":"infer_asvs(   data_tables,   my_direction,   my_primer_pair_id,   barcode_params,   output_directory_path )"},{"path":"/reference/infer_asvs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Location read files metadata file my_primer_pair_id specific barcode id output_directory_path path directory containing fastq, metadata, primerinfo_params files","code":""},{"path":"/reference/infer_asvs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"asv_data","code":""},{"path":"/reference/make_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"Quality filtering remove chimeras short sequences","code":""},{"path":"/reference/make_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"","code":"make_abund_matrix(   raw_seqtab,   temp_directory_path,   barcode_params = barcode_params,   barcode )"},{"path":"/reference/make_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"raw_seqtab R data file raw sequence data prior removal chimeras","code":""},{"path":"/reference/make_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make an amplified sequence variant (ASV) abundance matrix This function\ngenerates an ASV abundance matrix using raw reads processed during previous\nsteps, including read preparation, removing primers, and using DADA2 core\ndenoising alogrithm to infer ASVs. — make_asv_abund_matrix","title":"Make an amplified sequence variant (ASV) abundance matrix This function\ngenerates an ASV abundance matrix using raw reads processed during previous\nsteps, including read preparation, removing primers, and using DADA2 core\ndenoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"Make amplified sequence variant (ASV) abundance matrix function generates ASV abundance matrix using raw reads processed previous steps, including read preparation, removing primers, using DADA2 core denoising alogrithm infer ASVs.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make an amplified sequence variant (ASV) abundance matrix This function\ngenerates an ASV abundance matrix using raw reads processed during previous\nsteps, including read preparation, removing primers, and using DADA2 core\ndenoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"","code":"make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE)"},{"path":"/reference/make_asv_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make an amplified sequence variant (ASV) abundance matrix This function\ngenerates an ASV abundance matrix using raw reads processed during previous\nsteps, including read preparation, removing primers, and using DADA2 core\ndenoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make an amplified sequence variant (ASV) abundance matrix This function\ngenerates an ASV abundance matrix using raw reads processed during previous\nsteps, including read preparation, removing primers, and using DADA2 core\ndenoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"ASV abundance matrix (asv_abund_matrix)","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make an amplified sequence variant (ASV) abundance matrix This function\ngenerates an ASV abundance matrix using raw reads processed during previous\nsteps, including read preparation, removing primers, and using DADA2 core\ndenoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"function processes data unique barcode separately, inferring ASVs, merging reads, creating ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make an amplified sequence variant (ASV) abundance matrix This function\ngenerates an ASV abundance matrix using raw reads processed during previous\nsteps, including read preparation, removing primers, and using DADA2 core\ndenoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"","code":"# The primary wrapper function for DADA2 ASV inference steps analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a  new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Cutadapt input #>   sample_name samplename_barcode primer_name well organism #> 1          S1             S1_its         its  A01      Cry #> 2          S1             S1_its         its  A01      Cry #> 3          S1           S1_rps10       rps10  A01      Cry #> 4          S1           S1_rps10       rps10  A01      Cry #> 5          S2             S2_its         its  B01      Cin #> 6          S2             S2_its         its  B01      Cin #> 7          S2           S2_rps10       rps10  B01      Cin #> 8          S2           S2_rps10       rps10  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 5 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 6 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 7   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 8   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 5 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 6 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 7   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 8   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc file_id direction #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R1   Forward #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R2   Reverse #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R1   Forward #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R2   Reverse #> 5  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R1   Forward #> 6  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R2   Reverse #> 7 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R1   Forward #> 8 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 5 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 6 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #> 7 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 8 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #>                                                                   trimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   untrimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   filtered_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_rps10.fastq.gz #> Primer input #>   primer_name                forward                f_compt #> 1         its CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT #> 2       rps10   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA #>                    f_rev                   f_rc               reverse #> 1 AATGAAGGAGATTTACTGGTTC TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC #> 2   TCAGAARAYGAGATTGGTTG   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT #>                 r_compt                 r_rev                  r_rc #> 1  CGACGCAAGAAGTAGCTACG  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 2 TAYRRATCTTTCTRARCTTGA TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> Fastq input #>   file_id sample_name direction #> 1   S1_R1          S1   Forward #> 2   S1_R2          S1   Reverse #> 3   S2_R1          S2   Forward #> 4   S2_R2          S2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> Parameters input #> # A tibble: 2 × 20 #>   primer_name already_trimmed minCutadaptlength multithread verbose  maxN #>   <chr>       <lgl>                       <dbl> <lgl>       <lgl>   <dbl> #> 1 rps10       FALSE                         150 TRUE        TRUE        0 #> 2 its         FALSE                          50 TRUE        TRUE        0 #> # ℹ 14 more variables: maxEE_forward <dbl>, maxEE_reverse <dbl>, #> #   truncLen_forward <dbl>, truncLen_reverse <dbl>, truncQ <dbl>, minLen <dbl>, #> #   maxLen <dbl>, minQ <dbl>, trimLeft <dbl>, trimRight <dbl>, #> #   rm.lowcomplex <dbl>, minOverlap <dbl>, maxMismatch <dbl>, #> #   min_asv_length <dbl> #> Metadata input  #>   samplename_barcode primer_name sample_name well organism #> 1             S1_its         its          S1  A01      Cry #> 3           S1_rps10       rps10          S1  A01      Cry #> 2             S2_its         its          S2  B01      Cin #> 4           S2_rps10       rps10          S2  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Error in list.files(path = directory_path, pattern = pattern, full.names = TRUE,     recursive = TRUE): invalid 'path' argument make_asv_abund_matrix( analysis_setup,  overwrite_existing = FALSE ) #> Error in list.files(directory_path_temp, pattern = files_to_check, full.names = TRUE): invalid 'path' argument"},{"path":"/reference/make_cutadapt_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and\nspecify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and\nspecify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Prepare primmer trimming Cutaapt. Make new sub-directories specify paths trimmed untrimmed reads","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and\nspecify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"","code":"make_cutadapt_tibble(fastq_data, metadata, temp_directory_path)"},{"path":"/reference/make_cutadapt_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and\nspecify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"fastq_data path FASTQ files analysis metadata, primer_info files metadata Loaded metadata pairing user's metadata file primer data temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and\nspecify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Returns larger data frame containing paths temporary read directories, used input running Cutadapt","code":""},{"path":"/reference/make_seqhist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a histogram of read length counts of all sequences within the ASV\nmatrix — make_seqhist","title":"Plots a histogram of read length counts of all sequences within the ASV\nmatrix — make_seqhist","text":"Plots histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/make_seqhist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a histogram of read length counts of all sequences within the ASV\nmatrix — make_seqhist","text":"","code":"make_seqhist(asv_abund_matrix, output_directory_path)"},{"path":"/reference/make_seqhist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a histogram of read length counts of all sequences within the ASV\nmatrix — make_seqhist","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_seqhist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a histogram of read length counts of all sequences within the ASV\nmatrix — make_seqhist","text":"histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/merge_reads_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge forward and reverse reads — merge_reads_command","title":"Merge forward and reverse reads — merge_reads_command","text":"Merge forward reverse reads","code":""},{"path":"/reference/merge_reads_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge forward and reverse reads — merge_reads_command","text":"","code":"merge_reads_command(   output_directory_path,   temp_directory_path,   barcode_params,   barcode )"},{"path":"/reference/merge_reads_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge forward and reverse reads — merge_reads_command","text":"output_directory_path path directory resulting files output merged_read_data_path Path R data file containing merged read data","code":""},{"path":"/reference/merge_reads_command.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge forward and reverse reads — merge_reads_command","text":"merged_reads Intermediate merged read R data file","code":""},{"path":"/reference/orient_primers.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's forward and reverse sequences and creates the complement,\nreverse, reverse complement of primers in one data frame — orient_primers","title":"Take in user's forward and reverse sequences and creates the complement,\nreverse, reverse complement of primers in one data frame — orient_primers","text":"Take user's forward reverse sequences creates complement, reverse, reverse complement primers one data frame","code":""},{"path":"/reference/orient_primers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's forward and reverse sequences and creates the complement,\nreverse, reverse complement of primers in one data frame — orient_primers","text":"","code":"orient_primers(primers_params_path)"},{"path":"/reference/orient_primers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's forward and reverse sequences and creates the complement,\nreverse, reverse complement of primers in one data frame — orient_primers","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/orient_primers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's forward and reverse sequences and creates the complement,\nreverse, reverse complement of primers in one data frame — orient_primers","text":"data frame oriented primer information.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Wrapper script plotQualityProfile trim steps primer removal.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"","code":"plot_post_trim_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_post_trim_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Quality profiles reads primer trimming","code":""},{"path":"/reference/plot_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for plotQualityProfile function — plot_qc","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Wrapper function plotQualityProfile function","code":""},{"path":"/reference/plot_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"","code":"plot_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Dada2 wrapper function making quality profiles sample","code":""},{"path":"/reference/prep_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare final ASV abundance matrix — prep_abund_matrix","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"Prepare final ASV abundance matrix","code":""},{"path":"/reference/prep_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"","code":"prep_abund_matrix(cutadapt_data, asv_abund_matrix, data_tables, locus)"},{"path":"/reference/prep_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix locus barcode selected analysis directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/prepare_metadata_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Read metadata file from user and combine and reformat it, given primer data.\nIncluded in a larger function prepare_reads. — prepare_metadata_table","title":"Read metadata file from user and combine and reformat it, given primer data.\nIncluded in a larger function prepare_reads. — prepare_metadata_table","text":"Read metadata file user combine reformat , given primer data. Included larger function prepare_reads.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read metadata file from user and combine and reformat it, given primer data.\nIncluded in a larger function prepare_reads. — prepare_metadata_table","text":"","code":"prepare_metadata_table(metadata_file_path, primer_data)"},{"path":"/reference/prepare_metadata_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read metadata file from user and combine and reformat it, given primer data.\nIncluded in a larger function prepare_reads. — prepare_metadata_table","text":"primer_data data frame oriented primer information returned orient_primers function. metadata_path path metadata file.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read metadata file from user and combine and reformat it, given primer data.\nIncluded in a larger function prepare_reads. — prepare_metadata_table","text":"dataframe containing merged metadata primer data.","code":""},{"path":"/reference/prepare_reads.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare reads for primer trimming using Cutadapt — prepare_reads","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"Prepare reads primer trimming using Cutadapt","code":""},{"path":"/reference/prepare_reads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"prepare_reads(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\",   multithread = FALSE,   overwrite_existing = FALSE )"},{"path":"/reference/prepare_reads.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run. multithread Logical, indicating whether use multithreading certain operations. Default FALSE. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/prepare_reads.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"list containing data tables, including metadata, primer sequences search based orientation, paths trimming reads, user-defined parameters subsequent steps.","code":""},{"path":"/reference/prepare_reads.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"# Pre-filter raw reads and parse metadata and primer_information to prepare  # for primer trimming and filter analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a  new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Cutadapt input #>   sample_name samplename_barcode primer_name well organism #> 1          S1             S1_its         its  A01      Cry #> 2          S1             S1_its         its  A01      Cry #> 3          S1           S1_rps10       rps10  A01      Cry #> 4          S1           S1_rps10       rps10  A01      Cry #> 5          S2             S2_its         its  B01      Cin #> 6          S2             S2_its         its  B01      Cin #> 7          S2           S2_rps10       rps10  B01      Cin #> 8          S2           S2_rps10       rps10  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 5 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 6 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 7   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 8   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 5 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 6 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 7   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 8   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc file_id direction #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R1   Forward #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S1_R2   Reverse #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R1   Forward #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S1_R2   Reverse #> 5  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R1   Forward #> 6  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC   S2_R2   Reverse #> 7 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R1   Forward #> 8 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT   S2_R2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 5 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 6 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #> 7 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 8 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 5 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 6 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #>                                                                   trimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/trimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   untrimmed_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/untrimmed_sequences/S2_R2_rps10.fastq.gz #>                                                                   filtered_path #> 1   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_its.fastq.gz #> 2   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_its.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R1_rps10.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S1_R2_rps10.fastq.gz #> 5   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_its.fastq.gz #> 6   /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_its.fastq.gz #> 7 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R1_rps10.fastq.gz #> 8 /tmp/RtmpGpvMZP/demulticoder_run_temp/filtered_sequences/S2_R2_rps10.fastq.gz #> Primer input #>   primer_name                forward                f_compt #> 1         its CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT #> 2       rps10   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA #>                    f_rev                   f_rc               reverse #> 1 AATGAAGGAGATTTACTGGTTC TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC #> 2   TCAGAARAYGAGATTGGTTG   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT #>                 r_compt                 r_rev                  r_rc #> 1  CGACGCAAGAAGTAGCTACG  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 2 TAYRRATCTTTCTRARCTTGA TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> Fastq input #>   file_id sample_name direction #> 1   S1_R1          S1   Forward #> 2   S1_R2          S1   Reverse #> 3   S2_R1          S2   Forward #> 4   S2_R2          S2   Reverse #>                                                                           directory_data_path #> 1 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R1.fastq.gz #> 2 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S1_R2.fastq.gz #> 3 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R1.fastq.gz #> 4 /home/marthasudermann/R/x86_64-pc-linux-gnu-library/4.1/demulticoder/extdata/S2_R2.fastq.gz #>                                         temp_data_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/S2_R2.fastq.gz #>                                                             prefiltered_path #> 1 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R1.fastq.gz #> 2 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S1_R2.fastq.gz #> 3 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R1.fastq.gz #> 4 /tmp/RtmpGpvMZP/demulticoder_run_temp/prefiltered_sequences/S2_R2.fastq.gz #> Parameters input #> # A tibble: 2 × 20 #>   primer_name already_trimmed minCutadaptlength multithread verbose  maxN #>   <chr>       <lgl>                       <dbl> <lgl>       <lgl>   <dbl> #> 1 rps10       FALSE                         150 TRUE        TRUE        0 #> 2 its         FALSE                          50 TRUE        TRUE        0 #> # ℹ 14 more variables: maxEE_forward <dbl>, maxEE_reverse <dbl>, #> #   truncLen_forward <dbl>, truncLen_reverse <dbl>, truncQ <dbl>, minLen <dbl>, #> #   maxLen <dbl>, minQ <dbl>, trimLeft <dbl>, trimRight <dbl>, #> #   rm.lowcomplex <dbl>, minOverlap <dbl>, maxMismatch <dbl>, #> #   min_asv_length <dbl> #> Metadata input  #>   samplename_barcode primer_name sample_name well organism #> 1             S1_its         its          S1  A01      Cry #> 3           S1_rps10       rps10          S1  A01      Cry #> 2             S2_its         its          S2  B01      Cin #> 4           S2_rps10       rps10          S2  B01      Cin #>                  forward                f_compt                  f_rev #> 1 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 3   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #> 2 CTTGGTCATTTAGAGGAAGTAA GAACCAGTAAATCTCCTTCATT AATGAAGGAGATTTACTGGTTC #> 4   GTTGGTTAGAGYARAAGACT   CAACCAATCTCRTYTTCTGA   TCAGAARAYGAGATTGGTTG #>                     f_rc               reverse               r_compt #> 1 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 3   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #> 2 TTACTTCCTCTAAATGACCAAG  GCTGCGTTCTTCATCGATGC  CGACGCAAGAAGTAGCTACG #> 4   AGTCTTYTRCTCTAACCAAC ATRYYTAGAAAGAYTYGAACT TAYRRATCTTTCTRARCTTGA #>                   r_rev                  r_rc #> 1  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 3 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT #> 2  CGTAGCTACTTCTTGCGTCG  GCATCGATGAAGAACGCAGC #> 4 TCAAGYTYAGAAAGATYYRTA AGTTCRARTCTTTCTARRYAT"},{"path":"/reference/primer_check.html","id":null,"dir":"Reference","previous_headings":"","what":"Matching Order Primer Check — primer_check","title":"Matching Order Primer Check — primer_check","text":"Matching Order Primer Check","code":""},{"path":"/reference/primer_check.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matching Order Primer Check — primer_check","text":"","code":"primer_check(fastq_data)"},{"path":"/reference/primer_check.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matching Order Primer Check — primer_check","text":"fastq_data data frame FASTQ file paths, direction sequences, names sequences","code":""},{"path":"/reference/primer_check.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matching Order Primer Check — primer_check","text":"None","code":""},{"path":"/reference/process_single_barcode.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the information from an ASV abundance matrix to run DADA2 for single\nbarcode — process_single_barcode","title":"Process the information from an ASV abundance matrix to run DADA2 for single\nbarcode — process_single_barcode","text":"Process information ASV abundance matrix run DADA2 single barcode","code":""},{"path":"/reference/process_single_barcode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the information from an ASV abundance matrix to run DADA2 for single\nbarcode — process_single_barcode","text":"","code":"process_single_barcode(   data_tables,   temp_directory_path,   output_directory_path,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   locus = barcode )"},{"path":"/reference/process_single_barcode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the information from an ASV abundance matrix to run DADA2 for single\nbarcode — process_single_barcode","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants tryRC (Optional). Default FALSE.  TRUE, reverse-complement sequences used classification better match reference sequences forward sequence. verbose (Optional). Default FALSE. TRUE, print status standard output. multithread (Optional). Default FALSE. TRUE, multithreading enabled number available threads automatically determined.    integer provided, number threads use set passing argument setThreadOptions.","code":""},{"path":"/reference/read_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"Takes in the FASTQ files from the user and creates a data frame with the\npaths to files that will be created and used in the future. Included in a\nlarger 'read_prefilt_fastq' function. — read_fastq","title":"Takes in the FASTQ files from the user and creates a data frame with the\npaths to files that will be created and used in the future. Included in a\nlarger 'read_prefilt_fastq' function. — read_fastq","text":"Takes FASTQ files user creates data frame paths files created used future. Included larger 'read_prefilt_fastq' function.","code":""},{"path":"/reference/read_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Takes in the FASTQ files from the user and creates a data frame with the\npaths to files that will be created and used in the future. Included in a\nlarger 'read_prefilt_fastq' function. — read_fastq","text":"","code":"read_fastq(data_directory_path, temp_directory_path)"},{"path":"/reference/read_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Takes in the FASTQ files from the user and creates a data frame with the\npaths to files that will be created and used in the future. Included in a\nlarger 'read_prefilt_fastq' function. — read_fastq","text":"data_directory_path path directory containing FASTQ, metadata, primer_info files temp_directory_path User-defined temporary directory place reads throughout workflow.","code":""},{"path":"/reference/read_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Takes in the FASTQ files from the user and creates a data frame with the\npaths to files that will be created and used in the future. Included in a\nlarger 'read_prefilt_fastq' function. — read_fastq","text":"data frame FASTQ file paths, primer orientations sequences, parsed sample names","code":""},{"path":"/reference/read_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"Take user's DADA2 parameters make dataframe downstream steps","code":""},{"path":"/reference/read_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"","code":"read_parameters(primers_params_path)"},{"path":"/reference/read_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/read_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"data frame information DADA2 parameters.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"A function for calling read_fastq, primer_check, and remove_ns functions.\nThis will process and edit the FASTQ and make them ready for the trimming of\nprimers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","title":"A function for calling read_fastq, primer_check, and remove_ns functions.\nThis will process and edit the FASTQ and make them ready for the trimming of\nprimers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"function calling read_fastq, primer_check, remove_ns functions. process edit FASTQ make ready trimming primers Cutadapt. Part larger 'prepare_reads' function.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function for calling read_fastq, primer_check, and remove_ns functions.\nThis will process and edit the FASTQ and make them ready for the trimming of\nprimers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"","code":"read_prefilt_fastq(   data_directory_path = data_directory_path,   multithread = FALSE,   temp_directory_path )"},{"path":"/reference/read_prefilt_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function for calling read_fastq, primer_check, and remove_ns functions.\nThis will process and edit the FASTQ and make them ready for the trimming of\nprimers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"data_directory_path path directory containing FASTQ, metadata.csv, primerinfo_params.csv files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function for calling read_fastq, primer_check, and remove_ns functions.\nThis will process and edit the FASTQ and make them ready for the trimming of\nprimers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"Returns filtered reads Ns","code":""},{"path":"/reference/remove_ns.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for core DADA2 filter and trim function for first filtering\nstep — remove_ns","title":"Wrapper function for core DADA2 filter and trim function for first filtering\nstep — remove_ns","text":"Wrapper function core DADA2 filter trim function first filtering step","code":""},{"path":"/reference/remove_ns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for core DADA2 filter and trim function for first filtering\nstep — remove_ns","text":"","code":"remove_ns(fastq_data, multithread = TRUE, temp_directory_path)"},{"path":"/reference/remove_ns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for core DADA2 filter and trim function for first filtering\nstep — remove_ns","text":"fastq_data data frame fastq file paths, direction sequences, names sequences metadata, primer_info files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow metadata metadata containing concatenated metadata primer data","code":""},{"path":"/reference/remove_ns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for core DADA2 filter and trim function for first filtering\nstep — remove_ns","text":"Return prefiltered reads Ns","code":""},{"path":"/reference/run_cutadapt.html","id":null,"dir":"Reference","previous_headings":"","what":"Core function for running cutadapt — run_cutadapt","title":"Core function for running cutadapt — run_cutadapt","text":"Core function running cutadapt","code":""},{"path":"/reference/run_cutadapt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core function for running cutadapt — run_cutadapt","text":"","code":"run_cutadapt(   cutadapt_path,   cutadapt_data_barcode,   barcode_params,   minCutadaptlength )"},{"path":"/reference/run_cutadapt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core function for running cutadapt — run_cutadapt","text":"cutadapt_path path cutadapt program. minCutadaptlength Read lengths lower threshold discarded. Default 50. cutadapt_data Directory_data folder trimmed filtered reads sample.","code":""},{"path":"/reference/run_cutadapt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core function for running cutadapt — run_cutadapt","text":"Trimmed read.","code":""},{"path":"/reference/setup_directories.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up directory paths for subsequent analyses — setup_directories","title":"Set up directory paths for subsequent analyses — setup_directories","text":"function sets directory paths subsequent analyses. checks whether specified output directories exist creates . function also provides paths primer metadata files within data directory.","code":""},{"path":"/reference/setup_directories.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up directory paths for subsequent analyses — setup_directories","text":"","code":"setup_directories(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\" )"},{"path":"/reference/setup_directories.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up directory paths for subsequent analyses — setup_directories","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run.","code":""},{"path":"/reference/setup_directories.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up directory paths for subsequent analyses — setup_directories","text":"list paths data, output, temporary directories, primer, metadata files.","code":""}]
