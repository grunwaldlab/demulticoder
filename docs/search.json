[{"path":"/articles/01_getting_started.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before You Start","title":"Getting Started","text":"following example, demonstrate key package functionality using subset reads two samples containing pooled ITS1 fungal rps10 oomycete amplicons. databases used assign taxonomy step also abridged versions full UNITE oomyceteDB databases. can follow along test data associated CSV input files loaded package. Additional examples also available website. Please note, speed, test dataset comprised randomly subset reads samples (S1 S2), due database size, full UNITE database included package, also smaller subset larger database. need prepare raw read files fill metadata.csv primerinfo_params.csv templates.","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-the-pe-amplicon-files","dir":"Articles","previous_headings":"","what":"Format of the PE amplicon files","title":"Getting Started","text":"package takes forward reverse Illumina short read sequence data. avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz.","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"","what":"Format of metadata file (metadata.csv)","title":"Getting Started","text":"format CSV file simple. template . two necessary columns (names) : sample_name column primer_info column additional metadata pasted two columns. can referenced later analysis steps save step loading metadata later. S1 S2 come rhododendron rhizobiome dataset random subset reads. notice S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled reads (rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. Example using test dataset:","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-primer-and-parameters-file-primerinfo_parms-csv","dir":"Articles","previous_headings":"","what":"Format of primer and parameters file (primerinfo_parms.csv)","title":"Getting Started","text":"DADA2 Primer sequence information user-defined parameters placed primerinfo_params.csv. simplify functions called, user provide parameters within input file. recommend using template linked . Remember add additional optional DADA2 parameters want use. required columns user must fill : 1.primer_name compatible options currently (add cells written crucial database formatting step): rps10, , r16S, other1, other2 2.forward-forward sequence 3.reverse-reverse sequence Example template ‘primerinfo_params.csv’ info parameter specifics, eee Documentation tab.","code":""},{"path":"/articles/01_getting_started.html","id":"reference-database-format","dir":"Articles","previous_headings":"","what":"Reference Database Format","title":"Getting Started","text":"now, package compatible following databases: oomycetedb : http://www.oomycetedb.org/ SILVA 16S database species assignments: https://zenodo.org/records/4587955/files/silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1 UNITE fungal database https://unite.ut.ee/repository.php user can select two databases, first need reformat headers exactly like SILVA database. See ‘Documentation’ tab. Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function","code":""},{"path":"/articles/01_getting_started.html","id":"additional-notes","dir":"Articles","previous_headings":"","what":"Additional Notes","title":"Getting Started","text":"Computer specifications may limiting factor. using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases comprised randomly subset reads. following example, run personal computer least 16 GB RAM. Users need upload databases input data folder. computer crashes taxonomic assignment step, need switch computer sufficient memory. Please also ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/01_getting_started.html","id":"loading-the-package","dir":"Articles","previous_headings":"","what":"Loading the Package","title":"Getting Started","text":"now, package loaded retrieving GitHub. Eventually, package uploaded CRAN Bioconductor.","code":"#devtools::install_github(\"grunwaldlab/demulticoder\") devtools::load_all(\"~/demulticoder\") library(\"demulticoder\") library(\"metacoder\") library(\"dplyr\")"},{"path":"/articles/01_getting_started.html","id":"reorganize-data-tables-and-set-up-data-directory-structure","dir":"Articles","previous_headings":"","what":"Reorganize data tables and set-up data directory structure","title":"Getting Started","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    tempdir_path = \"~/temp_test_dataset\",   tempdir_id = \"test_dataset\",   overwrite_existing = TRUE)"},{"path":"/articles/01_getting_started.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"","what":"Remove primers with Cutadapt","title":"Getting Started","text":"running Cutadapt, please ensure installed .","code":"#cutadapt_path=\"/usr/bin/cutadapt\",  demulticoder::cut_trim(   analysis_setup,   cutadapt_path = \"/opt/homebrew/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 4.1 for its sequence data  #> Running Cutadapt 4.1 for rps10 sequence data"},{"path":"/articles/01_getting_started.html","id":"asv-inference-step","dir":"Articles","previous_headings":"","what":"ASV inference step","title":"Getting Started","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = TRUE) #> 710847 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 654 unique sequences. #> Sample 2 - 1215 reads in 610 unique sequences. #> 724232 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input. #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input. #> Identified 0 bimeras out of 5 input sequences. #> $its #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/01_getting_started.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"","what":"Taxonomic assignment step","title":"Getting Started","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing = TRUE) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/01_getting_started.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Getting Started","text":"","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, minimum_bootstrap = 0, save_outputs = TRUE) #> Rows: 38 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 5 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":[]},{"path":"/articles/01_getting_started.html","id":"objects-can-now-be-used-for-downstream-data-analysis","dir":"Articles","previous_headings":"","what":"Objects can now be used for downstream data analysis","title":"Getting Started","text":"make heattrees using taxmap object. First make heat tree -barcoded samples  Now make heat tree rps10-barcoded samples  can also variety analyses, convert phyloseq object demonstrate make stacked bar plot relative abundance taxa sample -barcoded samples  Finally,demonstrate make stacked bar plot relative abundance taxa sample rps10-barcoded samples","code":"objs$taxmap_its %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") objs$taxmap_rps10 %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated.  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/articles/02_documentation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Documentation","text":"documentation provides comprehensive information use Demulticoder R package processing analyzing metabarcode sequencing data. covers input file requirements, parameter settings, key parameters.","code":""},{"path":"/articles/02_documentation.html","id":"quick-start-guide","dir":"Articles","previous_headings":"","what":"Quick Start Guide","title":"Documentation","text":"Prepare input files (metadata.csv, primerinfo_params.csv, unformatted reference databases, PE Illumina read files). Place input files single directory. Ensure file names comply specified format. Run pipeline default settings adjust parameters needed.","code":""},{"path":[]},{"path":"/articles/02_documentation.html","id":"directory-structure","dir":"Articles","previous_headings":"Input Files","what":"Directory Structure","title":"Documentation","text":"Place input files single directory. directory contain following files: metadata.csv primerinfo_params.csv PE Illumina read files Unformatted reference databases","code":""},{"path":[]},{"path":"/articles/02_documentation.html","id":"read-name-format","dir":"Articles","previous_headings":"Input Files > File Naming Conventions","what":"Read Name Format","title":"Documentation","text":"avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz","code":""},{"path":"/articles/02_documentation.html","id":"metadata-csv","dir":"Articles","previous_headings":"Input Files","what":"metadata.csv","title":"Documentation","text":"metadata.csv file contains information samples primers (associated barcodes) used experiment. following two required columns: sample_name: Identifier sample (e.g., S1, S2) primer_name: Name primer used (e.g., rps10, , r16S, other1, oteher2) Please add associated metadata file two required columns. can used downstream exploratory diversity analyses, sample data incorporated final phyloseq taxmap objects. Example file (optional third column):","code":"sample_name,primer_name,organism S1,rps10,Cry S2,rps10,Cin S1,its,Cry S2,its,Cin"},{"path":"/articles/02_documentation.html","id":"primerinfo_params-csv","dir":"Articles","previous_headings":"Input Files","what":"primerinfo_params.csv","title":"Documentation","text":"primerinfo_params.csv file contains information primer sequences used experiment, along optional additional parameters part DADA2 pipeline. anything specified, default values used. Required columns: primer_name: Name primer/barcode (e.g., , rps10) forward: Forward primer sequence reverse: Reverse primer sequence DADA2 filterAndTrim function parameters: already_trimmed: Boolean indicating primers already trimmed (TRUE/FALSE) (default: FALSE) minCutadaptlength: Cutadapt parameter-Minimum length Cutadapt trimming (default: 0) multithread: Boolean multithreading (TRUE/FALSE) (default: FALSE) verbose: Boolean verbose output (TRUE/FALSE) (default: FALSE) maxN: Maximum number N bases allowed (default: 0) maxEE_forward: Maximum expected errors forward reads (default: Inf) maxEE_reverse: Maximum expected errors reverse reads (default: Inf) truncLen_forward: Truncation length forward reads (default: 0) truncLen_reverse: Truncation length reverse reads (default: 0) truncQ: Truncation quality threshold (default: 2) minLen: Minimum length reads processing (default: 20) maxLen: Maximum length reads processing (default: Inf) minQ: Minimum quality score (default: 0) trimLeft: Number bases trim start reads (default: 0) trimRight: Number bases trim end reads (default: 0) rm.lowcomplex: Boolean removing low complexity sequences (default: TRUE) DADA2 learnErrors function parameters: nbases: Number bases use error rate learning (default: 1e+08) randomize: Randomize reads error rate learning (default: FALSE) MAX_CONSIST: Maximum number self-consistency iterations (default: 10) OMEGA_C: Convergence threshold error rates (default: 0) qualityType: Quality score type (“Auto”, “FastqQuality”, “ShortRead”) (default: “Auto”) DADA2 plot errors parameters: err_out: Return error rates used inference (default: TRUE) err_in: Use input error rates instead learning (default: FALSE) nominalQ: Use nominal Q-scores (default: FALSE) obs: Return observed error rates (default: TRUE) DADA2 dada function parameters: OMP: Use OpenMP multi-threading available (default: TRUE) n: Number reads use error rate estimation (default: 1e+05) id.sep: Character separating sample ID sequence name (default: “\\s”) orient.fwd: NULL TRUE/FALSE orient sequences (default: NULL) pool: Pool samples error rate estimation (default: FALSE) selfConsist: Perform self-consistency iterations (default: FALSE) DADA2 mergePairs function parameters: minOverlap: Minimum overlap merging paired-end reads (default: 12) maxMismatch: Maximum mismatches allowed overlap region (default: 0) DADA2 removeBimeraDenovo function parameters: method: Method sample inference (“consensus” “pooled”) (default: “consensus”) parameters include CSV input file: min_asv_length: Minimum length Amplicon Sequence Variants (ASVs) core dada ASV inference steps (default=0) Example file (select optional columns forward reverse primer sequence columns):","code":"primer_name,forward,reverse,already_trimmed,minCutadaptlength,multithread,verbose,maxN,maxEE_forward,maxEE_reverse,truncLen_forward,truncLen_reverse,truncQ,minLen,maxLen,minQ,trimLeft,trimRight,rm.lowcomplex,minOverlap,maxMismatch,min_asv_length rps10,GTTGGTTAGAGYARAAGACT,ATRYYTAGAAAGAYTYGAACT,FALSE,100,TRUE,FALSE,1.00E+05,5,5,0,0,5,150,Inf,0,0,0,0,15,0,50 its,CTTGGTCATTTAGAGGAAGTAA,GCTGCGTTCTTCATCGATGC,FALSE,50,TRUE,FALSE,1.00E+05,5,5,0,0,5,50,Inf,0,0,0,0,15,0,50"},{"path":"/articles/02_documentation.html","id":"reference-database","dir":"Articles","previous_headings":"Input Files","what":"Reference Database","title":"Documentation","text":"Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function. now, package compatible following databases: oomycetedb : http://www.oomycetedb.org/ SILVA 16S database species assignments: https://zenodo.org/records/4587955/files/silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1 UNITE fungal database https://unite.ut.ee/repository.php two reference databases. user need reformat headers exactly outlined DADA2 database format, similar SILVA database format. user can specify path database input file. database fasta format.","code":""},{"path":"/articles/02_documentation.html","id":"faq","dir":"Articles","previous_headings":"","what":"FAQ","title":"Documentation","text":"progress","code":""},{"path":"/articles/02_documentation.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Documentation","text":"progress","code":""},{"path":"/articles/03_16S_mothur_validation.html","id":"demonstration-of-how-to-use-demulticoder-on-a-dataset-that-is-actually-three-separate-datasets-rps10-its-and-16s-at-once","dir":"Articles","previous_headings":"","what":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","title":"16S Mothur SOP Validation","text":"Input metadata primerinfo_params files data folder required columns first sample names, second primer name/barcode used. subsequent columns user-specific columns downstream steps metadata.csv file included necessary second file name barcode selected, primer sequences, optional DADA2 parameter options. referenced DADA2 tutorial select proper parameter options. Note, primers already trimmed reads, just certain, included Earth Microbiome primers described , primer sequences still found within small number reads. primerinfo_params.csv","code":""},{"path":"/articles/03_16S_mothur_validation.html","id":"step-1-remove-ns-and-create-directory-structure-for-downstream-steps","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 1-Remove N’s and create directory structure for downstream steps","title":"16S Mothur SOP Validation","text":"","code":"outputs<-prepare_reads(   data_directory = \"~/benchmark_demulticoder/mothur_16S_sop/data\",    output_directory = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs\",   tempdir_path = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs\",   tempdir_id = \"temp_files\",   overwrite_existing = TRUE) #> Rows: 1 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 1 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 20 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, When #> dbl (1): Day #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Warning in get_read_names(paired_file_paths[1]) == #> get_read_names(paired_file_paths[2]): longer object length is not a multiple of #> shorter object length #> Creating output directory: /Users/masudermann/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/prefiltered_sequences"},{"path":"/articles/03_16S_mothur_validation.html","id":"step-2-run-cutadapt-to-remove-primers-and-then-trim-reads-with-dada2-filterandtrim-function","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","title":"16S Mothur SOP Validation","text":"","code":"cut_trim(   outputs,   #cutadapt_path=\"/usr/bin/cutadapt\",   \"/opt/homebrew/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 4.1 for r16S sequence data  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D0_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D0_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D1_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D1_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D141_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D141_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D142_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D142_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D143_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D143_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D144_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D144_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D145_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D145_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D146_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D146_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D147_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D147_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D148_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D148_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D149_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D149_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D150_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D150_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D2_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D2_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D3_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D3_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D5_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D5_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D6_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D6_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D7_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D7_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D8_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D8_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D9_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/F3D9_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/Mock_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/trimmed_sequences/Mock_R2_r16S.fastq.gz"},{"path":"/articles/03_16S_mothur_validation.html","id":"step-3-core-asv-inference-step","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 3-Core ASV inference step","title":"16S Mothur SOP Validation","text":"","code":"make_asv_abund_matrix(   outputs,   overwrite_existing = TRUE) #> 33513360 total bases in 139639 reads from 20 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .................... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #> Convergence after  5  rounds. #> Error rate plot for the Forward read of primer pair r16S #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 7112 reads in 1978 unique sequences. #> Sample 2 - 5299 reads in 1639 unique sequences. #> Sample 3 - 5463 reads in 1477 unique sequences. #> Sample 4 - 2914 reads in 904 unique sequences. #> Sample 5 - 2941 reads in 939 unique sequences. #> Sample 6 - 4312 reads in 1267 unique sequences. #> Sample 7 - 6741 reads in 1756 unique sequences. #> Sample 8 - 4560 reads in 1438 unique sequences. #> Sample 9 - 15636 reads in 3589 unique sequences. #> Sample 10 - 11412 reads in 2761 unique sequences. #> Sample 11 - 12017 reads in 3021 unique sequences. #> Sample 12 - 5032 reads in 1566 unique sequences. #> Sample 13 - 18075 reads in 3707 unique sequences. #> Sample 14 - 6250 reads in 1479 unique sequences. #> Sample 15 - 4052 reads in 1195 unique sequences. #> Sample 16 - 7369 reads in 1832 unique sequences. #> Sample 17 - 4765 reads in 1183 unique sequences. #> Sample 18 - 4871 reads in 1382 unique sequences. #> Sample 19 - 6504 reads in 1709 unique sequences. #> Sample 20 - 4314 reads in 897 unique sequences. #> 22342240 total bases in 139639 reads from 20 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .................... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #>    selfConsist step 7 #> Convergence after  7  rounds. #> Error rate plot for the Reverse read of primer pair r16S #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 7112 reads in 1659 unique sequences. #> Sample 2 - 5299 reads in 1349 unique sequences. #> Sample 3 - 5463 reads in 1335 unique sequences. #> Sample 4 - 2914 reads in 853 unique sequences. #> Sample 5 - 2941 reads in 880 unique sequences. #> Sample 6 - 4312 reads in 1286 unique sequences. #> Sample 7 - 6741 reads in 1803 unique sequences. #> Sample 8 - 4560 reads in 1265 unique sequences. #> Sample 9 - 15636 reads in 3413 unique sequences. #> Sample 10 - 11412 reads in 2522 unique sequences. #> Sample 11 - 12017 reads in 2771 unique sequences. #> Sample 12 - 5032 reads in 1415 unique sequences. #> Sample 13 - 18075 reads in 3290 unique sequences. #> Sample 14 - 6250 reads in 1390 unique sequences. #> Sample 15 - 4052 reads in 1134 unique sequences. #> Sample 16 - 7369 reads in 1635 unique sequences. #> Sample 17 - 4765 reads in 1084 unique sequences. #> Sample 18 - 4871 reads in 1161 unique sequences. #> Sample 19 - 6504 reads in 1502 unique sequences. #> Sample 20 - 4314 reads in 732 unique sequences. #> 6540 paired-reads (in 107 unique pairings) successfully merged out of 6891 (in 197 pairings) input. #> 5027 paired-reads (in 101 unique pairings) successfully merged out of 5189 (in 157 pairings) input. #> 4986 paired-reads (in 81 unique pairings) successfully merged out of 5267 (in 166 pairings) input. #> 2595 paired-reads (in 52 unique pairings) successfully merged out of 2754 (in 108 pairings) input. #> 2553 paired-reads (in 60 unique pairings) successfully merged out of 2785 (in 119 pairings) input. #> 3646 paired-reads (in 55 unique pairings) successfully merged out of 4109 (in 157 pairings) input. #> 6079 paired-reads (in 81 unique pairings) successfully merged out of 6514 (in 198 pairings) input. #> 3968 paired-reads (in 91 unique pairings) successfully merged out of 4388 (in 187 pairings) input. #> 14233 paired-reads (in 143 unique pairings) successfully merged out of 15355 (in 352 pairings) input. #> 10529 paired-reads (in 120 unique pairings) successfully merged out of 11165 (in 277 pairings) input. #> 11154 paired-reads (in 137 unique pairings) successfully merged out of 11797 (in 298 pairings) input. #> 4349 paired-reads (in 85 unique pairings) successfully merged out of 4802 (in 179 pairings) input. #> 17431 paired-reads (in 153 unique pairings) successfully merged out of 17812 (in 272 pairings) input. #> 5850 paired-reads (in 81 unique pairings) successfully merged out of 6095 (in 159 pairings) input. #> 3713 paired-reads (in 86 unique pairings) successfully merged out of 3891 (in 147 pairings) input. #> 6865 paired-reads (in 99 unique pairings) successfully merged out of 7191 (in 187 pairings) input. #> 4428 paired-reads (in 68 unique pairings) successfully merged out of 4605 (in 128 pairings) input. #> 4576 paired-reads (in 101 unique pairings) successfully merged out of 4739 (in 174 pairings) input. #> 6092 paired-reads (in 109 unique pairings) successfully merged out of 6315 (in 173 pairings) input. #> 4269 paired-reads (in 20 unique pairings) successfully merged out of 4281 (in 28 pairings) input. #> Identified 61 bimeras out of 293 input sequences. #> $r16S #> [1] \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/temp_files/asvabund_matrixDADA2_r16S.RData\""},{"path":"/articles/03_16S_mothur_validation.html","id":"step-4-assign-taxonomy-step","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 4-Assign taxonomy step","title":"16S Mothur SOP Validation","text":"","code":"assign_tax(   outputs,   asv_abund_matrix,   db_16S=\"silva_nr99_v138.2_toSpecies_trainset.fa.gz\",   retrieve_files=FALSE,   overwrite_existing=TRUE) #>    samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           F3D0_r16S  7733     7112      6976      6979   6540    6528 #> 2           F3D1_r16S  5829     5299      5226      5239   5027    5016 #> 3         F3D141_r16S  5926     5463      5331      5357   4986    4863 #> 4         F3D142_r16S  3158     2914      2799      2830   2595    2521 #> 5         F3D143_r16S  3164     2941      2822      2868   2553    2519 #> 6         F3D144_r16S  4798     4312      4150      4228   3646    3507 #> 7         F3D145_r16S  7331     6741      6592      6627   6079    5820 #> 8         F3D146_r16S  4993     4560      4450      4470   3968    3879 #> 9         F3D147_r16S 16956    15636     15433     15505  14233   13006 #> 10        F3D148_r16S 12332    11412     11250     11267  10529    9935 #> 11        F3D149_r16S 13006    12017     11857     11898  11154   10653 #> 12        F3D150_r16S  5474     5032      4879      4925   4349    4240 #> 13          F3D2_r16S 19489    18075     17907     17939  17431   16835 #> 14          F3D3_r16S  6726     6250      6145      6176   5850    5486 #> 15          F3D5_r16S  4418     4052      3930      3991   3713    3713 #> 16          F3D6_r16S  7933     7369      7231      7294   6865    6678 #> 17          F3D7_r16S  5103     4765      4646      4673   4428    4217 #> 18          F3D8_r16S  5274     4871      4786      4802   4576    4547 #> 19          F3D9_r16S  7023     6504      6341      6442   6092    6015 #> 20          Mock_r16S  4748     4314      4287      4286   4269    4269"},{"path":"/articles/03_16S_mothur_validation.html","id":"step-5-convert-asv-matrix-to-taxmap-and-phyloseq-objects-with-one-function","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 5-convert asv matrix to taxmap and phyloseq objects with one function","title":"16S Mothur SOP Validation","text":"","code":"objs<-convert_asv_matrix_to_objs(outputs, save_outputs=TRUE, overwrite_existing = TRUE) #> Rows: 232 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (20): F3D0_r16S, F3D1_r16S, F3D141_r16S, F3D142_r16S, F3D143_r16S, F3D14... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For r16S dataset  #> Taxmap object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/taxmap_obj_r16S.RData  #> Phyloseq object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/phylo_obj_r16S.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/03_16S_mothur_validation.html","id":"step-6-evaluate-accuracy-using-mock-community-as-shown-in-dada2-tutorial","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 6-evaluate accuracy using mock community, as shown in dada2 tutorial","title":"16S Mothur SOP Validation","text":"looking mock community sample, able extract 20 bacterial sequences 0% mismatch, matched described previously.","code":"track_reads_demulticoder<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/track_reads_r16S.csv\", row.names = 1) summary(track_reads_demulticoder) #>      input          filtered       denoisedF       denoisedR     #>  Min.   : 3158   Min.   : 2914   Min.   : 2799   Min.   : 2830   #>  1st Qu.: 4944   1st Qu.: 4498   1st Qu.: 4409   1st Qu.: 4424   #>  Median : 5878   Median : 5381   Median : 5278   Median : 5298   #>  Mean   : 7571   Mean   : 6982   Mean   : 6852   Mean   : 6890   #>  3rd Qu.: 7783   3rd Qu.: 7176   3rd Qu.: 7040   3rd Qu.: 7058   #>  Max.   :19489   Max.   :18075   Max.   :17907   Max.   :17939   #>      merged         nonchim      #>  Min.   : 2553   Min.   : 2519   #>  1st Qu.: 4194   1st Qu.: 4132   #>  Median : 5006   Median : 4940   #>  Mean   : 6444   Mean   : 6212   #>  3rd Qu.: 6621   3rd Qu.: 6566   #>  Max.   :17431   Max.   :16835  tax_matrix<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/final_asv_abundance_matrix_r16S.csv\")  unqs.mock <- tax_matrix[, c(2, which(colnames(tax_matrix) == \"Mock_r16S\"))]  unqs.mock <- unqs.mock[unqs.mock$Mock_r16S != 0,]  cat(\"DADA2 inferred\", nrow(unqs.mock), \"sample sequences present in the Mock community.\\n\") #> DADA2 inferred 20 sample sequences present in the Mock community.  mock.ref <- dada2::getSequences(file.path(\"~/benchmark_demulticoder/mothur_16S_sop/data\", \"HMP_MOCK.v35.fasta\")) match.ref <- sum(sapply(unqs.mock$sequence, function(x) any(grepl(x, mock.ref)))) cat(\"Of those,\", sum(match.ref), \"were exact matches to the expected reference sequences.\\n\") #> Of those, 20 were exact matches to the expected reference sequences."},{"path":"/articles/03_16S_mothur_validation.html","id":"step-7-follow-up-work-using-phyloseq-to-do-side-by-side-comparison-with-dada2-example-and-to-examine-alpha-diversity-results","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 7-Follow-up work using phyloseq to do side-by-side comparison with dada2 example and to examine alpha diversity results","title":"16S Mothur SOP Validation","text":"","code":"objs$phyloseq_r16S <- phyloseq::prune_samples(phyloseq::sample_names(objs$phyloseq_r16S) != \"Mock_r16S\", objs$phyloseq_r16S) # Remove mock sample phyloseq::plot_richness(objs$phyloseq_r16S, x=\"Day\", measures=c(\"Shannon\", \"Simpson\"), color=\"When\") #> Warning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have #> any singletons. This is highly suspicious. Results of richness #> estimates (for example) are probably unreliable, or wrong, if you have already #> trimmed low-abundance taxa from the data. #>  #> We recommended that you find the un-trimmed data and retry."},{"path":"/articles/03_16S_mothur_validation.html","id":"step-8-examine-ordination-plots-as-additional-point-of-comparison-with-dada2-tutorial","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 8-Examine ordination plots as additional point of comparison with DADA2 tutorial","title":"16S Mothur SOP Validation","text":"","code":"# Transform data to proportions as appropriate for Bray-Curtis distances ps.prop <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(otu) otu/sum(otu)) ord.nmds.bray <- phyloseq::ordinate(ps.prop, method=\"NMDS\", distance=\"bray\") #> Run 0 stress 0.0808378  #> Run 1 stress 0.09061366  #> Run 2 stress 0.1231378  #> Run 3 stress 0.121153  #> Run 4 stress 0.08635462  #> Run 5 stress 0.0808378  #> ... Procrustes: rmse 5.812942e-06  max resid 1.554258e-05  #> ... Similar to previous best #> Run 6 stress 0.1231378  #> Run 7 stress 0.09061366  #> Run 8 stress 0.08096389  #> ... Procrustes: rmse 0.009176341  max resid 0.02810282  #> Run 9 stress 0.1433231  #> Run 10 stress 0.0808378  #> ... Procrustes: rmse 5.646215e-06  max resid 1.499963e-05  #> ... Similar to previous best #> Run 11 stress 0.1231378  #> Run 12 stress 0.0809639  #> ... Procrustes: rmse 0.009215858  max resid 0.02823353  #> Run 13 stress 0.121153  #> Run 14 stress 0.1320722  #> Run 15 stress 0.08635462  #> Run 16 stress 0.121153  #> Run 17 stress 0.08096389  #> ... Procrustes: rmse 0.009186834  max resid 0.02813753  #> Run 18 stress 0.08635462  #> Run 19 stress 0.08635462  #> Run 20 stress 0.09466617  #> *** Best solution repeated 2 times phyloseq::plot_ordination(ps.prop, ord.nmds.bray, color=\"When\", title=\"Bray NMDS\")"},{"path":"/articles/03_16S_mothur_validation.html","id":"step-9-lets-look-at-what-the-top-20-taxa-are-in-the-early-vs--late-samples-time-points-as-shown-in-the-dada2-tutorial","dir":"Articles","previous_headings":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","what":"Step 9-Let’s look at what the top 20 taxa are in the early vs. late samples time points, as shown in the dada2 tutorial","title":"16S Mothur SOP Validation","text":"","code":"top20 <- names(sort(phyloseq::taxa_sums(objs$phyloseq_r16S), decreasing=TRUE))[1:20] ps.top20 <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(OTU) OTU/sum(OTU)) ps.top20 <- phyloseq::prune_taxa(top20, ps.top20) phyloseq::plot_bar(ps.top20, x=\"Day\", fill=\"Family\") + ggplot2::facet_wrap(~When, scales=\"free_x\") sessioninfo::session_info() #> ─ Session info ─────────────────────────────────────────────────────────────── #>  setting  value #>  version  R version 4.3.2 (2023-10-31) #>  os       macOS Ventura 13.2.1 #>  system   aarch64, darwin20 #>  ui       X11 #>  language en #>  collate  en_US.UTF-8 #>  ctype    en_US.UTF-8 #>  tz       America/Los_Angeles #>  date     2025-01-27 #>  pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown) #>  #> ─ Packages ─────────────────────────────────────────────────────────────────── #>  ! package              * version    date (UTC) lib source #>    abind                  1.4-5      2016-07-21 [2] CRAN (R 4.3.0) #>    ade4                   1.7-22     2023-02-06 [2] CRAN (R 4.3.0) #>    ape                    5.8        2024-04-11 [2] CRAN (R 4.3.1) #>    Biobase                2.62.0     2023-10-26 [2] Bioconductor #>    BiocGenerics           0.48.1     2023-11-02 [2] Bioconductor #>    BiocParallel           1.36.0     2023-10-26 [2] Bioconductor #>    biomformat             1.30.0     2023-10-26 [2] Bioconductor #>    Biostrings             2.70.3     2024-04-03 [2] bioc_xgit (@c213e35) #>    bit                    4.5.0.1    2024-12-03 [2] CRAN (R 4.3.3) #>    bit64                  4.6.0-1    2025-01-16 [2] CRAN (R 4.3.3) #>    bitops                 1.0-8      2024-07-29 [2] CRAN (R 4.3.3) #>    bslib                  0.8.0      2024-07-29 [2] CRAN (R 4.3.3) #>    cachem                 1.1.0      2024-05-16 [2] CRAN (R 4.3.2) #>    cli                    3.6.3      2024-06-21 [2] CRAN (R 4.3.2) #>    cluster                2.1.6      2023-12-01 [2] CRAN (R 4.3.1) #>    codetools              0.2-20     2024-03-31 [2] CRAN (R 4.3.1) #>    colorspace             2.1-1      2024-07-26 [2] CRAN (R 4.3.3) #>    crayon                 1.5.3      2024-06-20 [2] CRAN (R 4.3.3) #>    dada2                  1.30.0     2024-01-10 [2] bioc_xgit (@ec87892) #>    data.table             1.15.4     2024-03-30 [2] CRAN (R 4.3.1) #>    DelayedArray           0.28.0     2023-11-06 [2] Bioconductor #>    deldir                 2.0-4      2024-02-28 [2] CRAN (R 4.3.1) #>  P demulticoder         * 0.0.0.9000 2025-01-27 [?] load_all() #>    desc                   1.4.3      2023-12-10 [2] CRAN (R 4.3.1) #>    devtools               2.4.5      2022-10-11 [2] CRAN (R 4.3.0) #>    digest                 0.6.37     2024-08-19 [2] CRAN (R 4.3.3) #>    dplyr                  1.1.4      2023-11-17 [2] CRAN (R 4.3.1) #>    ellipsis               0.3.2      2021-04-29 [2] CRAN (R 4.3.0) #>    evaluate               1.0.3      2025-01-10 [2] CRAN (R 4.3.3) #>    farver                 2.1.2      2024-05-13 [2] CRAN (R 4.3.3) #>    fastmap                1.2.0      2024-05-15 [2] CRAN (R 4.3.2) #>    foreach                1.5.2      2022-02-02 [2] CRAN (R 4.3.0) #>    fs                     1.6.5      2024-10-30 [2] CRAN (R 4.3.3) #>    furrr                  0.3.1      2022-08-15 [2] CRAN (R 4.3.0) #>    future                 1.34.0     2024-07-29 [2] CRAN (R 4.3.3) #>    generics               0.1.3      2022-07-05 [2] CRAN (R 4.3.0) #>    GenomeInfoDb           1.38.8     2024-03-16 [2] Bioconductor 3.18 (R 4.3.3) #>    GenomeInfoDbData       1.2.11     2023-11-14 [2] Bioconductor #>    GenomicAlignments      1.38.2     2024-01-20 [2] Bioconductor 3.18 (R 4.3.2) #>    GenomicRanges          1.54.1     2023-10-30 [2] Bioconductor #>    ggplot2                3.5.1      2024-04-23 [2] CRAN (R 4.3.1) #>    globals                0.16.3     2024-03-08 [2] CRAN (R 4.3.1) #>    glue                   1.8.0      2024-09-30 [2] CRAN (R 4.3.3) #>    gtable                 0.3.6      2024-10-25 [2] CRAN (R 4.3.3) #>    hms                    1.1.3      2023-03-21 [2] CRAN (R 4.3.0) #>    htmltools              0.5.8.1    2024-04-04 [2] CRAN (R 4.3.1) #>    htmlwidgets            1.6.4      2023-12-06 [2] CRAN (R 4.3.1) #>    httpuv                 1.6.15     2024-03-26 [2] CRAN (R 4.3.1) #>    hwriter                1.3.2.1    2022-04-08 [2] CRAN (R 4.3.0) #>    igraph                 2.0.3      2024-03-13 [2] CRAN (R 4.3.1) #>    interp                 1.1-6      2024-01-26 [2] CRAN (R 4.3.1) #>    IRanges                2.36.0     2023-10-26 [2] Bioconductor #>    iterators              1.0.14     2022-02-05 [2] CRAN (R 4.3.0) #>    jpeg                   0.1-10     2022-11-29 [2] CRAN (R 4.3.0) #>    jquerylib              0.1.4      2021-04-26 [2] CRAN (R 4.3.0) #>    jsonlite               1.8.9      2024-09-20 [2] CRAN (R 4.3.3) #>    knitr                  1.49       2024-11-08 [2] CRAN (R 4.3.3) #>    labeling               0.4.3      2023-08-29 [2] CRAN (R 4.3.0) #>    later                  1.3.2      2023-12-06 [2] CRAN (R 4.3.1) #>    lattice                0.22-6     2024-03-20 [2] CRAN (R 4.3.1) #>    latticeExtra           0.6-30     2022-07-04 [2] CRAN (R 4.3.0) #>    lazyeval               0.2.2      2019-03-15 [2] CRAN (R 4.3.0) #>    lifecycle              1.0.4      2023-11-07 [2] CRAN (R 4.3.1) #>    listenv                0.9.1      2024-01-29 [2] CRAN (R 4.3.1) #>    magrittr               2.0.3      2022-03-30 [2] CRAN (R 4.3.0) #>    MASS                   7.3-60.0.1 2024-01-13 [2] CRAN (R 4.3.1) #>    Matrix                 1.6-5      2024-01-11 [2] CRAN (R 4.3.1) #>    MatrixGenerics         1.14.0     2023-10-26 [2] Bioconductor #>    matrixStats            1.3.0      2024-04-11 [2] CRAN (R 4.3.1) #>    memoise                2.0.1      2021-11-26 [2] CRAN (R 4.3.0) #>    metacoder              0.3.7      2024-02-20 [2] CRAN (R 4.3.1) #>    mgcv                   1.9-1      2023-12-21 [2] CRAN (R 4.3.1) #>    mime                   0.12       2021-09-28 [2] CRAN (R 4.3.0) #>    miniUI                 0.1.1.1    2018-05-18 [2] CRAN (R 4.3.0) #>    multtest               2.58.0     2023-10-26 [2] Bioconductor #>    munsell                0.5.1      2024-04-01 [2] CRAN (R 4.3.1) #>    nlme                   3.1-165    2024-06-06 [2] CRAN (R 4.3.3) #>    parallelly             1.41.0     2024-12-18 [2] CRAN (R 4.3.3) #>    permute                0.9-7      2022-01-27 [2] CRAN (R 4.3.0) #>    phyloseq               1.46.0     2024-04-03 [2] bioc_xgit (@7320133) #>    pillar                 1.10.1     2025-01-07 [2] CRAN (R 4.3.3) #>    pkgbuild               1.4.6      2025-01-16 [2] CRAN (R 4.3.3) #>    pkgconfig              2.0.3      2019-09-22 [2] CRAN (R 4.3.0) #>    pkgdown                2.1.1      2024-09-17 [2] CRAN (R 4.3.3) #>    pkgload                1.4.0      2024-06-28 [2] CRAN (R 4.3.3) #>    plyr                   1.8.9      2023-10-02 [2] CRAN (R 4.3.1) #>    png                    0.1-8      2022-11-29 [2] CRAN (R 4.3.0) #>    profvis                0.3.8      2023-05-02 [2] CRAN (R 4.3.0) #>    promises               1.3.0      2024-04-05 [2] CRAN (R 4.3.1) #>    purrr                * 1.0.2      2023-08-10 [2] CRAN (R 4.3.0) #>    R6                     2.5.1      2021-08-19 [2] CRAN (R 4.3.0) #>    ragg                   1.3.2      2024-05-15 [2] CRAN (R 4.3.3) #>    RColorBrewer           1.1-3      2022-04-03 [2] CRAN (R 4.3.0) #>    Rcpp                   1.0.13     2024-07-17 [2] CRAN (R 4.3.2) #>    RcppParallel           5.1.9      2024-08-19 [2] CRAN (R 4.3.3) #>    RCurl                  1.98-1.16  2024-07-11 [2] CRAN (R 4.3.3) #>    readr                  2.1.5      2024-01-10 [2] CRAN (R 4.3.1) #>    remotes                2.5.0      2024-03-17 [2] CRAN (R 4.3.1) #>    reshape2               1.4.4      2020-04-09 [2] CRAN (R 4.3.0) #>    rhdf5                  2.46.1     2023-12-02 [2] Bioconductor 3.18 (R 4.3.2) #>    rhdf5filters           1.14.1     2023-11-06 [2] Bioconductor #>    Rhdf5lib               1.24.2     2024-02-10 [2] Bioconductor 3.18 (R 4.3.2) #>    rlang                  1.1.4      2024-06-04 [2] CRAN (R 4.3.3) #>    rmarkdown              2.29       2024-11-04 [2] CRAN (R 4.3.3) #>    rprojroot              2.0.4      2023-11-05 [2] CRAN (R 4.3.1) #>    Rsamtools              2.18.0     2023-10-26 [2] Bioconductor #>    rstudioapi             0.16.0     2024-03-24 [2] CRAN (R 4.3.1) #>    S4Arrays               1.2.1      2024-03-05 [2] Bioconductor 3.18 (R 4.3.2) #>    S4Vectors              0.40.2     2023-11-25 [2] Bioconductor 3.18 (R 4.3.2) #>    sass                   0.4.9      2024-03-15 [2] CRAN (R 4.3.1) #>    scales                 1.3.0      2023-11-28 [2] CRAN (R 4.3.1) #>    sessioninfo            1.2.2      2021-12-06 [2] CRAN (R 4.3.0) #>    shiny                  1.9.1      2024-08-01 [2] CRAN (R 4.3.3) #>    ShortRead              1.60.0     2024-01-10 [2] bioc_xgit (@4304db4) #>    SparseArray            1.2.4      2024-02-10 [2] Bioconductor 3.18 (R 4.3.2) #>    stringi                1.8.4      2024-05-06 [2] CRAN (R 4.3.2) #>    stringr                1.5.1      2023-11-14 [2] CRAN (R 4.3.1) #>    SummarizedExperiment   1.32.0     2023-11-06 [2] Bioconductor #>    survival               3.7-0      2024-06-05 [2] CRAN (R 4.3.3) #>    systemfonts            1.1.0      2024-05-15 [2] CRAN (R 4.3.3) #>    textshaping            0.4.0      2024-05-24 [2] CRAN (R 4.3.3) #>    tibble                 3.2.1      2023-03-20 [2] CRAN (R 4.3.0) #>    tidyr                  1.3.1      2024-01-24 [2] CRAN (R 4.3.1) #>    tidyselect             1.2.1      2024-03-11 [2] CRAN (R 4.3.1) #>    tzdb                   0.4.0      2023-05-12 [2] CRAN (R 4.3.0) #>    urlchecker             1.0.1      2021-11-30 [2] CRAN (R 4.3.0) #>    usethis                3.0.0      2024-07-29 [2] CRAN (R 4.3.3) #>    vctrs                  0.6.5      2023-12-01 [2] CRAN (R 4.3.1) #>    vegan                  2.6-6.1    2024-05-21 [2] CRAN (R 4.3.3) #>    viridisLite            0.4.2      2023-05-02 [2] CRAN (R 4.3.0) #>    vroom                  1.6.5      2023-12-05 [2] CRAN (R 4.3.1) #>    withr                  3.0.2      2024-10-28 [2] CRAN (R 4.3.3) #>    xfun                   0.50       2025-01-07 [2] CRAN (R 4.3.3) #>    xtable                 1.8-4      2019-04-21 [2] CRAN (R 4.3.0) #>    XVector                0.42.0     2023-10-26 [2] Bioconductor #>    yaml                   2.3.10     2024-07-26 [2] CRAN (R 4.3.3) #>    zlibbioc               1.48.2     2024-03-19 [2] Bioconductor 3.18 (R 4.3.3) #>  #>  [1] /private/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T/RtmpDLnZdQ/temp_libpath68857f7d16b0 #>  [2] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library #>  #>  P ── Loaded and on-disk path mismatch. #>  #> ──────────────────────────────────────────────────────────────────────────────"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"demonstration-of-how-to-use-demulticoder-on-a-dataset-that-is-actually-three-separate-datasets-rps10-its-and-16s-at-once","dir":"Articles","previous_headings":"","what":"Demonstration of how to use demulticoder on a dataset that is actually three separate datasets (RPS10, ITS, and 16S) at once","title":"16S Mothur SOP Validation","text":"Input metadata primerinfo_params files data folder required columns first sample names, second primer name/barcode used. subsequent columns user-specific columns downstream steps metadata.csv file included necessary second file name barcode selected, primer sequences, optional DADA2 parameter options. referenced DADA2 tutorial select proper parameter options. Note, primers already trimmed reads, just certain, included Earth Microbiome primers described , primer sequences still found within small number reads. primerinfo_params.csv","code":""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-1-remove-ns-and-create-directory-structure-for-downstream-steps","dir":"Articles","previous_headings":"","what":"Step 1-Remove N’s and create directory structure for downstream steps","title":"16S Mothur SOP Validation","text":"","code":"options(mc.cores = 1)  outputs<-prepare_reads(   data_directory = \"~/benchmark_demulticoder/mothur_16S_sop/data\",    output_directory = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs\",   tempdir_path = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp\",   tempdir_id = \"temp_files\",   overwrite_existing = TRUE) #> Rows: 1 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 1 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 20 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, When #> dbl (1): Day #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Warning in get_read_names(paired_file_paths[1]) == #> get_read_names(paired_file_paths[2]): longer object length is not a multiple of #> shorter object length #> Creating output directory: /Users/masudermann/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/prefiltered_sequences"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-2-run-cutadapt-to-remove-primers-and-then-trim-reads-with-dada2-filterandtrim-function","dir":"Articles","previous_headings":"","what":"Step 2-Run Cutadapt to remove primers and then trim reads with DADA2 filterAndTrim function","title":"16S Mothur SOP Validation","text":"","code":"cut_trim(   outputs,   cutadapt_path=\"/opt/homebrew/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 4.1 for r16S sequence data  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D0_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D0_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D1_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D1_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D141_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D141_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D142_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D142_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D143_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D143_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D144_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D144_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D145_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D145_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D146_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D146_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D147_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D147_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D148_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D148_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D149_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D149_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D150_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D150_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D2_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D2_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D3_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D3_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D5_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D5_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D6_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D6_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D7_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D7_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D8_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D8_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D9_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/F3D9_R2_r16S.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/Mock_R1_r16S.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/trimmed_sequences/Mock_R2_r16S.fastq.gz #> Read in 7733 paired-sequences, output 7112 (92%) filtered paired-sequences. #> Read in 5829 paired-sequences, output 5299 (90.9%) filtered paired-sequences. #> Read in 5926 paired-sequences, output 5463 (92.2%) filtered paired-sequences. #> Read in 3158 paired-sequences, output 2914 (92.3%) filtered paired-sequences. #> Read in 3164 paired-sequences, output 2941 (93%) filtered paired-sequences. #> Read in 4798 paired-sequences, output 4312 (89.9%) filtered paired-sequences. #> Read in 7331 paired-sequences, output 6741 (92%) filtered paired-sequences. #> Read in 4993 paired-sequences, output 4560 (91.3%) filtered paired-sequences. #> Read in 16956 paired-sequences, output 15636 (92.2%) filtered paired-sequences. #> Read in 12332 paired-sequences, output 11412 (92.5%) filtered paired-sequences. #> Read in 13006 paired-sequences, output 12017 (92.4%) filtered paired-sequences. #> Read in 5474 paired-sequences, output 5032 (91.9%) filtered paired-sequences. #> Read in 19489 paired-sequences, output 18075 (92.7%) filtered paired-sequences. #> Read in 6726 paired-sequences, output 6250 (92.9%) filtered paired-sequences. #> Read in 4418 paired-sequences, output 4052 (91.7%) filtered paired-sequences. #> Read in 7933 paired-sequences, output 7369 (92.9%) filtered paired-sequences. #> Read in 5103 paired-sequences, output 4765 (93.4%) filtered paired-sequences. #> Read in 5274 paired-sequences, output 4871 (92.4%) filtered paired-sequences. #> Read in 7023 paired-sequences, output 6504 (92.6%) filtered paired-sequences. #> Read in 4748 paired-sequences, output 4314 (90.9%) filtered paired-sequences."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-3-core-asv-inference-step","dir":"Articles","previous_headings":"","what":"Step 3-Core ASV inference step","title":"16S Mothur SOP Validation","text":"","code":"make_asv_abund_matrix(   outputs,   overwrite_existing = TRUE) #> 33513360 total bases in 139639 reads from 20 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .................... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #> Convergence after  5  rounds. #> Error rate plot for the Forward read of primer pair r16S #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 7112 reads in 1978 unique sequences. #> Sample 2 - 5299 reads in 1639 unique sequences. #> Sample 3 - 5463 reads in 1477 unique sequences. #> Sample 4 - 2914 reads in 904 unique sequences. #> Sample 5 - 2941 reads in 939 unique sequences. #> Sample 6 - 4312 reads in 1267 unique sequences. #> Sample 7 - 6741 reads in 1756 unique sequences. #> Sample 8 - 4560 reads in 1438 unique sequences. #> Sample 9 - 15636 reads in 3589 unique sequences. #> Sample 10 - 11412 reads in 2761 unique sequences. #> Sample 11 - 12017 reads in 3021 unique sequences. #> Sample 12 - 5032 reads in 1566 unique sequences. #> Sample 13 - 18075 reads in 3707 unique sequences. #> Sample 14 - 6250 reads in 1479 unique sequences. #> Sample 15 - 4052 reads in 1195 unique sequences. #> Sample 16 - 7369 reads in 1832 unique sequences. #> Sample 17 - 4765 reads in 1183 unique sequences. #> Sample 18 - 4871 reads in 1382 unique sequences. #> Sample 19 - 6504 reads in 1709 unique sequences. #> Sample 20 - 4314 reads in 897 unique sequences. #> 22342240 total bases in 139639 reads from 20 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .................... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #>    selfConsist step 7 #> Convergence after  7  rounds. #> Error rate plot for the Reverse read of primer pair r16S #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 7112 reads in 1659 unique sequences. #> Sample 2 - 5299 reads in 1349 unique sequences. #> Sample 3 - 5463 reads in 1335 unique sequences. #> Sample 4 - 2914 reads in 853 unique sequences. #> Sample 5 - 2941 reads in 880 unique sequences. #> Sample 6 - 4312 reads in 1286 unique sequences. #> Sample 7 - 6741 reads in 1803 unique sequences. #> Sample 8 - 4560 reads in 1265 unique sequences. #> Sample 9 - 15636 reads in 3413 unique sequences. #> Sample 10 - 11412 reads in 2522 unique sequences. #> Sample 11 - 12017 reads in 2771 unique sequences. #> Sample 12 - 5032 reads in 1415 unique sequences. #> Sample 13 - 18075 reads in 3290 unique sequences. #> Sample 14 - 6250 reads in 1390 unique sequences. #> Sample 15 - 4052 reads in 1134 unique sequences. #> Sample 16 - 7369 reads in 1635 unique sequences. #> Sample 17 - 4765 reads in 1084 unique sequences. #> Sample 18 - 4871 reads in 1161 unique sequences. #> Sample 19 - 6504 reads in 1502 unique sequences. #> Sample 20 - 4314 reads in 732 unique sequences. #> 6540 paired-reads (in 107 unique pairings) successfully merged out of 6891 (in 197 pairings) input. #> 5027 paired-reads (in 101 unique pairings) successfully merged out of 5189 (in 157 pairings) input. #> 4986 paired-reads (in 81 unique pairings) successfully merged out of 5267 (in 166 pairings) input. #> 2595 paired-reads (in 52 unique pairings) successfully merged out of 2754 (in 108 pairings) input. #> 2553 paired-reads (in 60 unique pairings) successfully merged out of 2785 (in 119 pairings) input. #> 3646 paired-reads (in 55 unique pairings) successfully merged out of 4109 (in 157 pairings) input. #> 6079 paired-reads (in 81 unique pairings) successfully merged out of 6514 (in 198 pairings) input. #> 3968 paired-reads (in 91 unique pairings) successfully merged out of 4388 (in 187 pairings) input. #> 14233 paired-reads (in 143 unique pairings) successfully merged out of 15355 (in 352 pairings) input. #> 10529 paired-reads (in 120 unique pairings) successfully merged out of 11165 (in 277 pairings) input. #> 11154 paired-reads (in 137 unique pairings) successfully merged out of 11797 (in 298 pairings) input. #> 4349 paired-reads (in 85 unique pairings) successfully merged out of 4802 (in 179 pairings) input. #> 17431 paired-reads (in 153 unique pairings) successfully merged out of 17812 (in 272 pairings) input. #> 5850 paired-reads (in 81 unique pairings) successfully merged out of 6095 (in 159 pairings) input. #> 3713 paired-reads (in 86 unique pairings) successfully merged out of 3891 (in 147 pairings) input. #> 6865 paired-reads (in 99 unique pairings) successfully merged out of 7191 (in 187 pairings) input. #> 4428 paired-reads (in 68 unique pairings) successfully merged out of 4605 (in 128 pairings) input. #> 4576 paired-reads (in 101 unique pairings) successfully merged out of 4739 (in 174 pairings) input. #> 6092 paired-reads (in 109 unique pairings) successfully merged out of 6315 (in 173 pairings) input. #> 4269 paired-reads (in 20 unique pairings) successfully merged out of 4281 (in 28 pairings) input. #> Identified 61 bimeras out of 293 input sequences. #> $r16S #> [1] \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp/temp_files/asvabund_matrixDADA2_r16S.RData\""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-4-assign-taxonomy-step","dir":"Articles","previous_headings":"","what":"Step 4-Assign taxonomy step","title":"16S Mothur SOP Validation","text":"","code":"assign_tax(   outputs,   asv_abund_matrix,   db_16S=\"silva_nr99_v138.2_toSpecies_trainset.fa.gz\",   retrieve_files=FALSE,   overwrite_existing=FALSE) #> Warning in assign_tax(outputs, asv_abund_matrix, db_16S = #> \"silva_nr99_v138.2_toSpecies_trainset.fa.gz\", : No existing files found. The #> analysis will be run. #>    samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           F3D0_r16S  7733     7112      6976      6979   6540    6528 #> 2           F3D1_r16S  5829     5299      5226      5239   5027    5016 #> 3         F3D141_r16S  5926     5463      5331      5357   4986    4863 #> 4         F3D142_r16S  3158     2914      2799      2830   2595    2521 #> 5         F3D143_r16S  3164     2941      2822      2868   2553    2519 #> 6         F3D144_r16S  4798     4312      4150      4228   3646    3507 #> 7         F3D145_r16S  7331     6741      6592      6627   6079    5820 #> 8         F3D146_r16S  4993     4560      4450      4470   3968    3879 #> 9         F3D147_r16S 16956    15636     15433     15505  14233   13006 #> 10        F3D148_r16S 12332    11412     11250     11267  10529    9935 #> 11        F3D149_r16S 13006    12017     11857     11898  11154   10653 #> 12        F3D150_r16S  5474     5032      4879      4925   4349    4240 #> 13          F3D2_r16S 19489    18075     17907     17939  17431   16835 #> 14          F3D3_r16S  6726     6250      6145      6176   5850    5486 #> 15          F3D5_r16S  4418     4052      3930      3991   3713    3713 #> 16          F3D6_r16S  7933     7369      7231      7294   6865    6678 #> 17          F3D7_r16S  5103     4765      4646      4673   4428    4217 #> 18          F3D8_r16S  5274     4871      4786      4802   4576    4547 #> 19          F3D9_r16S  7023     6504      6341      6442   6092    6015 #> 20          Mock_r16S  4748     4314      4287      4286   4269    4269"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-5-convert-asv-matrix-to-taxmap-and-phyloseq-objects-with-one-function","dir":"Articles","previous_headings":"","what":"Step 5-convert asv matrix to taxmap and phyloseq objects with one function","title":"16S Mothur SOP Validation","text":"","code":"objs<-convert_asv_matrix_to_objs(outputs, save_outputs=TRUE, overwrite_existing = TRUE) #> Rows: 232 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (20): F3D0_r16S, F3D1_r16S, F3D141_r16S, F3D142_r16S, F3D143_r16S, F3D14... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For r16S dataset  #> Taxmap object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/taxmap_obj_r16S.RData  #> Phyloseq object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/phylo_obj_r16S.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-6-evaluate-accuracy-using-mock-community-as-shown-in-dada2-tutorial","dir":"Articles","previous_headings":"","what":"Step 6-evaluate accuracy using mock community, as shown in dada2 tutorial","title":"16S Mothur SOP Validation","text":"looking mock community sample, able extract 20 bacterial sequences 0% mismatch, matched described previously.","code":"track_reads_demulticoder<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/track_reads_r16S.csv\", row.names = 1) summary(track_reads_demulticoder) #>      input          filtered       denoisedF       denoisedR     #>  Min.   : 3158   Min.   : 2914   Min.   : 2799   Min.   : 2830   #>  1st Qu.: 4944   1st Qu.: 4498   1st Qu.: 4409   1st Qu.: 4424   #>  Median : 5878   Median : 5381   Median : 5278   Median : 5298   #>  Mean   : 7571   Mean   : 6982   Mean   : 6852   Mean   : 6890   #>  3rd Qu.: 7783   3rd Qu.: 7176   3rd Qu.: 7040   3rd Qu.: 7058   #>  Max.   :19489   Max.   :18075   Max.   :17907   Max.   :17939   #>      merged         nonchim      #>  Min.   : 2553   Min.   : 2519   #>  1st Qu.: 4194   1st Qu.: 4132   #>  Median : 5006   Median : 4940   #>  Mean   : 6444   Mean   : 6212   #>  3rd Qu.: 6621   3rd Qu.: 6566   #>  Max.   :17431   Max.   :16835  tax_matrix<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/final_asv_abundance_matrix_r16S.csv\")  unqs.mock <- tax_matrix[, c(2, which(colnames(tax_matrix) == \"Mock_r16S\"))]  unqs.mock <- unqs.mock[unqs.mock$Mock_r16S != 0,]  cat(\"DADA2 inferred\", nrow(unqs.mock), \"sample sequences present in the Mock community.\\n\") #> DADA2 inferred 20 sample sequences present in the Mock community.  mock.ref <- dada2::getSequences(file.path(\"~/benchmark_demulticoder/mothur_16S_sop/data\", \"HMP_MOCK.v35.fasta\")) match.ref <- sum(sapply(unqs.mock$sequence, function(x) any(grepl(x, mock.ref)))) cat(\"Of those,\", sum(match.ref), \"were exact matches to the expected reference sequences.\\n\") #> Of those, 20 were exact matches to the expected reference sequences."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-7-follow-up-work-using-phyloseq-to-do-side-by-side-comparison-with-dada2-example-and-to-examine-alpha-diversity-results","dir":"Articles","previous_headings":"","what":"Step 7-Follow-up work using phyloseq to do side-by-side comparison with dada2 example and to examine alpha diversity results","title":"16S Mothur SOP Validation","text":"","code":"objs$phyloseq_r16S <- phyloseq::prune_samples(phyloseq::sample_names(objs$phyloseq_r16S) != \"Mock_r16S\", objs$phyloseq_r16S) # Remove mock sample phyloseq::plot_richness(objs$phyloseq_r16S, x=\"Day\", measures=c(\"Shannon\", \"Simpson\"), color=\"When\") #> Warning in estimate_richness(physeq, split = TRUE, measures = measures): The data you have provided does not have #> any singletons. This is highly suspicious. Results of richness #> estimates (for example) are probably unreliable, or wrong, if you have already #> trimmed low-abundance taxa from the data. #>  #> We recommended that you find the un-trimmed data and retry."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-8-examine-ordination-plots-as-additional-point-of-comparison-with-dada2-tutorial","dir":"Articles","previous_headings":"","what":"Step 8-Examine ordination plots as additional point of comparison with DADA2 tutorial","title":"16S Mothur SOP Validation","text":"","code":"# Transform data to proportions as appropriate for Bray-Curtis distances ps.prop <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(otu) otu/sum(otu)) ord.nmds.bray <- phyloseq::ordinate(ps.prop, method=\"NMDS\", distance=\"bray\") #> Run 0 stress 0.0808378  #> Run 1 stress 0.09061366  #> Run 2 stress 0.1231378  #> Run 3 stress 0.121153  #> Run 4 stress 0.08635462  #> Run 5 stress 0.0808378  #> ... Procrustes: rmse 5.812942e-06  max resid 1.554258e-05  #> ... Similar to previous best #> Run 6 stress 0.1231378  #> Run 7 stress 0.09061366  #> Run 8 stress 0.08096389  #> ... Procrustes: rmse 0.009176341  max resid 0.02810282  #> Run 9 stress 0.1433231  #> Run 10 stress 0.0808378  #> ... Procrustes: rmse 5.646215e-06  max resid 1.499963e-05  #> ... Similar to previous best #> Run 11 stress 0.1231378  #> Run 12 stress 0.0809639  #> ... Procrustes: rmse 0.009215858  max resid 0.02823353  #> Run 13 stress 0.121153  #> Run 14 stress 0.1320722  #> Run 15 stress 0.08635462  #> Run 16 stress 0.121153  #> Run 17 stress 0.08096389  #> ... Procrustes: rmse 0.009186834  max resid 0.02813753  #> Run 18 stress 0.08635462  #> Run 19 stress 0.08635462  #> Run 20 stress 0.09466617  #> *** Best solution repeated 2 times phyloseq::plot_ordination(ps.prop, ord.nmds.bray, color=\"When\", title=\"Bray NMDS\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-9-lets-look-at-what-the-top-20-taxa-are-in-the-early-vs--late-samples-time-points-as-shown-in-the-dada2-tutorial","dir":"Articles","previous_headings":"","what":"Step 9-Let’s look at what the top 20 taxa are in the early vs. late samples time points, as shown in the dada2 tutorial","title":"16S Mothur SOP Validation","text":"","code":"top20 <- names(sort(phyloseq::taxa_sums(objs$phyloseq_r16S), decreasing=TRUE))[1:20] ps.top20 <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(OTU) OTU/sum(OTU)) ps.top20 <- phyloseq::prune_taxa(top20, ps.top20) phyloseq::plot_bar(ps.top20, x=\"Day\", fill=\"Family\") + ggplot2::facet_wrap(~When, scales=\"free_x\")"},{"path":"/articles/Documentation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Documentation","text":"documentation provides comprehensive information use Demulticoder R package processing analyzing metabarcode sequencing data. covers input file requirements, parameter settings, key parameters.","code":""},{"path":"/articles/Documentation.html","id":"quick-start-guide","dir":"Articles","previous_headings":"","what":"Quick Start Guide","title":"Documentation","text":"Prepare input files (metadata.csv, primerinfo_params.csv, unformatted reference databases, PE Illumina read files). Place input files single directory. Ensure file names comply specified format. Run pipeline default settings adjust parameters needed.","code":""},{"path":[]},{"path":"/articles/Documentation.html","id":"directory-structure","dir":"Articles","previous_headings":"Input Files","what":"Directory Structure","title":"Documentation","text":"Place input files single directory. directory contain following files: metadata.csv primerinfo_params.csv PE Illumina read files Unformatted reference databases","code":""},{"path":[]},{"path":"/articles/Documentation.html","id":"read-name-format","dir":"Articles","previous_headings":"Input Files > File Naming Conventions","what":"Read Name Format","title":"Documentation","text":"avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz","code":""},{"path":"/articles/Documentation.html","id":"metadata-csv","dir":"Articles","previous_headings":"Input Files","what":"metadata.csv","title":"Documentation","text":"metadata.csv file contains information samples primers (associated barcodes) used experiment. following two required columns: sample_name: Identifier sample (e.g., S1, S2) primer_name: Name primer used (e.g., rps10, , r16S, other1, oteher2) Please add associated metadata file two required columns. can used downstream exploratory diversity analyses, sample data incorporated final phyloseq taxmap objects. Example file (optional third column):","code":"sample_name,primer_name,organism S1,rps10,Cry S2,rps10,Cin S1,its,Cry S2,its,Cin"},{"path":"/articles/Documentation.html","id":"primerinfo_params-csv","dir":"Articles","previous_headings":"Input Files","what":"primerinfo_params.csv","title":"Documentation","text":"primerinfo_params.csv file contains information primer sequences used experiment, along optional additional parameters part DADA2 pipeline. anything specified, default values used. Required columns: primer_name: Name primer/barcode (e.g., , rps10) forward: Forward primer sequence reverse: Reverse primer sequence DADA2 filterAndTrim function parameters: already_trimmed: Boolean indicating primers already trimmed (TRUE/FALSE) (default: FALSE) minCutadaptlength: Cutadapt parameter-Minimum length Cutadapt trimming (default: 0) multithread: Boolean multithreading (TRUE/FALSE) (default: FALSE) verbose: Boolean verbose output (TRUE/FALSE) (default: FALSE) maxN: Maximum number N bases allowed (default: 0) maxEE_forward: Maximum expected errors forward reads (default: Inf) maxEE_reverse: Maximum expected errors reverse reads (default: Inf) truncLen_forward: Truncation length forward reads (default: 0) truncLen_reverse: Truncation length reverse reads (default: 0) truncQ: Truncation quality threshold (default: 2) minLen: Minimum length reads processing (default: 20) maxLen: Maximum length reads processing (default: Inf) minQ: Minimum quality score (default: 0) trimLeft: Number bases trim start reads (default: 0) trimRight: Number bases trim end reads (default: 0) rm.lowcomplex: Boolean removing low complexity sequences (default: TRUE) DADA2 learnErrors function parameters: nbases: Number bases use error rate learning (default: 1e+08) randomize: Randomize reads error rate learning (default: FALSE) MAX_CONSIST: Maximum number self-consistency iterations (default: 10) OMEGA_C: Convergence threshold error rates (default: 0) qualityType: Quality score type (“Auto”, “FastqQuality”, “ShortRead”) (default: “Auto”) DADA2 plot errors parameters: err_out: Return error rates used inference (default: TRUE) err_in: Use input error rates instead learning (default: FALSE) nominalQ: Use nominal Q-scores (default: FALSE) obs: Return observed error rates (default: TRUE) DADA2 dada function parameters: OMP: Use OpenMP multi-threading available (default: TRUE) n: Number reads use error rate estimation (default: 1e+05) id.sep: Character separating sample ID sequence name (default: “\\s”) orient.fwd: NULL TRUE/FALSE orient sequences (default: NULL) pool: Pool samples error rate estimation (default: FALSE) selfConsist: Perform self-consistency iterations (default: FALSE) DADA2 mergePairs function parameters: minOverlap: Minimum overlap merging paired-end reads (default: 12) maxMismatch: Maximum mismatches allowed overlap region (default: 0) DADA2 removeBimeraDenovo function parameters: method: Method sample inference (“consensus” “pooled”) (default: “consensus”) parameters include CSV input file: min_asv_length: Minimum length Amplicon Sequence Variants (ASVs) core dada ASV inference steps (default=0) Example file (select optional columns forward reverse primer sequence columns):","code":"primer_name,forward,reverse,already_trimmed,minCutadaptlength,multithread,verbose,maxN,maxEE_forward,maxEE_reverse,truncLen_forward,truncLen_reverse,truncQ,minLen,maxLen,minQ,trimLeft,trimRight,rm.lowcomplex,minOverlap,maxMismatch,min_asv_length rps10,GTTGGTTAGAGYARAAGACT,ATRYYTAGAAAGAYTYGAACT,FALSE,100,TRUE,FALSE,1.00E+05,5,5,0,0,5,150,Inf,0,0,0,0,15,0,50 its,CTTGGTCATTTAGAGGAAGTAA,GCTGCGTTCTTCATCGATGC,FALSE,50,TRUE,FALSE,1.00E+05,5,5,0,0,5,50,Inf,0,0,0,0,15,0,50"},{"path":"/articles/Documentation.html","id":"reference-database","dir":"Articles","previous_headings":"Input Files","what":"Reference Database","title":"Documentation","text":"Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function. now, package compatible following databases: oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ UNITE fungal database https://unite.ut.ee/repository.php two reference databases. user need reformat headers exactly outlined DADA2 database format, similar SILVA database format. user can specify path database input file. database fasta format.","code":""},{"path":"/articles/Documentation.html","id":"faq","dir":"Articles","previous_headings":"","what":"FAQ","title":"Documentation","text":"progress","code":""},{"path":"/articles/Documentation.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Documentation","text":"progress","code":""},{"path":"/articles/Getting_started.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before You Start","title":"Getting Started","text":"following example, demonstrate key package functionality using subset reads two samples containing pooled ITS1 fungal rps10 oomycete amplicons. databases used assign taxonomy step also abridged versions full UNITE oomyceteDB databases. can follow along test data associated CSV input files loaded package. Additional examples also available website. Please note, speed, test dataset comprised randomly subset reads samples (S1 S2), due database size, full UNITE database included package, also smaller subset larger database. need prepare raw read files fill metadata.csv primerinfo_params.csv templates.","code":""},{"path":"/articles/Getting_started.html","id":"format-of-the-pe-amplicon-files","dir":"Articles","previous_headings":"","what":"Format of the PE amplicon files","title":"Getting Started","text":"package takes forward reverse Illumina short read sequence data. avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz.","code":""},{"path":"/articles/Getting_started.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"","what":"Format of metadata file (metadata.csv)","title":"Getting Started","text":"format CSV file simple. template . two necessary columns (names) : sample_name column primer_info column additional metadata pasted two columns. can referenced later analysis steps save step loading metadata later. S1 S2 come rhododendron rhizobiome dataset random subset reads. notice S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled reads (rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. Example using test dataset:","code":""},{"path":"/articles/Getting_started.html","id":"format-of-primer-and-parameters-file-primerinfo_parms-csv","dir":"Articles","previous_headings":"","what":"Format of primer and parameters file (primerinfo_parms.csv)","title":"Getting Started","text":"DADA2 Primer sequence information user-defined parameters placed primerinfo_params.csv. simplify functions called, user provide parameters within input file. recommend using template linked . Remember add additional optional DADA2 parameters want use. required columns user must fill : 1.primer_name compatible options currently (add cells written crucial database formatting step): rps10, , r16S, other1, other2 2.forward-forward sequence 3.reverse-reverse sequence Example template ‘primerinfo_params.csv’ info parameter specifics, see Documentation tab.","code":""},{"path":"/articles/Getting_started.html","id":"reference-database-format","dir":"Articles","previous_headings":"","what":"Reference Database Format","title":"Getting Started","text":"now, package compatible following databases: oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ UNITE fungal database https://unite.ut.ee/repository.php user can select two databases, first need reformat headers exactly like SILVA database. See ‘Documentation’ tab. Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function","code":""},{"path":"/articles/Getting_started.html","id":"additional-notes","dir":"Articles","previous_headings":"","what":"Additional Notes","title":"Getting Started","text":"Computer specifications may limiting factor. using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases comprised randomly subset reads. following example, run personal computer least 16 GB RAM. Users need upload databases input data folder. computer crashes taxonomic assignment step, need switch computer sufficient memory. Please also ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/Getting_started.html","id":"loading-the-package","dir":"Articles","previous_headings":"","what":"Loading the Package","title":"Getting Started","text":"now, package loaded retrieving GitHub. Eventually, package uploaded CRAN Bioconductor.","code":"#devtools::install_github(\"grunwaldlab/demulticoder\") devtools::load_all(\"~/demulticoder\") library(\"demulticoder\") library(\"metacoder\") library(\"dplyr\")"},{"path":"/articles/Getting_started.html","id":"reorganize-data-tables-and-set-up-data-directory-structure","dir":"Articles","previous_headings":"","what":"Reorganize data tables and set-up data directory structure","title":"Getting Started","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    tempdir_path = \"~/temp_test_dataset\",   tempdir_id = \"test_dataset\",   overwrite_existing = TRUE)"},{"path":"/articles/Getting_started.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"","what":"Remove primers with Cutadapt","title":"Getting Started","text":"running Cutadapt, please ensure installed .","code":"demulticoder::cut_trim(   analysis_setup,   cutadapt_path = \"/opt/homebrew/bin/cutadapt\",   #cutadapt_path = \"/usr/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 4.1 for its sequence data #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 4.1 for rps10 sequence data #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences."},{"path":"/articles/Getting_started.html","id":"asv-inference-step","dir":"Articles","previous_headings":"","what":"ASV inference step","title":"Getting Started","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = TRUE) #> 710847 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 654 unique sequences. #> Sample 2 - 1215 reads in 610 unique sequences. #> 724232 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input. #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input. #> Identified 0 bimeras out of 5 input sequences. #> $its #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/Getting_started.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"","what":"Taxonomic assignment step","title":"Getting Started","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing = TRUE) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/Getting_started.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Getting Started","text":"","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, minimum_bootstrap = 0, save_outputs = TRUE) #> Rows: 38 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 5 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":[]},{"path":"/articles/Getting_started.html","id":"objects-can-now-be-used-for-downstream-data-analysis","dir":"Articles","previous_headings":"","what":"Objects can now be used for downstream data analysis","title":"Getting Started","text":"make heattrees using taxmap object. First make heat tree -barcoded samples  Now make heat tree rps10-barcoded samples  can also variety analyses, convert phyloseq object demonstrate make stacked bar plot relative abundance taxa sample -barcoded samples  Finally,demonstrate make stacked bar plot relative abundance taxa sample rps10-barcoded samples","code":"objs$taxmap_its %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") objs$taxmap_rps10 %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated.  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martha . Sudermann. Author, maintainer. Zachary S. L Foster. Author. Samantha Dawson. Author. Hung Phan. Author. Jeff H. Chang. Author. Niklaus Grünwald. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sudermann M, Foster Z, Dawson S, Phan H, H. Chang J, Grünwald N (2025). demulticoder: R Package Integrated Analysis Multiplex Metabarcodes. R package version 0.0.0.9000, https://grunwaldlab.github.io/demulticoder/.","code":"@Manual{,   title = {demulticoder: An R Package for the Integrated Analysis of Multiplex Metabarcodes},   author = {Martha A. Sudermann and Zachary S. L Foster and Samantha Dawson and Hung Phan and Jeff {H. Chang} and Niklaus Grünwald},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://grunwaldlab.github.io/demulticoder/}, }"},{"path":"/index.html","id":"demulticoder-r-package","dir":"","previous_headings":"","what":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package actively development. message removed, use caution. Additional testing, documentation, examples progress.","code":""},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"demulticoder package Cutadapt DADA2 wrapper package metabarcodng analyses. main commands outputs intuitive comprehensive, helps account complex iterative nature metabarcoding analyses. brief schematic general workflow:","code":""},{"path":"/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key features","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"ability analysis either demultiplexed pooled amplicons within samples Amplicons multiple datasets trimmed primers, filtered, denoised, merged, given taxonomic assignments one go (different parameters dataset desired) package handles just 16S datasets using default UNITE fungal Silva 16S databases also oomycete rps10 analyses using oomycetedb (https://oomycetedb.org), two custom databases (provided formatted described : https://benjjneb.github.io/dada2/training.html).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"install development version package:","code":"devtools::install_github(\"grunwaldlab/demulticoder\")"},{"path":"/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick start","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"1. Set-input directory files installing package, make data directory add following files: - PE short read amplicon data. files must end either *_R1.fastq.gz* , *_R2.fastq.gz* sample must R1 R2 files. metadata.csv file (unique row sample, samples entered twice contain pooled amplicons, example template) primerinfo_params.csv file (new row unique barcode associated primer sequences, also optional Cutadapt, DADA2 filtering parameters can added adjusted) 2. Prepare reads 3. Cut trim reads 4. Make ASV abundance matrix 5. Assign taxonomy 6. Convert ASV matrix taxmap phyloseq objects","code":"output<-prepare_reads(   data_directory = \"<DATADIR>\",   output_directory = \"<OUTDIR>\") cut_trim(   output,   cutadapt_path=\"<CUTADAPTPATH>\") make_asv_abund_matrix(   output) assign_tax(   output,   asv_abund_matrix) objs<-convert_asv_matrix_to_objs(output)"},{"path":"/index.html","id":"check-out-the-website-to-view-the-documentation-and-see-more-examples","dir":"","previous_headings":"","what":"Check out the website to view the documentation and see more examples","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"information, key functions, inputs, example vignettes, check documentation : https://grunwaldlab.github.io/demulticoder","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package developed Martha Sudermann, Zachary Foster, Samantha Dawson, Hung Phan, Jeff Chang, Niklaus Grünwald Stay tuned associated manuscript.","code":""},{"path":"/reference/add_pid_to_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Add PID and bootstrap values to tax result. — add_pid_to_tax","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"Add PID bootstrap values tax result.","code":""},{"path":"/reference/add_pid_to_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"","code":"add_pid_to_tax(tax_results, asv_pid)"},{"path":"/reference/add_pid_to_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"tax_results dataframe containing taxonomic assignments asv_pid Percent identity information ASV relative reference database sequence","code":""},{"path":"/reference/assignTax_as_char.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"Combine taxonomic assignments bootstrap values locus single falsification vector","code":""},{"path":"/reference/assignTax_as_char.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"","code":"assignTax_as_char(tax_results, temp_directory_path, locus)"},{"path":"/reference/assignTax_as_char.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"tax_results dataframe containing taxonomic assignments","code":""},{"path":"/reference/assign_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy functions — assign_tax","title":"Assign taxonomy functions — assign_tax","text":"Assign taxonomy functions","code":""},{"path":"/reference/assign_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy functions — assign_tax","text":"","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   retrieve_files = FALSE,   overwrite_existing = FALSE,   db_rps10 = \"oomycetedb.fasta\",   db_its = \"fungidb.fasta\",   db_16S = \"bacteriadb.fasta\",   db_other1 = \"otherdb1.fasta\",   db_other2 = \"otherdb2.fasta\" )"},{"path":"/reference/assign_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy functions — assign_tax","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function asv_abund_matrix ASV abundance matrix. tryRC Whether try reverse complementing sequences taxonomic assignment verbose Logical, indicating whether display verbose output multithread Logical, indicating whether use multithreading retrieve_files Specify TRUE/FALSE whether copy files temp directory output directory overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. db_rps10 reference database rps10 locus db_its reference database locus db_16S reference database 16S locus db_other1 reference database different locus 1 (assumes format like SILVA DB entries) db_other2 reference database different locus 2 (assumes format like SILVA DB entries)","code":""},{"path":"/reference/assign_tax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign taxonomy functions — assign_tax","text":"Taxonomic assignments unique ASV sequence","code":""},{"path":"/reference/assign_tax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign taxonomy functions — assign_tax","text":"","code":"# Assign taxonomies to ASVs on a per barcode basis analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpgaR3Jq/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Error in system2(cutadapt, args = \"--version\", stdout = TRUE, stderr = TRUE): error in running command make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> Error in 1:i: argument of length 0 assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing = TRUE ) #> Warning: cannot open compressed file '/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpgaR3Jq/demulticoder_run_temp/asvabund_matrixDADA2_its.RData', probable reason 'No such file or directory' #> Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection"},{"path":"/reference/assign_taxonomyDada2.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy — assign_taxonomyDada2","title":"Assign taxonomy — assign_taxonomyDada2","text":"Assign taxonomy","code":""},{"path":"/reference/assign_taxonomyDada2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy — assign_taxonomyDada2","text":"","code":"assign_taxonomyDada2(   asv_abund_matrix,   temp_directory_path,   minBoot = 0,   tryRC = FALSE,   verbose = FALSE,   multithread = TRUE,   locus = \"barcode\" )"},{"path":"/reference/assign_taxonomyDada2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy — assign_taxonomyDada2","text":"asv_abund_matrix ASV abundance matrix temp_directory_path temporary directory path minBoot Minimum bootstrap value taxonomy assignment (default 0) tryRC Try reverse complement (default FALSE) verbose Print additional information (default FALSE) multithread Use multiple threads (default TRUE) locus locus taxonomy assignment (e.g., rps10, other1, other2)","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"Filter ASV abundance matrix convert taxmap phyloseq objects","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"convert_asv_matrix_to_objs(   analysis_setup,   min_read_depth = 0,   minimum_bootstrap = 0,   save_outputs = FALSE,   overwrite_existing = FALSE )"},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function min_read_depth ASV filter parameter. mean read depth across samples less threshold, ASV filtered. minimum_bootstrap Threshold bootstrap support value taxonomic assignments. designated minimum bootstrap threshold, taxnomoic assignments set N/save_outputs Logical, indicating whether save taxmap object. Default FALSE. overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"ASV matrix converted taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"# Convert final matrix to taxmap and phyloseq objects for downstream analysis steps analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpgaR3Jq/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Error in system2(cutadapt, args = \"--version\", stdout = TRUE, stderr = TRUE): error in running command make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> Error in 1:i: argument of length 0 assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing=TRUE ) #> Warning: cannot open compressed file '/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpgaR3Jq/demulticoder_run_temp/asvabund_matrixDADA2_its.RData', probable reason 'No such file or directory' #> Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection objs<-convert_asv_matrix_to_objs( analysis_setup )"},{"path":"/reference/countOverlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Count overlap to see how well the reads were merged — countOverlap","title":"Count overlap to see how well the reads were merged — countOverlap","text":"Count overlap see well reads merged","code":""},{"path":"/reference/countOverlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count overlap to see how well the reads were merged — countOverlap","text":"","code":"countOverlap(data_tables, merged_reads, barcode, output_directory_path)"},{"path":"/reference/countOverlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count overlap to see how well the reads were merged — countOverlap","text":"data_tables data tables containing paths read files, metadata, primer sequences merged_reads Intermediate merged read R data file barcode barcode used analysis output_directory_path path directory resulting files output","code":""},{"path":"/reference/countOverlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count overlap to see how well the reads were merged — countOverlap","text":"plot describing well reads merged information overlap reads","code":""},{"path":"/reference/createASVSequenceTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Make ASV sequence matrix — createASVSequenceTable","title":"Make ASV sequence matrix — createASVSequenceTable","text":"Make ASV sequence matrix","code":""},{"path":"/reference/createASVSequenceTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make ASV sequence matrix — createASVSequenceTable","text":"","code":"createASVSequenceTable(merged_reads, orderBy = \"abundance\")"},{"path":"/reference/createASVSequenceTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make ASV sequence matrix — createASVSequenceTable","text":"merged_reads Intermediate merged read R data file orderBy (Optional). character(1). Default \"abundance\". Specifies sequences (columns) returned table ordered (decreasing). Valid values: \"abundance\", \"nsamples\", NULL.","code":""},{"path":"/reference/createASVSequenceTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make ASV sequence matrix — createASVSequenceTable","text":"raw_seqtab","code":""},{"path":"/reference/cut_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"Main command trim primers using Cutadapt core DADA2 functions. samples contain pooled barcodes, reads also demultiplexed","code":""},{"path":"/reference/cut_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"","code":"cut_trim(analysis_setup, cutadapt_path, overwrite_existing = FALSE)"},{"path":"/reference/cut_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function cutadapt_path Path Cutadapt program. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/cut_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"Trimmed reads, primer counts, quality plots, ASV matrix.","code":""},{"path":"/reference/cut_trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"","code":"# Remove remaining primers from raw reads, demultiplex pooled barcoded samples,  # and then trim reads based on specific DADA2 parameters analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpgaR3Jq/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\",  overwrite_existing = TRUE ) #> Error in system2(cutadapt, args = \"--version\", stdout = TRUE, stderr = TRUE): error in running command"},{"path":"/reference/filter_and_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Wrapper function filterAndTrim function DADA2, used primer trimming","code":""},{"path":"/reference/filter_and_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"","code":"filter_and_trim(   output_directory_path,   temp_directory_path,   cutadapt_data_barcode,   barcode_params,   barcode )"},{"path":"/reference/filter_and_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"output_directory_path path directory resulting files output cutadapt_data_barcode directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/filter_and_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Filtered trimmed reads","code":""},{"path":"/reference/format_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Format ASV abundance matrix — format_abund_matrix","title":"Format ASV abundance matrix — format_abund_matrix","text":"Format ASV abundance matrix","code":""},{"path":"/reference/format_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format ASV abundance matrix — format_abund_matrix","text":"","code":"format_abund_matrix(   data_tables,   asv_abund_matrix,   seq_tax_asv,   output_directory_path,   locus )"},{"path":"/reference/format_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format ASV abundance matrix — format_abund_matrix","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants seq_tax_asv amplified sequence variants matrix taxonomic information","code":""},{"path":"/reference/format_database.html","id":null,"dir":"Reference","previous_headings":"","what":"General functions to format user-specified databases — format_database","title":"General functions to format user-specified databases — format_database","text":"General functions format user-specified databases","code":""},{"path":"/reference/format_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General functions to format user-specified databases — format_database","text":"","code":"format_database(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   barcode,   db_its,   db_rps10,   db_16S,   db_other1,   db_other2 )"},{"path":"/reference/format_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General functions to format user-specified databases — format_database","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files barcode barcode database formatted","code":""},{"path":"/reference/format_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General functions to format user-specified databases — format_database","text":"formatted database based specified barcode type","code":""},{"path":"/reference/format_db_16S.html","id":null,"dir":"Reference","previous_headings":"","what":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"16S database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_16S.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"","code":"format_db_16S(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_16S )"},{"path":"/reference/format_db_16S.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_16S name database","code":""},{"path":"/reference/format_db_16S.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"16S database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":null,"dir":"Reference","previous_headings":"","what":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"","code":"format_db_its(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_its )"},{"path":"/reference/format_db_its.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_its name database","code":""},{"path":"/reference/format_db_its.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other1.html","id":null,"dir":"Reference","previous_headings":"","what":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":", user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"","code":"format_db_other1(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other1 )"},{"path":"/reference/format_db_other1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other1 name database","code":""},{"path":"/reference/format_db_other1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other2.html","id":null,"dir":"Reference","previous_headings":"","what":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"second user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"","code":"format_db_other2(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other2 )"},{"path":"/reference/format_db_other2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other2 name database","code":""},{"path":"/reference/format_db_other2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_rps10.html","id":null,"dir":"Reference","previous_headings":"","what":"Create modified reference rps10 database for downstream analysis — format_db_rps10","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"Create modified reference rps10 database downstream analysis","code":""},{"path":"/reference/format_db_rps10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"","code":"format_db_rps10(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_rps10 )"},{"path":"/reference/format_db_rps10.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_rps10 name database","code":""},{"path":"/reference/format_db_rps10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"rps10 database modified headers output reference_databases folder.","code":""},{"path":"/reference/get_fastq_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"Retrieve paths filtered trimmed Fastq files","code":""},{"path":"/reference/get_fastq_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"","code":"get_fastq_paths(data_tables, my_direction, my_primer_pair_id)"},{"path":"/reference/get_fastq_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Whether primer forward reverse direction my_primer_pair_id specific barcode id cutadapt_data directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/get_pids.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"Align ASV sequences reference sequences database get percent ID. Get percent identities.","code":""},{"path":"/reference/get_pids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"","code":"get_pids(tax_results, temp_directory_path, output_directory_path, db, locus)"},{"path":"/reference/get_pids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"tax_results data frame containing taxonomic assignments","code":""},{"path":"/reference/get_post_trim_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_post_trim_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"","code":"get_post_trim_hits(primer_data, cutadapt_data, output_directory_path)"},{"path":"/reference/get_post_trim_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"primer_data primer data frame created orient_primers function cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_post_trim_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Table read counts across sample","code":""},{"path":"/reference/get_pre_primer_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"","code":"get_pre_primer_hits(primer_data, fastq_data, output_directory_path)"},{"path":"/reference/get_pre_primer_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"primer_data primer data data frame created orient_primers function fastq_data data frame FASTQ file paths, direction sequences, names sequences output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"number reads primer found number reads primer found","code":""},{"path":"/reference/get_read_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"Final inventory read counts step input removal chimeras. function deals one sample. TODO optimize one sample","code":""},{"path":"/reference/get_read_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"","code":"get_read_counts(   asv_abund_matrix,   temp_directory_path,   output_directory_path,   locus )"},{"path":"/reference/get_read_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"asv_abund_matrix abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/get_ref_seq.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"Align ASV sequences reference sequences database get percent ID. Start retrieving reference sequences.","code":""},{"path":"/reference/get_ref_seq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"","code":"get_ref_seq(tax_results, db)"},{"path":"/reference/get_ref_seq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"tax_results dataframe containing taxonomic assignments db reference database","code":""},{"path":"/reference/infer_asv_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to infer ASVs, for multiple loci — infer_asv_command","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"Function infer ASVs, multiple loci","code":""},{"path":"/reference/infer_asv_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"","code":"infer_asv_command(   output_directory_path,   temp_directory_path,   data_tables,   barcode_params,   barcode )"},{"path":"/reference/infer_asv_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"output_directory_path path directory resulting files output data_tables data tables containing paths read files, metadata, primer sequences denoised_data_path Path saved intermediate denoised data","code":""},{"path":"/reference/infer_asvs.html","id":null,"dir":"Reference","previous_headings":"","what":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"Core DADA2 function learn errors infer ASVs","code":""},{"path":"/reference/infer_asvs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"","code":"infer_asvs(   data_tables,   my_direction,   my_primer_pair_id,   barcode_params,   output_directory_path )"},{"path":"/reference/infer_asvs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Location read files metadata file my_primer_pair_id specific barcode id output_directory_path path directory containing fastq, metadata, primerinfo_params files","code":""},{"path":"/reference/infer_asvs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"asv_data","code":""},{"path":"/reference/make_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"Quality filtering remove chimeras short sequences","code":""},{"path":"/reference/make_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"","code":"make_abund_matrix(   raw_seqtab,   temp_directory_path,   barcode_params = barcode_params,   barcode )"},{"path":"/reference/make_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"raw_seqtab R data file raw sequence data prior removal chimeras","code":""},{"path":"/reference/make_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"Make amplified sequence variant (ASV) abundance matrix function generates ASV abundance matrix using raw reads processed previous steps, including read preparation, removing primers, using DADA2 core denoising alogrithm infer ASVs.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"","code":"make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE)"},{"path":"/reference/make_asv_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"ASV abundance matrix (asv_abund_matrix)","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"function processes data unique barcode separately, inferring ASVs, merging reads, creating ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"","code":"# The primary wrapper function for DADA2 ASV inference steps analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpgaR3Jq/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Error in system2(cutadapt, args = \"--version\", stdout = TRUE, stderr = TRUE): error in running command make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> Error in 1:i: argument of length 0"},{"path":"/reference/make_cutadapt_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Prepare primmer trimming Cutaapt. Make new sub-directories specify paths trimmed untrimmed reads","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"","code":"make_cutadapt_tibble(fastq_data, metadata, temp_directory_path)"},{"path":"/reference/make_cutadapt_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"fastq_data path FASTQ files analysis metadata, primer_info files metadata Loaded metadata pairing user's metadata file primer data temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Returns larger data frame containing paths temporary read directories, used input running Cutadapt","code":""},{"path":"/reference/make_seqhist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"Plots histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/make_seqhist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"","code":"make_seqhist(asv_abund_matrix, output_directory_path)"},{"path":"/reference/make_seqhist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_seqhist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/merge_reads_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge forward and reverse reads — merge_reads_command","title":"Merge forward and reverse reads — merge_reads_command","text":"Merge forward reverse reads","code":""},{"path":"/reference/merge_reads_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge forward and reverse reads — merge_reads_command","text":"","code":"merge_reads_command(   output_directory_path,   temp_directory_path,   barcode_params,   barcode )"},{"path":"/reference/merge_reads_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge forward and reverse reads — merge_reads_command","text":"output_directory_path path directory resulting files output merged_read_data_path Path R data file containing merged read data","code":""},{"path":"/reference/merge_reads_command.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge forward and reverse reads — merge_reads_command","text":"merged_reads Intermediate merged read R data file","code":""},{"path":"/reference/orient_primers.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"Take user's forward reverse sequences creates complement, reverse, reverse complement primers one data frame","code":""},{"path":"/reference/orient_primers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"","code":"orient_primers(primers_params_path)"},{"path":"/reference/orient_primers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/orient_primers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"data frame oriented primer information.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Wrapper script plotQualityProfile trim steps primer removal.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"","code":"plot_post_trim_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_post_trim_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Quality profiles reads primer trimming","code":""},{"path":"/reference/plot_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for plotQualityProfile function — plot_qc","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Wrapper function plotQualityProfile function","code":""},{"path":"/reference/plot_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"","code":"plot_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Dada2 wrapper function making quality profiles sample","code":""},{"path":"/reference/prep_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare final ASV abundance matrix — prep_abund_matrix","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"Prepare final ASV abundance matrix","code":""},{"path":"/reference/prep_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"","code":"prep_abund_matrix(cutadapt_data, asv_abund_matrix, data_tables, locus)"},{"path":"/reference/prep_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix locus barcode selected analysis directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/prepare_metadata_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"Read metadata file user combine reformat , given primer data. Included larger function prepare_reads.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"","code":"prepare_metadata_table(metadata_file_path, primer_data)"},{"path":"/reference/prepare_metadata_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"primer_data data frame oriented primer information returned orient_primers function. metadata_path path metadata file.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"dataframe containing merged metadata primer data.","code":""},{"path":"/reference/prepare_reads.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare reads for primer trimming using Cutadapt — prepare_reads","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"Prepare reads primer trimming using Cutadapt","code":""},{"path":"/reference/prepare_reads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"prepare_reads(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\",   overwrite_existing = FALSE )"},{"path":"/reference/prepare_reads.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. multithread Logical, indicating whether use multithreading certain operations. Default FALSE.","code":""},{"path":"/reference/prepare_reads.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"list containing data tables, including metadata, primer sequences search based orientation, paths trimming reads, user-defined parameters subsequent steps.","code":""},{"path":"/reference/prepare_reads.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"# Pre-filter raw reads and parse metadata and primer_information to prepare  # for primer trimming and filter analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpgaR3Jq/demulticoder_run_temp/prefiltered_sequences"},{"path":"/reference/primer_check.html","id":null,"dir":"Reference","previous_headings":"","what":"Matching Order Primer Check — primer_check","title":"Matching Order Primer Check — primer_check","text":"Matching Order Primer Check","code":""},{"path":"/reference/primer_check.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matching Order Primer Check — primer_check","text":"","code":"primer_check(fastq_data)"},{"path":"/reference/primer_check.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matching Order Primer Check — primer_check","text":"fastq_data data frame FASTQ file paths, direction sequences, names sequences","code":""},{"path":"/reference/primer_check.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matching Order Primer Check — primer_check","text":"None","code":""},{"path":"/reference/process_single_barcode.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"Process information ASV abundance matrix run DADA2 single barcode","code":""},{"path":"/reference/process_single_barcode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"","code":"process_single_barcode(   data_tables,   temp_directory_path,   output_directory_path,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   locus = barcode )"},{"path":"/reference/process_single_barcode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"data_tables data tables containing paths read files, metadata, primer sequences temp_directory_path temporary directory path asv_abund_matrix abundance matrix containing amplified sequence variants tryRC Try reverse complement (default FALSE) verbose Print additional information (default FALSE) multithread Use multiple threads (default TRUE) locus locus taxonomy assignment (e.g., rps10, other1, other2)","code":""},{"path":"/reference/read_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"Takes FASTQ files user creates data frame paths files created used future. Included larger 'read_prefilt_fastq' function.","code":""},{"path":"/reference/read_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"","code":"read_fastq(data_directory_path, temp_directory_path)"},{"path":"/reference/read_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data_directory_path path directory containing FASTQ, metadata, primer_info files temp_directory_path User-defined temporary directory place reads throughout workflow.","code":""},{"path":"/reference/read_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data frame FASTQ file paths, primer orientations sequences, parsed sample names","code":""},{"path":"/reference/read_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"Take user's DADA2 parameters make dataframe downstream steps","code":""},{"path":"/reference/read_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"","code":"read_parameters(primers_params_path)"},{"path":"/reference/read_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/read_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"data frame information DADA2 parameters.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"function calling read_fastq, primer_check, remove_ns functions. process edit FASTQ make ready trimming primers Cutadapt. Part larger 'prepare_reads' function.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"","code":"read_prefilt_fastq(   data_directory_path = data_directory_path,   multithread,   temp_directory_path )"},{"path":"/reference/read_prefilt_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"data_directory_path path directory containing FASTQ, metadata.csv, primerinfo_params.csv files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"Returns filtered reads Ns","code":""},{"path":"/reference/remove_ns.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Wrapper function core DADA2 filter trim function first filtering step","code":""},{"path":"/reference/remove_ns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"","code":"remove_ns(fastq_data, multithread, temp_directory_path)"},{"path":"/reference/remove_ns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"fastq_data data frame fastq file paths, direction sequences, names sequences metadata, primer_info files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow metadata metadata containing concatenated metadata primer data","code":""},{"path":"/reference/remove_ns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Return prefiltered reads Ns","code":""},{"path":"/reference/run_cutadapt.html","id":null,"dir":"Reference","previous_headings":"","what":"Core function for running cutadapt — run_cutadapt","title":"Core function for running cutadapt — run_cutadapt","text":"Core function running cutadapt","code":""},{"path":"/reference/run_cutadapt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core function for running cutadapt — run_cutadapt","text":"","code":"run_cutadapt(   cutadapt_path,   cutadapt_data_barcode,   barcode_params,   minCutadaptlength )"},{"path":"/reference/run_cutadapt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core function for running cutadapt — run_cutadapt","text":"cutadapt_path path cutadapt program. minCutadaptlength Read lengths lower threshold discarded. Default 50. cutadapt_data Directory_data folder trimmed filtered reads sample.","code":""},{"path":"/reference/run_cutadapt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core function for running cutadapt — run_cutadapt","text":"Trimmed read.","code":""},{"path":"/reference/setup_directories.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up directory paths for subsequent analyses — setup_directories","title":"Set up directory paths for subsequent analyses — setup_directories","text":"function sets directory paths subsequent analyses. checks whether specified output directories exist creates . function also provides paths primer metadata files within data directory.","code":""},{"path":"/reference/setup_directories.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up directory paths for subsequent analyses — setup_directories","text":"","code":"setup_directories(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\" )"},{"path":"/reference/setup_directories.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up directory paths for subsequent analyses — setup_directories","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run.","code":""},{"path":"/reference/setup_directories.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up directory paths for subsequent analyses — setup_directories","text":"list paths data, output, temporary directories, primer, metadata files.","code":""}]
