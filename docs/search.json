[{"path":"/articles/01_getting_started.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before You Start","title":"Getting Started","text":"following example, demonstrate key package functionality using subset reads two samples containing pooled ITS1 fungal rps10 oomycete amplicons. databases used assign taxonomy step also abridged versions full UNITE oomyceteDB databases. can follow along test data associated CSV input files loaded package. Additional examples also available website. Please note, speed, test dataset comprised randomly subset reads samples (S1 S2), due database size, full UNITE database included package, also smaller subset larger database. need prepare raw read files fill metadata.csv primerinfo_params.csv templates.","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-the-input-read-files","dir":"Articles","previous_headings":"","what":"Format of the input read files","title":"Getting Started","text":"package takes foward reverse Illumina short read sequence data. avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz. Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz error R1 R2 directly preceding ‘.fastq.gz’ suffix.","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"","what":"Format of metadata file (metadata.csv)","title":"Getting Started","text":"format CSV file simple. template . two necessary columns (names) : sample_name column primer_info column additional metadata pasted two columns. can referenced later analysis steps save step loading metadata later. S1 S2 come rhododendron rhizobiome dataset random subset reads. notice S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled reads (rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. Example using test dataset:","code":""},{"path":"/articles/01_getting_started.html","id":"format-of-primer-and-parameters-file-primerinfo_parms-csv","dir":"Articles","previous_headings":"","what":"Format of primer and parameters file (primerinfo_parms.csv)","title":"Getting Started","text":"DADA2 Primer sequence information user-defined parameters placed primerinfo_params.csv. simplify functions called, user provide parameters within input file. recommend using template linked required columns user must fill : 1.primer_name (rps10, , another barcode) 2.forward-forward sequence 3.reverse-reverse sequence user doesn’t add info subsequent columns, series default parameters used. Example template ‘primerinfo_params.csv’ TODO-table parameters, functions associated , default parameters, whether Cutadapt, DADA2, package specific parameters.","code":""},{"path":"/articles/01_getting_started.html","id":"reference-database-format","dir":"Articles","previous_headings":"","what":"Reference database format","title":"Getting Started","text":"now, package compatible following databases: oomycetedb : http://www.oomycetedb.org/ SILVA 16S database species assignments: https://zenodo.org/records/4587955/files/silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1 UNITE fungal database https://unite.ut.ee/repository.php user can select one database (now), first need reformat headers exactly like UNITE fungal database specifications. Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function","code":""},{"path":"/articles/01_getting_started.html","id":"note-on-computer-specifications","dir":"Articles","previous_headings":"","what":"Note on computer specifications","title":"Getting Started","text":"Computer specifications may limiting factor– using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases package randomly subsetted demonstration purposes. Users need upload databases input data folder. computer crashes taxonomic assignment step, please switch computing cluster. Please also ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/01_getting_started.html","id":"loading-the-package","dir":"Articles","previous_headings":"","what":"Loading the Package","title":"Getting Started","text":"now, package loaded retrieving GitHub. Eventually, package uploaded CRAN Bioconductor.","code":"#devtools::install_github(\"grunwaldlab/demulticoder\")  library(demulticoder)"},{"path":"/articles/01_getting_started.html","id":"reorganize-data-tables-and-set-up-data-directory-structure","dir":"Articles","previous_headings":"","what":"Reorganize data tables and set-up data directory structure","title":"Getting Started","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    overwrite_existing=TRUE)"},{"path":"/articles/01_getting_started.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"","what":"Remove primers with Cutadapt","title":"Getting Started","text":"running Cutadapt, please ensure installed .","code":"demulticoder::cut_trim(   analysis_setup,   cutadapt_path=\"/opt/homebrew/bin/cutadapt\",   overwrite_existing = FALSE) #> Warning in demulticoder::cut_trim(analysis_setup, cutadapt_path = #> \"/opt/homebrew/bin/cutadapt\", : Existing analysis files not found. The #> 'cut_trim' function was rerun. #> Running Cutadapt 4.1 for its sequence data  #> Running Cutadapt 4.1 for rps10 sequence data"},{"path":"/articles/01_getting_started.html","id":"asv-inference-step","dir":"Articles","previous_headings":"","what":"ASV inference step","title":"Getting Started","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = FALSE) #> Warning in make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE): #> No existing files found. The 'make_asv_abund_matrix' function will run. #> 1658423 total bases in 6294 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 3285 reads in 1348 unique sequences. #> Sample 2 - 3009 reads in 1363 unique sequences. #> 1689445 total bases in 6294 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Reverse read of primer pair its #> Sample 1 - 3285 reads in 2157 unique sequences. #> Sample 2 - 3009 reads in 1915 unique sequences. #> 1918751 total bases in 6828 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 3245 reads in 1996 unique sequences. #> Sample 2 - 3583 reads in 2250 unique sequences. #> 1911983 total bases in 6828 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 3245 reads in 2254 unique sequences. #> Sample 2 - 3583 reads in 2951 unique sequences. #> $its #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpQna2sh/demulticoder_run/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpQna2sh/demulticoder_run/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/01_getting_started.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"","what":"Taxonomic assignment step","title":"Getting Started","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing=FALSE) #> Warning in assign_tax(analysis_setup, asv_abund_matrix, retrieve_files = TRUE, #> : No existing files found. The analysis will be run. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  5678     3285      3202      3205   2976    2976 #> 2             S2_its  4827     3009      2911      2864   2742    2742 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  4119     3245      3243      3223   3215    3215 #> 2           S2_rps10  4985     3583      3579      3580   3186    3186"},{"path":"/articles/01_getting_started.html","id":"reformat-asv-matrix-or-matrices","dir":"Articles","previous_headings":"","what":"Reformat ASV matrix or matrices","title":"Getting Started","text":"","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, save_outputs=TRUE) #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/01_getting_started.html","id":"working-with-output-files-and-objects","dir":"Articles","previous_headings":"","what":"Working with output files and objects","title":"Getting Started","text":"TODO-add brief summary work output matrices make heattrees using taxmap objects First make heat tree -barcoded samples  Now make heat tree rps10-barcoded samples  can also variety analyses, convert phyloseq object demonstrate make stacked bar plot relative abundance taxa sample -barcoded samples  Finally,demonstrate make stacked bar plot relative abundance taxa sample rps10-barcoded samples","code":"metacoder::heat_tree(objs$taxmap_its,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\") metacoder::heat_tree(objs$taxmap_rps10,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\") data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated.  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/articles/package_workflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Package Workflow","text":"demulticoder package Cutadapt DADA2 wrapper package metabarcoding analyses. package designed users varying experience metabarcoding analysis. Outputs intuitive comprehensive account iterative nature metabarcodeing analyses.","code":""},{"path":"/articles/package_workflow.html","id":"key-features","dir":"Articles","previous_headings":"Introduction","what":"Key features","title":"Package Workflow","text":"ability analysis either demultiplexed pooled amplicons within sample Multiple datasets can trimmed primers, filtered, denoised, merged, assigned taxonomy one go package handles just 16S datasets using default UNITE fungal Silva 16S databases also oomycete rps10 analyses using oomycetedb (oomycetedb.org), two custom databases (provided formatted described : https://benjjneb.github.io/dada2/training.html). package developed Martha Sudermann, Zachary Foster, Niklaus Grunwald, Jeff Chang.","code":""},{"path":"/articles/package_workflow.html","id":"before-you-start","dir":"Articles","previous_headings":"Introduction","what":"Before You Start","title":"Package Workflow","text":"following example, demonstrate key package functionality using subset reads two samples containing pooled ITS1 fungal rps10 oomycete amplicons. can follow along test data associated CSV input files loaded package. Additional examples also available website. Please note, speed, test dataset comprised randomly subset reads samples (S1 S2), due database size, full UNITE database included package, also smaller subset larger database. need prepare raw read files fill metadata.csv primerinfo_params.csv templates.","code":""},{"path":"/articles/package_workflow.html","id":"format-of-the-raw-read-files","dir":"Articles","previous_headings":"Introduction","what":"Format of the Raw Read Files","title":"Package Workflow","text":"package takes foward reverse Illumina short read sequence data. avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. final characters .fastq.gz suffix MUST **_R1** **_R2**. Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible renamed : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz parsing functions error R1 R2 directly preceeding ‘.fastq.gz’ suffix.","code":""},{"path":"/articles/package_workflow.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"Introduction","what":"Format of metadata file (metadata.csv)","title":"Package Workflow","text":"format CSV file simple. two necessary columns (names) : sample_name column primer_info column additional metadata pasted two columns. can referenced later analysis steps save step loading metadata later. S1 S2 come rhododendron rhizobiome dataset random subset reads. notice S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled reads (rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. Example using test dataset:","code":""},{"path":"/articles/package_workflow.html","id":"primer-sequence-information-and-user-defined-parameters-are-placed-in-primerinfo_params-csv","dir":"Articles","previous_headings":"Introduction","what":"Primer sequence information and user-defined parameters are placed in primerinfo_params.csv","title":"Package Workflow","text":"simplify functions called, user provide parameters within input file. recommend using template provided documentation. required columns user must fill : 1.primer_name (rps10, , another barcode) 2.forward-forward sequence 3.reverse-reverse sequence 4.already_trimmed (TRUE/FALSE) (datasets require primers first removed reads. However, 16S datasets, protocols like Earth Microbiome Project followed, primers mostly removed demultiplexing barcoded samples following Illumina run). may primers still remain. already_trimmed flag specified, remaining primers removed reads primers located copied trimmed folder subsequent analyses. user doesn’t add info subsequent columns, default DADA2 parameters used. TODO-provide info different parameter descriptions functions associated . Example template ‘primerinfo_params.csv’","code":""},{"path":"/articles/package_workflow.html","id":"reference-database-format","dir":"Articles","previous_headings":"Introduction","what":"Reference Database Format","title":"Package Workflow","text":"now, package compatible following databases: oomycetedb : http://www.oomycetedb.org/ SILVA 16S database species assignments: https://zenodo.org/records/4587955/files/silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1 UNITE fungal database https://unite.ut.ee/repository.php user can select one database (now), first need reformat headers exactly like UNITE fungal database specifications. Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function","code":""},{"path":"/articles/package_workflow.html","id":"additional-notes","dir":"Articles","previous_headings":"Introduction","what":"Additional Notes","title":"Package Workflow","text":"Computer specifications may limiting factor– using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases package randomly subsetted demonstration purposes. Users need upload databases input data folder. computer crashes taxonomic assignment step, please switch computing cluster. Please also ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/package_workflow.html","id":"loading-the-package","dir":"Articles","previous_headings":"Introduction","what":"Loading the Package","title":"Package Workflow","text":"now, package loaded retrieving GitHub. Eventually, package uploaded CRAN Bioconductor.","code":"#devtools::install_github(\"grunwaldlab/demulticoder\")  library(demulticoder)"},{"path":"/articles/package_workflow.html","id":"reorganize-data-tables","dir":"Articles","previous_headings":"Introduction","what":"Reorganize Data Tables","title":"Package Workflow","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    tempdir_id = \"test_dataset\",   overwrite_existing=TRUE)"},{"path":"/articles/package_workflow.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"Introduction","what":"Remove Primers with Cutadapt","title":"Package Workflow","text":"running Cutadapt, please ensure installed .","code":"demulticoder::cut_trim(   analysis_setup,   cutadapt_path=\"/opt/homebrew/bin/cutadapt\",   overwrite_existing = TRUE) #> Running Cutadapt 4.1 for its sequence data  #> Running Cutadapt 4.1 for rps10 sequence data  #> Running Cutadapt 4.1 for sixteenS sequence data  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_1_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_1_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_2_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_2_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_3_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/1_3_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_1_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_1_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_2_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_2_R2_sixteenS.fastq.gz  #> Already trimmed forward reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_3_R1_sixteenS.fastq.gz  #> Already trimmed reverse reads were appended to trimmed read directory, and they are located here: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/trimmed_sequences/2_3_R2_sixteenS.fastq.gz"},{"path":"/articles/package_workflow.html","id":"asv-inference-step","dir":"Articles","previous_headings":"Introduction","what":"ASV inference step","title":"Package Workflow","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = TRUE) #> 48620525 total bases in 193819 reads from 6 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ...... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #>    selfConsist step 7 #>    selfConsist step 8 #>    selfConsist step 9 #> Convergence after  9  rounds. #> Error rate plot for the Forward read of primer pair sixteenS #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 31044 reads in 16523 unique sequences. #> Sample 2 - 33424 reads in 17777 unique sequences. #> Sample 3 - 31303 reads in 17153 unique sequences. #> Sample 4 - 32148 reads in 15953 unique sequences. #> Sample 5 - 33116 reads in 17060 unique sequences. #> Sample 6 - 32784 reads in 17064 unique sequences. #> 48621032 total bases in 193819 reads from 6 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 ...... #>    selfConsist step 2 #>    selfConsist step 3 #>    selfConsist step 4 #>    selfConsist step 5 #>    selfConsist step 6 #> Convergence after  6  rounds. #> Error rate plot for the Reverse read of primer pair sixteenS #> Sample 1 - 31044 reads in 20471 unique sequences. #> Sample 2 - 33424 reads in 22029 unique sequences. #> Sample 3 - 31303 reads in 21177 unique sequences. #> Sample 4 - 32148 reads in 20174 unique sequences. #> Sample 5 - 33116 reads in 21307 unique sequences. #> Sample 6 - 32784 reads in 21009 unique sequences. #> 713433 total bases in 2703 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1486 reads in 659 unique sequences. #> Sample 2 - 1217 reads in 611 unique sequences. #> 726880 total bases in 2703 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1486 reads in 1026 unique sequences. #> Sample 2 - 1217 reads in 815 unique sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821853 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> $sixteenS #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/asvabund_matrixDADA2_sixteenS.RData\" #>  #> $its #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpR3lMBC/test_dataset/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/package_workflow.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"Introduction","what":"Taxonomic assignment step","title":"Package Workflow","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing=TRUE) #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1       1_1_sixteenS 40437    31044     28831     28737  24286   23913 #> 2       1_2_sixteenS 44735    33424     31246     30900  25900   25448 #> 3       1_3_sixteenS 40114    31303     28922     29133  24504   24172 #> 4       2_1_sixteenS 39984    32148     29659     29631  25377   24748 #> 5       2_2_sixteenS 43128    33116     30536     30410  25790   25130 #> 6       2_3_sixteenS 43158    32784     30300     30131  25743   25121 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1486      1427      1433   1316    1316 #> 2             S2_its  1996     1217      1145      1124   1065    1065 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/package_workflow.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"Introduction","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Package Workflow","text":"","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, save_outputs=TRUE, overwrite=TRUE) #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> For sixteenS dataset  #> Taxmap object saved in: ~/output_test_dataset/obj_dada_sixteenS.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_sixteenS.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to NA  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":[]},{"path":"/articles/package_workflow.html","id":"now-we-demonstrate-how-to-make-a-heattree-using-taxmap-object--first-we-make-a-heat-tree-for-our-its-barcoded-samples-1","dir":"Articles","previous_headings":"Introduction","what":"Now we demonstrate how to make a heattree using taxmap object. First we make a heat tree for our ITS-barcoded samples","title":"Package Workflow","text":"","code":"metacoder::heat_tree(objs$taxmap_its,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\")"},{"path":"/articles/package_workflow.html","id":"now-we-demonstrate-how-to-make-a-heattree-using-taxmap-object--first-we-make-a-heat-tree-for-our-its-barcoded-sample","dir":"Articles","previous_headings":"Introduction","what":"Now we demonstrate how to make a heattree using taxmap object. First we make a heat tree for our ITS-barcoded sample","title":"Package Workflow","text":"","code":"metacoder::heat_tree(objs$taxmap_rps10,           node_label = taxon_names,           node_size = n_obs,           node_color = n_obs,           node_color_axis_label = \"ASV count\",           node_size_axis_label = \"Total Abundance of Taxa\",           layout = \"da\", initial_layout = \"re\")"},{"path":"/articles/package_workflow.html","id":"we-can-also-do-a-variety-of-analyses-if-we-convert-to-phyloseq-object","dir":"Articles","previous_headings":"Introduction","what":"We can also do a variety of analyses, if we convert to phyloseq object","title":"Package Workflow","text":"","code":"data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated.  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martha . Sudermann. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sudermann M (2024). demulticoder: R Package Integrated Analysis Multiplex Metabarcodes. R package version 0.0.0.9000, https://grunwaldlab.github.io/demulticoder/.","code":"@Manual{,   title = {demulticoder: An R Package for the Integrated Analysis of Multiplex Metabarcodes},   author = {Martha A. Sudermann},   year = {2024},   note = {R package version 0.0.0.9000},   url = {https://grunwaldlab.github.io/demulticoder/}, }"},{"path":"/index.html","id":"demulticoder-r-package","dir":"","previous_headings":"","what":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package actively development. message removed, use caution. Additional testing, documentation, examples progress.","code":""},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"demulticoder package Cutadapt DADA2 wrapper package metabarcodng analyses. main commands outputs intuitive comprehensive, helps account complex iterative nature metabarcoding analyses. brief schematic general workflow:","code":""},{"path":"/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key features","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"ability analysis either demultiplexed pooled amplicons within samples Amplicons multiple datasets trimmed primers, filtered, denoised, merged, given taxonomic assignments one go (different parameters dataset desired) package handles just 16S datasets using default UNITE fungal Silva 16S databases also oomycete rps10 analyses using oomycetedb (https://oomycetedb.org), two custom databases (provided formatted described : https://benjjneb.github.io/dada2/training.html).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"install development version package:","code":"devtools::install_github(\"grunwaldlab/demulticoder\")"},{"path":"/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick start","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"1. Set-input directory files installing package, make data directory add following files: - PE short read amplicon data. files must end either *_R1.fastq.gz* , *_R2.fastq.gz* sample must R1 R2 files. metadata.csv file (unique row sample, samples entered twice contain pooled amplicions, example template) primerinfo_params.csv file (new row unique barcode associated primer sequences, also optional Cutadapt, DADA2 filtering parameters can added adjusted) 2. Prepare reads 3. Cut trim reads 4. Make ASV abundance matrix 5. Assign taxonomy 6. Convert ASV matrix taxmap phyloseq objects","code":"output<-prepare_reads(   data_directory = \"<DATADIR>\",   output_directory = \"<OUTDIR>\") cut_trim(   output,   cutadapt_path=\"<CUTADAPTPATH>\") make_asv_abund_matrix(   output) assign_tax(   output,   asv_abund_matrix) objs<-convert_asv_matrix_to_objs(output)"},{"path":"/index.html","id":"check-out-the-website-to-view-the-documentation-and-see-more-examples","dir":"","previous_headings":"","what":"Check out the website to view the documentation and see more examples","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"information, key functions, inputs, example vignettes, check documentation : https://grunwaldlab.github.io/demulticoder","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package developed Martha Sudermann, Zachary Foster, Samantha Dawson, Hung Phan, Niklaus Grnwald, Jeff Chang. Stay tuned associated manuscript.","code":""},{"path":"/reference/add_pid_to_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Add PID and bootstrap values to tax result. — add_pid_to_tax","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"Add PID bootstrap values tax result.","code":""},{"path":"/reference/add_pid_to_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"","code":"add_pid_to_tax(tax_results, asv_pid)"},{"path":"/reference/add_pid_to_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"tax_results dataframe containing taxonomic assignments asv_pid Percent identity information ASV relative reference database sequence","code":""},{"path":"/reference/assignTax_as_char.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"Combine taxonomic assignments bootstrap values locus single falsification vector","code":""},{"path":"/reference/assignTax_as_char.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"","code":"assignTax_as_char(tax_results, temp_directory_path, locus)"},{"path":"/reference/assignTax_as_char.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"tax_results dataframe containing taxonomic assignments","code":""},{"path":"/reference/assign_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy functions — assign_tax","title":"Assign taxonomy functions — assign_tax","text":"Assign taxonomy functions","code":""},{"path":"/reference/assign_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy functions — assign_tax","text":"","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   retrieve_files = FALSE,   overwrite_existing = FALSE,   db_rps10 = \"oomycetedb.fasta\",   db_its = \"fungidb.fasta\",   db_16s = \"bacteriadb.fasta\",   db_other1 = \"otherdb1.fasta\",   db_other2 = \"otherdb2.fasta\" )"},{"path":"/reference/assign_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy functions — assign_tax","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function asv_abund_matrix ASV abundance matrix. tryRC Whether try reverse complementing sequences taxonomic assignment verbose Logical, indicating whether display verbose output multithread Logical, indicating whether use multithreading retrieve_files Specify TRUE/FALSE whether copy files temp directory output directory overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. db_rps10 reference database rps10 locus db_its reference database locus db_16s reference database 16S locus db_other1 reference database different locus 1 (assumes format like SILVA DB entries) db_other2 reference database different locus 2 (assumes format like SILVA DB entries)","code":""},{"path":"/reference/assign_tax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign taxonomy functions — assign_tax","text":"Taxonomic assignments unique ASV sequence","code":""},{"path":"/reference/assign_tax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign taxonomy functions — assign_tax","text":"","code":"# Assign taxonomies to ASVs on a per barcode basis analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Warning: Existing analysis setup tables are not found. The 'prepare_reads' function was rerun #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpppLldL/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Warning: Existing analysis files not found. The 'cut_trim' function was rerun. #> Running Cutadapt 4.1 for its sequence data  #> Running Cutadapt 4.1 for rps10 sequence data   make_asv_abund_matrix( analysis_setup,  overwrite_existing = FALSE ) #> Warning: No existing files found. The 'make_asv_abund_matrix' function will run. #> Error in infer_asvs(direction, primer_name, barcode_params, output_directory_path): object 'analysis_setup' not found assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing=FALSE ) #> Warning: No existing files found. The analysis will be run. #> Warning: cannot open compressed file '/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpppLldL/demulticoder_run_temp/asvabund_matrixDADA2_its.RData', probable reason 'No such file or directory' #> Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection"},{"path":"/reference/assign_taxonomyDada2.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy — assign_taxonomyDada2","title":"Assign taxonomy — assign_taxonomyDada2","text":"Assign taxonomy","code":""},{"path":"/reference/assign_taxonomyDada2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy — assign_taxonomyDada2","text":"","code":"assign_taxonomyDada2(   asv_abund_matrix,   temp_directory_path,   minBoot = 0,   tryRC = FALSE,   verbose = FALSE,   multithread = TRUE,   locus = barcode )"},{"path":"/reference/assign_taxonomyDada2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy — assign_taxonomyDada2","text":"asv_abund_matrix ASV abundance matrix minBoot (Optional). Default 50.  minimum bootstrap confidence assigning taxonomic level. tryRC (Optional). Default FALSE.  TRUE, reverse-complement sequences used classification better match reference sequences forward sequence. verbose (Optional). Default FALSE. TRUE, print status standard output. multithread (Optional). Default FALSE. TRUE, multithreading enabled number available threads automatically determined.    integer provided, number threads use set passing argument setThreadOptions. ref_database reference database used taxonomic inference steps","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"Filter ASV abundance matrix convert taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"","code":"convert_asv_matrix_to_objs(   analysis_setup,   min_read_depth = 0,   minimum_bootstrap = 0,   save_outputs = FALSE,   overwrite_existing = FALSE )"},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function min_read_depth ASV filter parameter. mean read depth across samples less threshold, ASV filtered. reads less value across samples minimum_bootstrap Threshold bootstrap support value taxonomic assignments. designated minimum bootstrap threshold, taxnomoic assignments set N/save_outputs Logical, indicating whether save taxmap object. Default FALSE. overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"ASV matrix converted taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter ASV abundance matrix and convert to taxmap object — convert_asv_matrix_to_objs","text":"","code":"# Convert final matrix to taxmap and phyloseq objects for downstream analysis steps analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE. make_asv_abund_matrix( analysis_setup,  overwrite_existing = FALSE ) #> Warning: No existing files found. The 'make_asv_abund_matrix' function will run. #> Error in infer_asvs(direction, primer_name, barcode_params, output_directory_path): object 'analysis_setup' not found assign_tax( analysis_setup, asv_abund_matrix, retrieve_files=FALSE,  overwrite_existing=FALSE ) #> Warning: No existing files found. The analysis will be run. #> Warning: cannot open compressed file '/var/folders/59/9jp4sjjd00n2wp4kgqtvq7dh0000gn/T//RtmpppLldL/demulticoder_run_temp/asvabund_matrixDADA2_its.RData', probable reason 'No such file or directory' #> Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection objs<-convert_asv_matrix_to_objs( analysis_setup,  save_outputs=FALSE )"},{"path":"/reference/countOverlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Count overlap to see how well the reads were merged — countOverlap","title":"Count overlap to see how well the reads were merged — countOverlap","text":"Count overlap see well reads merged","code":""},{"path":"/reference/countOverlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count overlap to see how well the reads were merged — countOverlap","text":"","code":"countOverlap(data_tables, merged_reads, barcode, output_directory_path)"},{"path":"/reference/countOverlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count overlap to see how well the reads were merged — countOverlap","text":"data_tables data tables containing paths read files, metadata, primer sequences merged_reads Intermediate merged read R data file barcode barcode used analysis output_directory_path path directory resulting files output","code":""},{"path":"/reference/countOverlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count overlap to see how well the reads were merged — countOverlap","text":"plot describing well reads merged information overlap reads","code":""},{"path":"/reference/createASVSequenceTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Make ASV sequence matrix — createASVSequenceTable","title":"Make ASV sequence matrix — createASVSequenceTable","text":"Make ASV sequence matrix","code":""},{"path":"/reference/createASVSequenceTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make ASV sequence matrix — createASVSequenceTable","text":"","code":"createASVSequenceTable(merged_reads, orderBy = \"abundance\")"},{"path":"/reference/createASVSequenceTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make ASV sequence matrix — createASVSequenceTable","text":"merged_reads Intermediate merged read R data file orderBy (Optional). character(1). Default \"abundance\". Specifies sequences (columns) returned table ordered (decreasing). Valid values: \"abundance\", \"nsamples\", NULL.","code":""},{"path":"/reference/createASVSequenceTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make ASV sequence matrix — createASVSequenceTable","text":"raw_seqtab","code":""},{"path":"/reference/cut_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"Main command trim primers using Cutadapt core DADA2 functions. samples contain pooled barcodes, reads also demultiplexed","code":""},{"path":"/reference/cut_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"","code":"cut_trim(analysis_setup, cutadapt_path, overwrite_existing = FALSE)"},{"path":"/reference/cut_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function cutadapt_path Path Cutadapt program. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/cut_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"Trimmed reads, primer counts, quality plots, ASV matrix.","code":""},{"path":"/reference/cut_trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main command to trim primers using Cutadapt and core DADA2 functions. If samples contain pooled barcodes, reads will also be demultiplexed — cut_trim","text":"","code":"# Remove remaining primers from raw reads, demultiplex pooled barcoded samples,  # and then trim reads based on specific DADA2 parameters analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE."},{"path":"/reference/filter_and_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Wrapper function filterAndTrim function DADA2, used primer trimming","code":""},{"path":"/reference/filter_and_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"","code":"filter_and_trim(   output_directory_path,   temp_directory_path,   cutadapt_data_barcode,   barcode_params,   barcode )"},{"path":"/reference/filter_and_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"output_directory_path path directory resulting files output cutadapt_data_barcode directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/filter_and_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Filtered trimmed reads","code":""},{"path":"/reference/format_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Format ASV abundance matrix — format_abund_matrix","title":"Format ASV abundance matrix — format_abund_matrix","text":"Format ASV abundance matrix","code":""},{"path":"/reference/format_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format ASV abundance matrix — format_abund_matrix","text":"","code":"format_abund_matrix(   data_tables,   asv_abund_matrix,   seq_tax_asv,   output_directory_path,   locus )"},{"path":"/reference/format_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format ASV abundance matrix — format_abund_matrix","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants seq_tax_asv amplified sequence variants matrix taxonomic information","code":""},{"path":"/reference/format_database.html","id":null,"dir":"Reference","previous_headings":"","what":"General functions to format user-specified databases — format_database","title":"General functions to format user-specified databases — format_database","text":"General functions format user-specified databases","code":""},{"path":"/reference/format_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General functions to format user-specified databases — format_database","text":"","code":"format_database(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   barcode,   db_its,   db_rps10,   db_16s,   db_other1,   db_other2 )"},{"path":"/reference/format_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General functions to format user-specified databases — format_database","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files barcode barcode database formatted","code":""},{"path":"/reference/format_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General functions to format user-specified databases — format_database","text":"formatted database based specified barcode type","code":""},{"path":"/reference/format_db_16s.html","id":null,"dir":"Reference","previous_headings":"","what":"An 16s database that has modified headers and is output in the reference_databases folder — format_db_16s","title":"An 16s database that has modified headers and is output in the reference_databases folder — format_db_16s","text":"16s database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_16s.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An 16s database that has modified headers and is output in the reference_databases folder — format_db_16s","text":"","code":"format_db_16s(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_16s )"},{"path":"/reference/format_db_16s.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An 16s database that has modified headers and is output in the reference_databases folder — format_db_16s","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_16s name database","code":""},{"path":"/reference/format_db_16s.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An 16s database that has modified headers and is output in the reference_databases folder — format_db_16s","text":"16s database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":null,"dir":"Reference","previous_headings":"","what":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"","code":"format_db_its(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_its )"},{"path":"/reference/format_db_its.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_its name database","code":""},{"path":"/reference/format_db_its.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other1.html","id":null,"dir":"Reference","previous_headings":"","what":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":", user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"","code":"format_db_other1(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other1 )"},{"path":"/reference/format_db_other1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other1 name database","code":""},{"path":"/reference/format_db_other1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other2.html","id":null,"dir":"Reference","previous_headings":"","what":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"second user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"","code":"format_db_other2(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other2 )"},{"path":"/reference/format_db_other2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other2 name database","code":""},{"path":"/reference/format_db_other2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_rps10.html","id":null,"dir":"Reference","previous_headings":"","what":"Create modified reference rps10 database for downstream analysis — format_db_rps10","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"Create modified reference rps10 database downstream analysis","code":""},{"path":"/reference/format_db_rps10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"","code":"format_db_rps10(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_rps10 )"},{"path":"/reference/format_db_rps10.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_rps10 name database","code":""},{"path":"/reference/format_db_rps10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"rps10 database modified headers output reference_databases folder.","code":""},{"path":"/reference/get_fastq_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"Retrieve paths filtered trimmed Fastq files","code":""},{"path":"/reference/get_fastq_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"","code":"get_fastq_paths(data_tables, my_direction, my_primer_pair_id)"},{"path":"/reference/get_fastq_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Whether primer forward reverse direction my_primer_pair_id specific barcode id cutadapt_data directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/get_pids.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"Align ASV sequences reference sequences database get percent ID. Get percent identities.","code":""},{"path":"/reference/get_pids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"","code":"get_pids(tax_results, temp_directory_path, output_directory_path, db, locus)"},{"path":"/reference/get_pids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"tax_results data frame containing taxonomic assignments","code":""},{"path":"/reference/get_post_trim_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_post_trim_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"","code":"get_post_trim_hits(primer_data, cutadapt_data, output_directory_path)"},{"path":"/reference/get_post_trim_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"primer_data primer data frame created orient_primers function cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_post_trim_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Table read counts across sample","code":""},{"path":"/reference/get_pre_primer_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"","code":"get_pre_primer_hits(primer_data, fastq_data, output_directory_path)"},{"path":"/reference/get_pre_primer_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"primer_data primer data data frame created orient_primers function fastq_data data frame FASTQ file paths, direction sequences, names sequences output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"number reads primer found","code":""},{"path":"/reference/get_read_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"Final inventory read counts step input removal chimeras. function deals one sample. TODO optimize one sample","code":""},{"path":"/reference/get_read_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"","code":"get_read_counts(   asv_abund_matrix,   temp_directory_path,   output_directory_path,   locus )"},{"path":"/reference/get_read_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"asv_abund_matrix abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/get_ref_seq.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"Align ASV sequences reference sequences database get percent ID. Start retrieving reference sequences.","code":""},{"path":"/reference/get_ref_seq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"","code":"get_ref_seq(tax_results, db)"},{"path":"/reference/get_ref_seq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"tax_results dataframe containing taxonomic assignments db reference database","code":""},{"path":"/reference/infer_asv_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to infer ASVs, for multiple loci — infer_asv_command","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"Function infer ASVs, multiple loci","code":""},{"path":"/reference/infer_asv_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"","code":"infer_asv_command(   output_directory_path,   temp_directory_path,   data_tables,   barcode_params,   barcode )"},{"path":"/reference/infer_asv_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"output_directory_path path directory resulting files output data_tables data tables containing paths read files, metadata, primer sequences denoised_data_path Path saved intermediate denoised data","code":""},{"path":"/reference/infer_asvs.html","id":null,"dir":"Reference","previous_headings":"","what":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"Core DADA2 function learn errors infer ASVs","code":""},{"path":"/reference/infer_asvs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"","code":"infer_asvs(   data_tables,   my_direction,   my_primer_pair_id,   barcode_params,   output_directory_path )"},{"path":"/reference/infer_asvs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Location read files metadata file my_primer_pair_id specific barcode id output_directory_path path directory containing fastq, metadata, primerinfo_params files","code":""},{"path":"/reference/infer_asvs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"asv_data","code":""},{"path":"/reference/make_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"Quality filtering remove chimeras short sequences","code":""},{"path":"/reference/make_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"","code":"make_abund_matrix(   raw_seqtab,   temp_directory_path,   barcode_params = barcode_params,   barcode )"},{"path":"/reference/make_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"raw_seqtab R data file raw sequence data prior removal chimeras","code":""},{"path":"/reference/make_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"Make amplified sequence variant (ASV) abundance matrix function generates ASV abundance matrix using raw reads processed previous steps, including read preparation, removing primers, using DADA2 core denoising alogrithm infer ASVs.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"","code":"make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE)"},{"path":"/reference/make_asv_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"ASV abundance matrix (asv_abund_matrix)","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"function processes data unique barcode separately, inferring ASVs, merging reads, creating ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make an amplified sequence variant (ASV) abundance matrix This function generates an ASV abundance matrix using raw reads processed during previous steps, including read preparation, removing primers, and using DADA2 core denoising alogrithm to infer ASVs. — make_asv_abund_matrix","text":"","code":"# The primary wrapper function for DADA2 ASV inference steps analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.  cut_trim( analysis_setup, cutadapt_path=\"/opt/homebrew/bin/cutadapt\",  overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE. make_asv_abund_matrix( analysis_setup,  overwrite_existing = FALSE ) #> Warning: No existing files found. The 'make_asv_abund_matrix' function will run. #> Error in infer_asvs(direction, primer_name, barcode_params, output_directory_path): object 'analysis_setup' not found"},{"path":"/reference/make_cutadapt_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Prepare primmer trimming Cutaapt. Make new sub-directories specify paths trimmed untrimmed reads","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"","code":"make_cutadapt_tibble(fastq_data, metadata, temp_directory_path)"},{"path":"/reference/make_cutadapt_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"fastq_data path FASTQ files analysis metadata, primer_info files metadata Loaded metadata pairing user's metadata file primer data temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Returns larger data frame containing paths temporary read directories, used input running Cutadapt","code":""},{"path":"/reference/make_seqhist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"Plots histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/make_seqhist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"","code":"make_seqhist(asv_abund_matrix, output_directory_path)"},{"path":"/reference/make_seqhist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_seqhist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/merge_reads_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge forward and reverse reads — merge_reads_command","title":"Merge forward and reverse reads — merge_reads_command","text":"Merge forward reverse reads","code":""},{"path":"/reference/merge_reads_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge forward and reverse reads — merge_reads_command","text":"","code":"merge_reads_command(   output_directory_path,   temp_directory_path,   barcode_params,   barcode )"},{"path":"/reference/merge_reads_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge forward and reverse reads — merge_reads_command","text":"output_directory_path path directory resulting files output merged_read_data_path Path R data file containing merged read data","code":""},{"path":"/reference/merge_reads_command.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge forward and reverse reads — merge_reads_command","text":"merged_reads Intermediate merged read R data file","code":""},{"path":"/reference/orient_primers.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"Take user's forward reverse sequences creates complement, reverse, reverse complement primers one data frame","code":""},{"path":"/reference/orient_primers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"","code":"orient_primers(primers_params_path)"},{"path":"/reference/orient_primers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/orient_primers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"data frame oriented primer information.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Wrapper script plotQualityProfile trim steps primer removal.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"","code":"plot_post_trim_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_post_trim_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Quality profiles reads primer trimming","code":""},{"path":"/reference/plot_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for plotQualityProfile function — plot_qc","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Wrapper function plotQualityProfile function","code":""},{"path":"/reference/plot_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"","code":"plot_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Dada2 wrapper function making quality profiles sample","code":""},{"path":"/reference/prep_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare final ASV abundance matrix — prep_abund_matrix","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"Prepare final ASV abundance matrix","code":""},{"path":"/reference/prep_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"","code":"prep_abund_matrix(cutadapt_data, asv_abund_matrix, data_tables, locus)"},{"path":"/reference/prep_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix locus barcode selected analysis directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/prepare_metadata_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"Read metadata file user combine reformat , given primer data. Included larger function prepare_reads.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"","code":"prepare_metadata_table(metadata_file_path, primer_data)"},{"path":"/reference/prepare_metadata_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"primer_data data frame oriented primer information returned orient_primers function. metadata_path path metadata file.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"dataframe containing merged metadata primer data.","code":""},{"path":"/reference/prepare_reads.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare reads for primer trimming using Cutadapt — prepare_reads","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"Prepare reads primer trimming using Cutadapt","code":""},{"path":"/reference/prepare_reads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"prepare_reads(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\",   multithread = FALSE,   overwrite_existing = FALSE )"},{"path":"/reference/prepare_reads.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run. multithread Logical, indicating whether use multithreading certain operations. Default FALSE. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/prepare_reads.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"list containing data tables, including metadata, primer sequences search based orientation, paths trimming reads, user-defined parameters subsequent steps.","code":""},{"path":"/reference/prepare_reads.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"# Pre-filter raw reads and parse metadata and primer_information to prepare  # for primer trimming and filter analysis_setup<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),    output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",    overwrite_existing = FALSE ) #> Existing data detected: Primer counts and N's may have been removed from previous runs. Loading existing output. To perform a new analysis, specify overwrite_existing = TRUE. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 22 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (3): already_trimmed, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 4 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): sample_name, primer_name, well, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 16 Columns: 7 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): primer_name, orientation, sequence #> dbl (4): S1_R1, S1_R2, S2_R1, S2_R2 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."},{"path":"/reference/primer_check.html","id":null,"dir":"Reference","previous_headings":"","what":"Matching Order Primer Check — primer_check","title":"Matching Order Primer Check — primer_check","text":"Matching Order Primer Check","code":""},{"path":"/reference/primer_check.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matching Order Primer Check — primer_check","text":"","code":"primer_check(fastq_data)"},{"path":"/reference/primer_check.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matching Order Primer Check — primer_check","text":"fastq_data data frame FASTQ file paths, direction sequences, names sequences","code":""},{"path":"/reference/primer_check.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matching Order Primer Check — primer_check","text":"None","code":""},{"path":"/reference/process_single_barcode.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"Process information ASV abundance matrix run DADA2 single barcode","code":""},{"path":"/reference/process_single_barcode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"","code":"process_single_barcode(   data_tables,   temp_directory_path,   output_directory_path,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   locus = barcode )"},{"path":"/reference/process_single_barcode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants tryRC (Optional). Default FALSE.  TRUE, reverse-complement sequences used classification better match reference sequences forward sequence. verbose (Optional). Default FALSE. TRUE, print status standard output. multithread (Optional). Default FALSE. TRUE, multithreading enabled number available threads automatically determined.    integer provided, number threads use set passing argument setThreadOptions.","code":""},{"path":"/reference/read_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"Takes FASTQ files user creates data frame paths files created used future. Included larger 'read_prefilt_fastq' function.","code":""},{"path":"/reference/read_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"","code":"read_fastq(data_directory_path, temp_directory_path)"},{"path":"/reference/read_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data_directory_path path directory containing FASTQ, metadata, primer_info files temp_directory_path User-defined temporary directory place reads throughout workflow.","code":""},{"path":"/reference/read_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data frame FASTQ file paths, primer orientations sequences, parsed sample names","code":""},{"path":"/reference/read_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"Take user's DADA2 parameters make dataframe downstream steps","code":""},{"path":"/reference/read_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"","code":"read_parameters(primers_params_path)"},{"path":"/reference/read_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/read_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"data frame information DADA2 parameters.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"function calling read_fastq, primer_check, remove_ns functions. process edit FASTQ make ready trimming primers Cutadapt. Part larger 'prepare_reads' function.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"","code":"read_prefilt_fastq(   data_directory_path = data_directory_path,   multithread = FALSE,   temp_directory_path )"},{"path":"/reference/read_prefilt_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"data_directory_path path directory containing FASTQ, metadata.csv, primerinfo_params.csv files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"Returns filtered reads Ns","code":""},{"path":"/reference/remove_ns.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Wrapper function core DADA2 filter trim function first filtering step","code":""},{"path":"/reference/remove_ns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"","code":"remove_ns(fastq_data, multithread = TRUE, temp_directory_path)"},{"path":"/reference/remove_ns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"fastq_data data frame fastq file paths, direction sequences, names sequences metadata, primer_info files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow metadata metadata containing concatenated metadata primer data","code":""},{"path":"/reference/remove_ns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Return prefiltered reads Ns","code":""},{"path":"/reference/run_cutadapt.html","id":null,"dir":"Reference","previous_headings":"","what":"Core function for running cutadapt — run_cutadapt","title":"Core function for running cutadapt — run_cutadapt","text":"Core function running cutadapt","code":""},{"path":"/reference/run_cutadapt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core function for running cutadapt — run_cutadapt","text":"","code":"run_cutadapt(   cutadapt_path,   cutadapt_data_barcode,   barcode_params,   minCutadaptlength )"},{"path":"/reference/run_cutadapt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core function for running cutadapt — run_cutadapt","text":"cutadapt_path path cutadapt program. minCutadaptlength Read lengths lower threshold discarded. Default 50. cutadapt_data Directory_data folder trimmed filtered reads sample.","code":""},{"path":"/reference/run_cutadapt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core function for running cutadapt — run_cutadapt","text":"Trimmed read.","code":""},{"path":"/reference/setup_directories.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up directory paths for subsequent analyses — setup_directories","title":"Set up directory paths for subsequent analyses — setup_directories","text":"function sets directory paths subsequent analyses. checks whether specified output directories exist creates . function also provides paths primer metadata files within data directory.","code":""},{"path":"/reference/setup_directories.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up directory paths for subsequent analyses — setup_directories","text":"","code":"setup_directories(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\" )"},{"path":"/reference/setup_directories.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up directory paths for subsequent analyses — setup_directories","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run.","code":""},{"path":"/reference/setup_directories.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up directory paths for subsequent analyses — setup_directories","text":"list paths data, output, temporary directories, primer, metadata files.","code":""}]
