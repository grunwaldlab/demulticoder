[{"path":"/articles/DADA2_16S_mothur_validation.html","id":"this-vignette-shows-how-demulticoder-was-used-to-analyze-the-mothur-16s-sop-dataset-featured-in-dada2-tutorials","dir":"Articles","previous_headings":"","what":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","title":"16S Mothur SOP Validation","text":"First, make sure input metadata primerinfo_params files data folder included necessary second file name barcode selected, primer sequences, optional DADA2 parameter options. referenced DADA2 tutorial select proper parameter options.","code":""},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"loading-the-package","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Loading the Package","title":"16S Mothur SOP Validation","text":"now, package loaded retrieving GitHub. submitting package CRAN.","code":"devtools::install_github(\"grunwaldlab/demulticoder\", force=TRUE) #devtools::load_all(\"~/demulticoder\") library(\"demulticoder\") library(dplyr) library(kableExtra)"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-1","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 1","title":"16S Mothur SOP Validation","text":"Remove N’s create directory structure downstream steps","code":"outputs<-prepare_reads(   data_directory = \"~/benchmark_demulticoder/mothur_16S_sop/data\",    output_directory = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs\",   tempdir_path = \"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs_temp\",   tempdir_id = \"temp_files\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-2","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 2","title":"16S Mothur SOP Validation","text":"Run Cutadapt remove primers trim reads DADA2 filterAndTrim function can now visualize outputs primer removal trimming steps. CSV files output showing samples still primer sequences barplot summarizes outputs. circumstances primer sequences may still remain. , ASVs residual primer sequences filtered end.","code":"cut_trim(   outputs,   cutadapt_path=\"/usr/bin/cutadapt\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-3","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 3","title":"16S Mothur SOP Validation","text":"Core ASV inference step can now visualize outputs ASV inference step. first plot shows reads merged terms mismatches indels. plot right shows overlap lengths across inferred ASVs.  can also look distribution ASV lengths","code":"make_asv_abund_matrix(   outputs)"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-4","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 4","title":"16S Mothur SOP Validation","text":"Assign taxonomy step check can take look read counts across workflow. sudden drops, reconsider adjusting certain DADA2 parameters re-running analysis.","code":"assign_tax(   outputs,   asv_abund_matrix,   db_16S=\"silva_nr99_v138.2_toSpecies_trainset.fa.gz\",   retrieve_files=FALSE) #> Tables showing read counts throughout the demulticoder DADA2 workflow #>    samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           F3D0_r16S  7733     7112      6976      6979   6540    6528 #> 2           F3D1_r16S  5829     5299      5226      5239   5027    5016 #> 3         F3D141_r16S  5926     5463      5331      5357   4986    4863 #> 4         F3D142_r16S  3158     2914      2799      2830   2595    2521 #> 5         F3D143_r16S  3164     2941      2822      2868   2553    2519 #> 6         F3D144_r16S  4798     4312      4150      4228   3646    3507 #> 7         F3D145_r16S  7331     6741      6592      6627   6079    5820 #> 8         F3D146_r16S  4993     4560      4450      4470   3968    3879 #> 9         F3D147_r16S 16956    15636     15433     15505  14233   13006 #> 10        F3D148_r16S 12332    11412     11250     11267  10529    9935 #> 11        F3D149_r16S 13006    12017     11857     11898  11154   10653 #> 12        F3D150_r16S  5474     5032      4879      4925   4349    4240 #> 13          F3D2_r16S 19489    18075     17907     17939  17431   16835 #> 14          F3D3_r16S  6726     6250      6145      6176   5850    5486 #> 15          F3D5_r16S  4418     4052      3930      3991   3713    3713 #> 16          F3D6_r16S  7933     7369      7231      7294   6865    6678 #> 17          F3D7_r16S  5103     4765      4646      4673   4428    4217 #> 18          F3D8_r16S  5274     4871      4786      4802   4576    4547 #> 19          F3D9_r16S  7023     6504      6341      6442   6092    6015 #> 20          Mock_r16S  4748     4314      4287      4286   4269    4269"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-5","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 5","title":"16S Mothur SOP Validation","text":"Convert asv matrix taxmap phyloseq objects one function","code":"objs<-convert_asv_matrix_to_objs(outputs) #> Rows: 232 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): asv_id, sequence, dada2_tax #> dbl (20): F3D0_r16S, F3D1_r16S, F3D141_r16S, F3D142_r16S, F3D143_r16S, F3D14... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For r16S dataset  #> Taxmap object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/taxmap_obj_r16S.RData  #> Phyloseq object saved in: ~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/phylo_obj_r16S.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-6","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 6","title":"16S Mothur SOP Validation","text":"Evaluate accuracy using mock community, shown dada2 tutorial looking mock community sample, able extract 20 bacterial sequences 0% mismatch, matched described previously.","code":"track_reads_demulticoder<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/track_reads_r16S.csv\", row.names = 1) summary(track_reads_demulticoder) #>      input          filtered       denoisedF       denoisedR     #>  Min.   : 3158   Min.   : 2914   Min.   : 2799   Min.   : 2830   #>  1st Qu.: 4944   1st Qu.: 4498   1st Qu.: 4409   1st Qu.: 4424   #>  Median : 5878   Median : 5381   Median : 5278   Median : 5298   #>  Mean   : 7571   Mean   : 6982   Mean   : 6852   Mean   : 6890   #>  3rd Qu.: 7783   3rd Qu.: 7176   3rd Qu.: 7040   3rd Qu.: 7058   #>  Max.   :19489   Max.   :18075   Max.   :17907   Max.   :17939   #>      merged         nonchim      #>  Min.   : 2553   Min.   : 2519   #>  1st Qu.: 4194   1st Qu.: 4132   #>  Median : 5006   Median : 4940   #>  Mean   : 6444   Mean   : 6212   #>  3rd Qu.: 6621   3rd Qu.: 6566   #>  Max.   :17431   Max.   :16835  tax_matrix<-read.csv(\"~/benchmark_demulticoder/mothur_16S_sop/vignette_outputs/final_asv_abundance_matrix_r16S.csv\")  unqs.mock <- tax_matrix[, c(2, which(colnames(tax_matrix) == \"Mock_r16S\"))]  unqs.mock <- unqs.mock[unqs.mock$Mock_r16S != 0,]  cat(\"DADA2 inferred\", nrow(unqs.mock), \"sample sequences present in the Mock community.\\n\") #> DADA2 inferred 20 sample sequences present in the Mock community.  mock.ref <- dada2::getSequences(file.path(\"~/benchmark_demulticoder/mothur_16S_sop/data\", \"HMP_MOCK.v35.fasta\")) match.ref <- sum(sapply(unqs.mock$sequence, function(x) any(grepl(x, mock.ref)))) cat(\"Of those,\", sum(match.ref), \"were exact matches to the expected reference sequences.\\n\") #> Of those, 20 were exact matches to the expected reference sequences."},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-7","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 7","title":"16S Mothur SOP Validation","text":"Follow-work using phyloseq side--side comparison dada2 example examine alpha diversity results","code":"objs$phyloseq_r16S <- phyloseq::prune_samples(phyloseq::sample_names(objs$phyloseq_r16S) != \"Mock_r16S\", objs$phyloseq_r16S) # Remove mock sample phyloseq::plot_richness(objs$phyloseq_r16S, x=\"Day\", measures=c(\"Shannon\", \"Simpson\"), color=\"When\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-8","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 8","title":"16S Mothur SOP Validation","text":"Examine ordination plots additional point comparison DADA2 tutorial","code":"# Transform data to proportions as appropriate for Bray-Curtis distances ps.prop <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(otu) otu/sum(otu)) ord.nmds.bray <- phyloseq::ordinate(ps.prop, method=\"NMDS\", distance=\"bray\") phyloseq::plot_ordination(ps.prop, ord.nmds.bray, color=\"When\", title=\"Bray NMDS\")"},{"path":"/articles/DADA2_16S_mothur_validation.html","id":"step-9","dir":"Articles","previous_headings":"This vignette shows how demulticoder was used to analyze the mothur 16S SOP dataset featured in DADA2 tutorials","what":"Step 9","title":"16S Mothur SOP Validation","text":"Let’s look top 20 taxa early vs. late samples time points, shown dada2 tutorial","code":"top20 <- names(sort(phyloseq::taxa_sums(objs$phyloseq_r16S), decreasing=TRUE))[1:20] ps.top20 <- phyloseq::transform_sample_counts(objs$phyloseq_r16S, function(OTU) OTU/sum(OTU)) ps.top20 <- phyloseq::prune_taxa(top20, ps.top20) phyloseq::plot_bar(ps.top20, x=\"Day\", fill=\"Family\") + ggplot2::facet_wrap(~When, scales=\"free_x\")"},{"path":"/articles/Documentation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Documentation","text":"documentation provides comprehensive information use Demulticoder R package processing analyzing metabarcode sequencing data. covers input file requirements, parameter settings, key parameters.","code":""},{"path":"/articles/Documentation.html","id":"quick-start-guide","dir":"Articles","previous_headings":"","what":"Quick Start Guide","title":"Documentation","text":"Prepare input files (metadata.csv, primerinfo_params.csv, unformatted reference databases, PE Illumina read files). Place input files single directory. Ensure file names comply specified format. Run pipeline default settings adjust parameters needed.","code":""},{"path":[]},{"path":"/articles/Documentation.html","id":"directory-structure","dir":"Articles","previous_headings":"Input Files","what":"Directory Structure","title":"Documentation","text":"Place input files single directory. directory contain following files: metadata.csv primerinfo_params.csv PE Illumina read files Unformatted reference databases","code":""},{"path":[]},{"path":"/articles/Documentation.html","id":"read-name-format","dir":"Articles","previous_headings":"Input Files > File Naming Conventions","what":"Read Name Format","title":"Documentation","text":"avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz Examples permissible sample names follows: Sample1_R1.fastq.gz Sample1_R2.fastq.gz permissible names : Sample1_001_R1.fastq.gz Sample1_001_R2.fastq.gz permissible : Sample1_001_R1_001.fastq.gz Sample1_001_R2_001.fastq.gz","code":""},{"path":"/articles/Documentation.html","id":"metadata-csv","dir":"Articles","previous_headings":"Input Files","what":"metadata.csv","title":"Documentation","text":"metadata.csv file contains information samples primers (associated barcodes) used experiment. following two required columns: sample_name: Identifier sample (e.g., S1, S2) primer_name: Name primer used (e.g., rps10, , r16S, other1, other2) Please add associated metadata file two required columns. can used downstream exploratory diversity analyses, sample data incorporated final phyloseq taxmap objects. Example file (optional third column):","code":"sample_name,primer_name,organism S1,rps10,Cry S2,rps10,Cin S1,its,Cry S2,its,Cin"},{"path":"/articles/Documentation.html","id":"primerinfo_params-csv","dir":"Articles","previous_headings":"Input Files","what":"primerinfo_params.csv","title":"Documentation","text":"primerinfo_params.csv file contains information primer sequences used experiment, along optional additional parameters part DADA2 pipeline. anything specified, default values used. Required columns: primer_name: Name primer/barcode (e.g., , rps10) forward: Forward primer sequence reverse: Reverse primer sequence DADA2 filterAndTrim function parameters: already_trimmed: Boolean indicating primers already trimmed (TRUE/FALSE) (default: FALSE) minCutadaptlength: Cutadapt parameter-Minimum length Cutadapt trimming (default: 0) multithread: Boolean multithreading (TRUE/FALSE) (default: FALSE) verbose: Boolean verbose output (TRUE/FALSE) (default: FALSE) maxN: Maximum number N bases allowed (default: 0) maxEE_forward: Maximum expected errors forward reads (default: Inf) maxEE_reverse: Maximum expected errors reverse reads (default: Inf) truncLen_forward: Truncation length forward reads (default: 0) truncLen_reverse: Truncation length reverse reads (default: 0) truncQ: Truncation quality threshold (default: 2) minLen: Minimum length reads processing (default: 20) maxLen: Maximum length reads processing (default: Inf) minQ: Minimum quality score (default: 0) trimLeft: Number bases trim start reads (default: 0) trimRight: Number bases trim end reads (default: 0) rm.lowcomplex: Boolean removing low complexity sequences (default: TRUE) DADA2 learnErrors function parameters: nbases: Number bases use error rate learning (default: 1e+08) randomize: Randomize reads error rate learning (default: FALSE) MAX_CONSIST: Maximum number self-consistency iterations (default: 10) OMEGA_C: Convergence threshold error rates (default: 0) qualityType: Quality score type (“Auto”, “FastqQuality”, “ShortRead”) (default: “Auto”) DADA2 plot errors parameters: err_out: Return error rates used inference (default: TRUE) err_in: Use input error rates instead learning (default: FALSE) nominalQ: Use nominal Q-scores (default: FALSE) obs: Return observed error rates (default: TRUE) DADA2 dada function parameters: OMP: Use OpenMP multi-threading available (default: TRUE) n: Number reads use error rate estimation (default: 1e+05) id.sep: Character separating sample ID sequence name (default: “\\s”) orient.fwd: NULL TRUE/FALSE orient sequences (default: NULL) pool: Pool samples error rate estimation (default: FALSE) selfConsist: Perform self-consistency iterations (default: FALSE) DADA2 mergePairs function parameters: minOverlap: Minimum overlap merging paired-end reads (default: 12) maxMismatch: Maximum mismatches allowed overlap region (default: 0) DADA2 removeBimeraDenovo function parameters: method: Method sample inference (“consensus” “pooled”) (default: “consensus”) parameters include CSV input file: min_asv_length: Minimum length Amplicon Sequence Variants (ASVs) core dada ASV inference steps (default=0) Example file (select optional columns forward reverse primer sequence columns):","code":"primer_name,forward,reverse,already_trimmed,minCutadaptlength,multithread,verbose,maxN,maxEE_forward,maxEE_reverse,truncLen_forward,truncLen_reverse,truncQ,minLen,maxLen,minQ,trimLeft,trimRight,rm.lowcomplex,minOverlap,maxMismatch,min_asv_length rps10,GTTGGTTAGAGYARAAGACT,ATRYYTAGAAAGAYTYGAACT,FALSE,100,TRUE,FALSE,1.00E+05,5,5,0,0,5,150,Inf,0,0,0,0,15,0,50 its,CTTGGTCATTTAGAGGAAGTAA,GCTGCGTTCTTCATCGATGC,FALSE,50,TRUE,FALSE,1.00E+05,5,5,0,0,5,50,Inf,0,0,0,0,15,0,50"},{"path":"/articles/Documentation.html","id":"reference-database","dir":"Articles","previous_headings":"Input Files","what":"Reference Database","title":"Documentation","text":"Databases copied user-specified data folder raw data files csv files located. names parameters assignTax function. now, package compatible following databases: oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ UNITE fungal database https://unite.ut.ee/repository.php two reference databases. user need reformat headers exactly outlined DADA2 database format, similar SILVA database format. user can specify path database input file. database fasta format.","code":""},{"path":"/articles/Documentation.html","id":"faq","dir":"Articles","previous_headings":"","what":"FAQ","title":"Documentation","text":"progress","code":""},{"path":"/articles/Documentation.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Documentation","text":"progress","code":""},{"path":"/articles/Getting_started.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before You Start","title":"Getting Started","text":"following example, demonstrate key package functionality using test dataset included package can follow along test data associated CSV input files contained package. Additional examples also available [Example vignettes]tab","code":""},{"path":"/articles/Getting_started.html","id":"components-of-the-test-dataset","dir":"Articles","previous_headings":"","what":"Components of the test dataset","title":"Getting Started","text":"Files: S1_R1.fastq.gz, S1_R2.fastq.gz, S2_R1.fastq.gz, S2_R1.fastq.gz files must end either R1.fastq.gz , R2.fastq.gz sample must R1 R2 files. New row unique sample Samples entered twice samples contain two pooled metabarcodes, test data template New row unique barcode associated primer sequence Optional cutadapt DADA2 parameters UNITE fungal database (abridged version) oomyceteDB","code":""},{"path":"/articles/Getting_started.html","id":"format-of-the-pe-read-files","dir":"Articles","previous_headings":"","what":"Format of the PE read files","title":"Getting Started","text":"package takes forward reverse Illumina short read sequence data. Format file names avoid errors, characters acceptable sample names letters numbers. Characters can separated underscores, symbols. files must end suffix R1.fastq.gz R2.fastq.gz.","code":""},{"path":"/articles/Getting_started.html","id":"format-of-metadata-file-metadata-csv","dir":"Articles","previous_headings":"","what":"Format of metadata file (metadata.csv)","title":"Getting Started","text":"format CSV file simple. template . Column 1. sample_name Column 2. primer_info Additional columns columns pasted two columns. can referenced later analysis steps save step loading metadata later. Notes S1 S2 come rhododendron rhizobiome dataset random subset full set reads. S1 S2 included twice ‘metadata.csv’ sheet. two samples contain pooled metabarcodes (ITS1 rps10). demultiplex run analyses tandem, include sample twice sample_name, change primer_name. metadata.csv looks like test dataset:","code":""},{"path":"/articles/Getting_started.html","id":"format-of-primer-and-parameters-file-primerinfo_parms-csv","dir":"Articles","previous_headings":"","what":"Format of primer and parameters file (primerinfo_parms.csv)","title":"Getting Started","text":"DADA2 Primer sequence information user-defined parameters placed *primerinfo_params.csv simplify functions called, user provide parameters within input file. recommend using template linked . Remember add additional optional DADA2 parameters want use. Required columns: primer_name Compatible options: rps10, , r16S, other1, other2 forward primer sequence reverse primer sequence Example template ‘primerinfo_params.csv’ info parameter specifics, see Documentation","code":""},{"path":"/articles/Getting_started.html","id":"reference-database-format","dir":"Articles","previous_headings":"","what":"Reference Database Format","title":"Getting Started","text":"oomycetedb : https://grunwaldlab.github.io/OomyceteDB/ SILVA 16S database species assignments: https://www.arb-silva.de/ UNITE fungal database https://unite.ut.ee/repository.php See ‘Documentation’ tab. Databases downloaded sources placed user-specified data folder raw data files csv files located.","code":""},{"path":"/articles/Getting_started.html","id":"additional-notes","dir":"Articles","previous_headings":"","what":"Additional Notes","title":"Getting Started","text":"Computer specifications may limiting factor. using SILVA UNITE databases taxonomic assignment steps, ordinary personal computer (unless sufficient RAM) may enough memory taxonomic assignment steps, even samples. test databases reads subsetted following example run personal computer least 16 GB RAM. computer crashes taxonomic assignment step, need switch computer sufficient memory. must ensure enough storage save intermediate files temporary directory (default) user-specified directory proceeding.","code":""},{"path":"/articles/Getting_started.html","id":"loading-the-package","dir":"Articles","previous_headings":"","what":"Loading the Package","title":"Getting Started","text":"now, package loaded retrieving GitHub. Submission CRAN progress.","code":"devtools::install_github(\"grunwaldlab/demulticoder\") library(\"demulticoder\") library(\"metacoder\")"},{"path":"/articles/Getting_started.html","id":"reorganize-data-tables-and-set-up-data-directory-structure","dir":"Articles","previous_headings":"","what":"Reorganize data tables and set-up data directory structure","title":"Getting Started","text":"sample names, primer sequences, metadata reorganized preparation running Cutadapt remove primers.","code":"analysis_setup<-demulticoder::prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = \"~/output_test_dataset\",    tempdir_path = \"~/temp_test_dataset\",   tempdir_id = \"test_dataset\",   overwrite_existing = TRUE)"},{"path":"/articles/Getting_started.html","id":"remove-primers-with-cutadapt","dir":"Articles","previous_headings":"","what":"Remove primers with Cutadapt","title":"Getting Started","text":"running Cutadapt, please ensure installed ","code":"demulticoder::cut_trim(   analysis_setup,   cutadapt_path = \"/usr/bin/cutadapt\", #CHANGE LOCATION TO YOUR LOCAL INSTALLATION   overwrite_existing = TRUE) #> Running Cutadapt 3.5 for its sequence data #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 3.5 for rps10 sequence data #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences."},{"path":"/articles/Getting_started.html","id":"asv-inference-step","dir":"Articles","previous_headings":"","what":"ASV inference step","title":"Getting Started","text":"Raw reads merged ASVs inferred","code":"make_asv_abund_matrix(   analysis_setup,   overwrite_existing = TRUE) #> 710804 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 660 unique sequences. #> Sample 2 - 1215 reads in 613 unique sequences. #> 724230 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input. #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821851 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10 #> Warning in scale_y_log10(): log-10 transformation introduced #> infinite values. #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input. #> Identified 0 bimeras out of 5 input sequences. #> $its #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"~/temp_test_dataset/test_dataset/asvabund_matrixDADA2_rps10.RData\""},{"path":"/articles/Getting_started.html","id":"taxonomic-assignment-step","dir":"Articles","previous_headings":"","what":"Taxonomic assignment step","title":"Getting Started","text":"Using core assignTaxonomy function DADA2, taxonomic assignments given ASVs.","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   retrieve_files=TRUE,   overwrite_existing = TRUE) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/articles/Getting_started.html","id":"format-of-output-matrices","dir":"Articles","previous_headings":"","what":"Format of output matrices","title":"Getting Started","text":"default output CSV file per metabarcode inferred ASVs sequences, taxonomic assignments, bootstrap supports provided DADA2. data can used input downstream steps. show first rows rps10 matrix checking ASV sequences look correct length taxonomic assignments seem reasonable. can also inspect first rows ITS1 matrix","code":"rps10_matrix<-read.csv(\"final_asv_abundance_matrix_rps10.csv\", header=TRUE)  kable(rps10_matrix, row.names = TRUE) %>%   kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\", full_width = F, font_size = 6)) %>%   scroll_box(width = \"100%\", height = \"100px\", extra_css = \"thead th { white-space: nowrap; }\") its_matrix<-read.csv(\"final_asv_abundance_matrix_its.csv\", header=TRUE)  kable(its_matrix, row.names = TRUE) %>%   kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\", full_width = F, font_size = 6)) %>%   scroll_box(width = \"100%\", height = \"100px\", extra_css = \"thead th { white-space: nowrap; }\")"},{"path":"/articles/Getting_started.html","id":"reformat-asv-matrix-as-taxmap-and-phyloseq-objects-after-optional-filtering-of-low-abundance-asvs","dir":"Articles","previous_headings":"","what":"Reformat ASV matrix as taxmap and phyloseq objects after optional filtering of low abundance ASVs","title":"Getting Started","text":"consider assignments bootstrap score 60","code":"objs<-convert_asv_matrix_to_objs(analysis_setup, minimum_bootstrap = 60, save_outputs = TRUE) #> Rows: 38 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_its.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 60 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 5 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: ~/output_test_dataset/taxmap_obj_rps10.RData  #> Phyloseq object saved in: ~/output_test_dataset/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 60 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/Getting_started.html","id":"objects-can-now-be-used-for-downstream-data-analysis","dir":"Articles","previous_headings":"","what":"Objects can now be used for downstream data analysis","title":"Getting Started","text":"make heattrees using taxmap object. First make heat tree ITS1-barcoded samples  Now make heat tree rps10-barcoded samples  can also variety analyses, convert phyloseq object demonstrate make stacked bar plot relative abundance taxa sample ITS1-barcoded samples  Finally, demonstrate make stacked bar plot relative abundance taxa sample rps10-barcoded samples","code":"objs$taxmap_its %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") objs$taxmap_rps10 %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") data <- objs$phyloseq_its %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot) data <- objs$phyloseq_rps10 %>%   phyloseq::transform_sample_counts(function(x) {x/sum(x)} ) %>%    phyloseq::psmelt() %>%                                           dplyr::filter(Abundance > 0.02) %>%                         dplyr::arrange(Genus)                                        abund_plot <- ggplot2::ggplot(data, ggplot2::aes(x = Sample, y = Abundance, fill = Genus)) +    ggplot2::geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", size = 0.2) +   ggplot2::scale_fill_viridis_d() +   ggplot2::theme_minimal() +   ggplot2::labs(     y = \"Relative Abundance\",     title = \"Relative abundance of taxa by sample\",     fill = \"Genus\"   ) +   ggplot2::theme(     axis.text.x = ggplot2::element_text(angle = 90, hjust = 1, vjust = 0.5, size = 14),     panel.grid.major = ggplot2::element_blank(),     panel.grid.minor = ggplot2::element_blank(),     legend.position = \"top\",     legend.text = ggplot2::element_text(size = 14),     legend.title = ggplot2::element_text(size = 14),  # Adjust legend title size     strip.text = ggplot2::element_text(size = 14),     strip.background = ggplot2::element_blank()   ) +   ggplot2::guides(     fill = ggplot2::guide_legend(       reverse = TRUE,       keywidth = 1,       keyheight = 1,       title.position = \"top\",       title.hjust = 0.5,  # Center the legend title       label.theme = ggplot2::element_text(size = 10)  # Adjust the size of the legend labels     )   )  print(abund_plot)"},{"path":"/articles/Pooled_barcode_example.html","id":"this-vignette-shows-how-demulticoder-can-be-used-when-you-have-pooled-rps10-and-its1-amplicons-within-each-sample-","dir":"Articles","previous_headings":"","what":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","title":"Pooled barcode example","text":"example shows analysis done full rhododendron dataset showcased Getting Started vignette. Reads databases sub-sampled test dataset.","code":""},{"path":"/articles/Pooled_barcode_example.html","id":"prepare-input-files","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Prepare input files","title":"Pooled barcode example","text":"First, make sure input metadata primerinfo_params files data folder required columns first sample names, second primer name/barcode used. subsequent columns user-specific columns downstream steps purposes example, just show first rows metadata.csv file included necessary second file name barcode selected, primer sequences, optional DADA2 parameter options. referenced DADA2 tutorial select proper parameter options.","code":""},{"path":"/articles/Pooled_barcode_example.html","id":"loading-the-package","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Loading the Package","title":"Pooled barcode example","text":"now, package loaded retrieving GitHub. submitting package CRAN","code":"devtools::install_github(\"grunwaldlab/demulticoder\", force=TRUE) #devtools::load_all(\"~/demulticoder\") library(\"demulticoder\")"},{"path":"/articles/Pooled_barcode_example.html","id":"step-1","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Step 1","title":"Pooled barcode example","text":"Remove N’s create directory structure downstream steps","code":"outputs <- prepare_reads(      data_directory = \"~/benchmark_demulticoder/demulticoder/data\",    output_directory = \"~/benchmark_demulticoder/demulticoder/vignette_outputs\",   tempdir_path = \"~/benchmark_demulticoder/demulticoder/vignette_temp\",   tempdir_id = \"temp_files\")"},{"path":"/articles/Pooled_barcode_example.html","id":"step-2","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Step 2","title":"Pooled barcode example","text":"Run Cutadapt remove primers trim reads DADA2 filterAndTrim function can now visualize outputs primer removal trimming steps. CSV files output showing samples still primer sequences barplot summarizes outputs. circumstances primer sequences may still remain. , ASVs residual primer sequences filtered end.","code":"cut_trim(   outputs,   cutadapt_path=\"/usr/bin/cutadapt\")"},{"path":"/articles/Pooled_barcode_example.html","id":"step-3","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Step 3","title":"Pooled barcode example","text":"Core ASV inference step can now visualize outputs ASV inference step. first plot shows reads merged terms mismatches indels. plot right shows overlap lengths across inferred ASVs. First see rps10 plots  Second, see plots  can also look distribution ASV lengths First see rps10 plots  see plots","code":"make_asv_abund_matrix(   outputs,   overwrite_existing = FALSE)"},{"path":"/articles/Pooled_barcode_example.html","id":"step-4","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Step 4","title":"Pooled barcode example","text":"Assign taxonomy step check can take look read counts across workflow. sudden drops, reconsider adjusting certain DADA2 parameters re-running analysis. First track rps10 reads demulticoder DADA2 workflow track ITS1 reads","code":"assign_tax(   outputs,   asv_abund_matrix,   db_its = \"sh_general_release_dynamic_18.07.2023.fasta\",   db_rps10 = \"oomycetedb_release2.fasta\",   retrieve_files=TRUE) #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10 36752    29365     29351     29346  29310   29310 #> 2          S10_rps10 27709    22996     22926     22985  22910   22903 #> 3         S100_rps10 52392    40666     40640     40617  38989   38989 #> 4         S101_rps10 41784    33442     33374     33386  33324   33161 #> 5         S102_rps10 90519    67515     67489     67466  67216   67157 #> 6         S103_rps10 71282    51653     51436     51637  51372   51372 #> Tables showing read counts throughout the demulticoder DADA2 workflow #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its 50668    28885     28662     28564  27695   27695 #> 2            S10_its 26971    13293     13083     13038  12214   12214 #> 3           S100_its 19379    11748     11559     11484  11121   11121 #> 4           S101_its 33203    15654     15416     15304  13061   13041 #> 5           S102_its 33268    19467     19251     19097  18467   18467 #> 6           S103_its 31711    20252     19942     19738  19095   19095"},{"path":"/articles/Pooled_barcode_example.html","id":"step-5","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Step 5","title":"Pooled barcode example","text":"Convert asv matrix taxmap phyloseq objects one function","code":"objs<-convert_asv_matrix_to_objs(outputs) #> Rows: 2372 Columns: 177 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr   (3): asv_id, sequence, dada2_tax #> dbl (174): S1_its, S10_its, S100_its, S101_its, S102_its, S103_its, S104_its... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: ~/benchmark_demulticoder/demulticoder/vignette_outputs/taxmap_obj_its.RData  #> Phyloseq object saved in: ~/benchmark_demulticoder/demulticoder/vignette_outputs/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 323 Columns: 177 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr   (3): asv_id, sequence, dada2_tax #> dbl (174): S1_rps10, S10_rps10, S100_rps10, S101_rps10, S102_rps10, S103_rps... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: ~/benchmark_demulticoder/demulticoder/vignette_outputs/taxmap_obj_rps10.RData  #> Phyloseq object saved in: ~/benchmark_demulticoder/demulticoder/vignette_outputs/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/articles/Pooled_barcode_example.html","id":"step-6-lets-make-some-quick-visuals-heat-trees-to-take-a-look-at-outputs-of-the-analyses","dir":"Articles","previous_headings":"This vignette shows how demulticoder can be used when you have pooled rps10 and ITS1 amplicons within each sample.","what":"Step 6-let’s make some quick visuals (heat trees) to take a look at outputs of the analyses","title":"Pooled barcode example","text":"point rps10 ITS1 analyses separate. Given greater complexity dataset, robust analyses detailed associated package manuscript analyses code also linked. First make heat tree -barcoded samples  Now make heat tree rps10-barcoded samples","code":"min_bootstrap <- 80  objs$taxmap_its$data$score$boot <- as.numeric(objs$taxmap_its$data$score$boot) max_boot <- objs$taxmap_its$data$score %>%   dplyr::group_by(taxon_id) %>%   dplyr::summarise(max = max(boot)) max_boot <- setNames(max_boot$max, max_boot$taxon_id) obj_subset_its <- filter_taxa(objs$taxmap_its, max_boot[taxon_ids] >= min_bootstrap | taxon_ranks %in% c(\"ASV\", \"Reference\"), reassign_obs = c(abund = TRUE, score = FALSE)) #> Warning: There is no \"taxon_id\" column in the data set \"4\", so there are no #> taxon IDs.  obj_subset_its %>%   filter_taxa(! is_stem) %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\") #> Warning: There is no \"taxon_id\" column in the data set \"4\", so there are no #> taxon IDs. min_bootstrap <- 80  objs$taxmap_rps10$data$score$boot <- as.numeric(objs$taxmap_rps10$data$score$boot) max_boot <- objs$taxmap_rps10$data$score %>%   dplyr::group_by(taxon_id) %>%   dplyr::summarise(max = max(boot)) max_boot <- setNames(max_boot$max, max_boot$taxon_id) obj_subset_rps10 <- filter_taxa(objs$taxmap_rps10, max_boot[taxon_ids] >= min_bootstrap | taxon_ranks %in% c(\"ASV\", \"Reference\"), reassign_obs = c(abund = TRUE, score = FALSE)) #> Warning: There is no \"taxon_id\" column in the data set \"4\", so there are no #> taxon IDs. obj_subset_rps10 %>%   filter_taxa(! grepl(x = taxon_names, \"_sp$\"), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"incertae_sedis\", ignore.case = TRUE), reassign_obs = FALSE) %>%   filter_taxa(! grepl(x = taxon_names, \"NA\", ignore.case = TRUE), reassign_obs = FALSE) %>%   metacoder::heat_tree(node_label = taxon_names,                        node_size = n_obs,                        node_color = n_obs,                        node_color_axis_label = \"ASV count\",                        node_size_axis_label = \"Total Abundance of Taxa\",                        layout = \"da\", initial_layout = \"re\")"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Martha . Sudermann. Author, maintainer. Zachary S. L Foster. Author. Samantha Dawson. Author. Hung Phan. Author. Jeff H. Chang. Author. Niklaus Grünwald. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sudermann M, Foster Z, Dawson S, Phan H, H. Chang J, Grünwald N (2025). demulticoder: R Package Integrated Analysis Multiplex Metabarcodes. R package version 0.0.0.9000, https://grunwaldlab.github.io/demulticoder/.","code":"@Manual{,   title = {demulticoder: An R Package for the Integrated Analysis of Multiplex Metabarcodes},   author = {Martha A. Sudermann and Zachary S. L Foster and Samantha Dawson and Hung Phan and Jeff {H. Chang} and Niklaus Grünwald},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://grunwaldlab.github.io/demulticoder/}, }"},{"path":[]},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Introduction","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"demulticoder package cutadapt DADA2 wrapper package metabarcodng analyses. main commands outputs intuitive comprehensive, helps account complex iterative nature metabarcoding analyses. brief schematic general workflow:","code":""},{"path":"/index.html","id":"key-features","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Key Features","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"automates use DADA2 analyze data derived multiple metabarcodes. reduces number manual input steps Handles analysis two metabarcodes multiplexed sequencing batch Analyze different types metabarcodes simultaneously Reproducible workflows oomycetes Supported metabarcodes: 16S rDNA, ITS1, rps10, two additional metabarcodes","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Installation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"Dependencies: First install cutadapt program following instructions : https://cutadapt.readthedocs.io/en/stable/installation.html Second, make sure following R packages installed: install, follow instructions: https://www.bioconductor.org/packages/release/bioc/html/dada2.html install: https://www.bioconductor.org/packages/release/bioc/html/phyloseq.html using latest version R (4.4.2) R studio (2024.12.0+467), may temporarily need install metacoder Github, CRAN approves latest version. install development version package (submission CRAN progress):","code":"#Here we install demulticoder (instructions will be updated once available  #through CRAN) devtools::install_github(\"grunwaldlab/demulticoder\") library(\"demulticoder\")  #If you need to install metacoder but have the latest version of R and R studio,  #you can temporarily install metacoder as follows.  #This message will be removed once updates to CRAN are made.   devtools::install_github(\"grunwaldlab/metacoder\")  #Let's make sure other packages are loaded: library(\"devtools\") library(\"DADA2\") library(\"phyloseq\") library(\"metacoder\")"},{"path":"/index.html","id":"quick-start","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Quick start","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"1. Set-input directory files demonstrate use package, small test dataset comes loaded package. dataset used workflow example . Already loaded test dataset directory following files: Files: S1_R1.fastq.gz, S1_R2.fastq.gz, S2_R1.fastq.gz, S2_R1.fastq.gz files must end either R1.fastq.gz , R2.fastq.gz sample must R1 R2 files. New row unique sample Samples entered twice samples contain two pooled metabarcodes, test data template New row unique barcode associated primer sequence Optional cutadapt DADA2 parameters UNITE fungal database (abridged version) oomyceteDB See Documentation format databases input files. details step, check Getting Started tab package website 2. Prepare reads 3. Cut trim reads User must install cutadapt local machine append path executable. 4. Make ASV abundance matrix 5. Assign taxonomy 6. Convert ASV matrix taxmap phyloseq objects","code":"output<-prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"), # This allows us to use the test directory located within the package   output_directory = \"<OUTDIR>\",   overwrite_existing = TRUE) # Change to you preferred location on your local computer (Example: \"~/demulticoder_test\") cut_trim(   output,   cutadapt_path=\"<CUTADAPTPATH>\",) # Change to the location on your computer. (Example: \"/usr/bin/cutadapt\")   overwrite_existing = TRUE) make_asv_abund_matrix(   output,   overwrite_existing = TRUE) assign_tax(   output,   asv_abund_matrix,   overwrite_existing = TRUE) objs<-convert_asv_matrix_to_objs(output)"},{"path":"/index.html","id":"check-out-the-associated-github-repository-to-view-source-code","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Check out the associated Github repository to view source code","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"information, see source code, submit issue, check :https://github.com/grunwaldlab/demulticoder/","code":""},{"path":"/index.html","id":"citation","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Citation","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"package developed Martha Sudermann, Zachary Foster, Samantha Dawson, Hung Phan, Jeff Chang, Niklaus Grünwald Stay tuned associated manuscript.","code":""},{"path":"/index.html","id":"acknowledgements","dir":"","previous_headings":"demulticoder: An R package for the simultaneous analysis of multiplexed metabarcodes","what":"Acknowledgements","title":"An R Package for the Integrated Analysis of Multiplex Metabarcodes","text":"demulticoder logo created https://BioRender.com/","code":""},{"path":"/reference/add_pid_to_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Add PID and bootstrap values to tax result. — add_pid_to_tax","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"Add PID bootstrap values tax result.","code":""},{"path":"/reference/add_pid_to_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"","code":"add_pid_to_tax(tax_results, asv_pid)"},{"path":"/reference/add_pid_to_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add PID and bootstrap values to tax result. — add_pid_to_tax","text":"tax_results dataframe containing taxonomic assignments asv_pid Percent identity information ASV relative reference database sequence","code":""},{"path":"/reference/assignTax_as_char.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"Combine taxonomic assignments bootstrap values locus single falsification vector","code":""},{"path":"/reference/assignTax_as_char.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"","code":"assignTax_as_char(tax_results, temp_directory_path, locus)"},{"path":"/reference/assignTax_as_char.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine taxonomic assignments and bootstrap values for each locus into single falsification vector — assignTax_as_char","text":"tax_results dataframe containing taxonomic assignments","code":""},{"path":"/reference/assign_tax.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy functions — assign_tax","title":"Assign taxonomy functions — assign_tax","text":"Assign taxonomy functions","code":""},{"path":"/reference/assign_tax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy functions — assign_tax","text":"","code":"assign_tax(   analysis_setup,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   retrieve_files = FALSE,   overwrite_existing = FALSE,   db_rps10 = \"oomycetedb.fasta\",   db_its = \"fungidb.fasta\",   db_16S = \"bacteriadb.fasta\",   db_other1 = \"otherdb1.fasta\",   db_other2 = \"otherdb2.fasta\" )"},{"path":"/reference/assign_tax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy functions — assign_tax","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function asv_abund_matrix ASV abundance matrix. tryRC Whether try reverse complementing sequences taxonomic assignment verbose Logical, indicating whether display verbose output multithread Logical, indicating whether use multithreading retrieve_files Specify TRUE/FALSE whether copy files temp directory output directory overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. db_rps10 reference database rps10 locus db_its reference database locus db_16S reference database 16S locus db_other1 reference database different locus 1 (assumes format like SILVA DB entries) db_other2 reference database different locus 2 (assumes format like SILVA DB entries)","code":""},{"path":"/reference/assign_tax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign taxonomy functions — assign_tax","text":"Taxonomic assignments unique ASV sequence","code":""},{"path":"/reference/assign_tax.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Assign taxonomy functions — assign_tax","text":"point DADA2 assignTaxonomy used assign taxonomy inferred ASVs.","code":""},{"path":"/reference/assign_tax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign taxonomy functions — assign_tax","text":"","code":"# Assign taxonomies to ASVs on a per barcode basis analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpkUBPWj/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Running Cutadapt 3.5 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 3.5 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> 710804 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 660 unique sequences. #> Sample 2 - 1215 reads in 613 unique sequences. #> 724230 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input.  #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821851 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values.  #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/tmp/RtmpkUBPWj/demulticoder_run_temp/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/RtmpkUBPWj/demulticoder_run_temp/asvabund_matrixDADA2_rps10.RData\" #>  assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing = TRUE ) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503"},{"path":"/reference/assign_taxonomyDada2.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign taxonomy — assign_taxonomyDada2","title":"Assign taxonomy — assign_taxonomyDada2","text":"Assign taxonomy","code":""},{"path":"/reference/assign_taxonomyDada2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign taxonomy — assign_taxonomyDada2","text":"","code":"assign_taxonomyDada2(   asv_abund_matrix,   temp_directory_path,   minBoot = 0,   tryRC = FALSE,   verbose = FALSE,   multithread = TRUE,   locus = \"barcode\" )"},{"path":"/reference/assign_taxonomyDada2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign taxonomy — assign_taxonomyDada2","text":"asv_abund_matrix ASV abundance matrix temp_directory_path temporary directory path minBoot Minimum bootstrap value taxonomy assignment (default 0) tryRC Try reverse complement (default FALSE) verbose Print additional information (default FALSE) multithread Use multiple threads (default TRUE) locus locus taxonomy assignment (e.g., rps10, other1, other2)","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"Filter ASV abundance matrix convert taxmap phyloseq objects","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"convert_asv_matrix_to_objs(   analysis_setup,   min_read_depth = 0,   minimum_bootstrap = 0,   save_outputs = FALSE )"},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function min_read_depth ASV filter parameter. mean read depth across samples less threshold, ASV filtered. minimum_bootstrap Threshold bootstrap support value taxonomic assignments. designated minimum bootstrap threshold, taxnomoic assignments set N/","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"ASV matrix converted taxmap object","code":""},{"path":"/reference/convert_asv_matrix_to_objs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter ASV abundance matrix and convert to taxmap and phyloseq objects — convert_asv_matrix_to_objs","text":"","code":"# Convert final matrix to taxmap and phyloseq objects for downstream analysis steps analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpkUBPWj/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Running Cutadapt 3.5 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 3.5 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> 710804 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 660 unique sequences. #> Sample 2 - 1215 reads in 613 unique sequences. #> 724230 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input.  #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821851 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values.  #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/tmp/RtmpkUBPWj/demulticoder_run_temp/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/RtmpkUBPWj/demulticoder_run_temp/asvabund_matrixDADA2_rps10.RData\" #>  assign_tax( analysis_setup, asv_abund_matrix,  retrieve_files=FALSE,  overwrite_existing=TRUE ) #> Duplicate sequences detected and merged. #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1             S1_its  2564     1479      1425      1431   1315    1315 #> 2             S2_its  1996     1215      1143      1122   1063    1063 #>   samplename_barcode input filtered denoisedF denoisedR merged nonchim #> 1           S1_rps10  1830     1429      1429      1422   1420    1420 #> 2           S2_rps10  2090     1506      1505      1505   1503    1503 objs<-convert_asv_matrix_to_objs( analysis_setup ) #> Rows: 38 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_its, S2_its #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For its dataset  #> Taxmap object saved in: /tmp/RtmpkUBPWj/taxmap_obj_its.RData  #> Phyloseq object saved in: /tmp/RtmpkUBPWj/phylo_obj_its.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #> Rows: 5 Columns: 5 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): asv_id, sequence, dada2_tax #> dbl (2): S1_rps10, S2_rps10 #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> For rps10 dataset  #> Taxmap object saved in: /tmp/RtmpkUBPWj/taxmap_obj_rps10.RData  #> Phyloseq object saved in: /tmp/RtmpkUBPWj/phylo_obj_rps10.RData  #> ASVs filtered by minimum read depth: 0  #> For taxonomic assignments, if minimum bootstrap was set to: 0 assignments were set to 'Unsupported'  #> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"},{"path":"/reference/countOverlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Count overlap to see how well the reads were merged — countOverlap","title":"Count overlap to see how well the reads were merged — countOverlap","text":"Count overlap see well reads merged","code":""},{"path":"/reference/countOverlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count overlap to see how well the reads were merged — countOverlap","text":"","code":"countOverlap(data_tables, merged_reads, barcode, output_directory_path)"},{"path":"/reference/countOverlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count overlap to see how well the reads were merged — countOverlap","text":"data_tables data tables containing paths read files, metadata, primer sequences merged_reads Intermediate merged read R data file barcode barcode used analysis output_directory_path path directory resulting files output","code":""},{"path":"/reference/countOverlap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count overlap to see how well the reads were merged — countOverlap","text":"plot describing well reads merged information overlap reads","code":""},{"path":"/reference/createASVSequenceTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Make ASV sequence matrix — createASVSequenceTable","title":"Make ASV sequence matrix — createASVSequenceTable","text":"Make ASV sequence matrix","code":""},{"path":"/reference/createASVSequenceTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make ASV sequence matrix — createASVSequenceTable","text":"","code":"createASVSequenceTable(merged_reads, orderBy = \"abundance\")"},{"path":"/reference/createASVSequenceTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make ASV sequence matrix — createASVSequenceTable","text":"merged_reads Intermediate merged read R data file orderBy (Optional). character(1). Default \"abundance\". Specifies sequences (columns) returned table ordered (decreasing). Valid values: \"abundance\", \"nsamples\", NULL.","code":""},{"path":"/reference/createASVSequenceTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make ASV sequence matrix — createASVSequenceTable","text":"raw_seqtab","code":""},{"path":"/reference/cut_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"Main command trim primers using Cutadapt core DADA2 functions","code":""},{"path":"/reference/cut_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"","code":"cut_trim(analysis_setup, cutadapt_path, overwrite_existing = FALSE)"},{"path":"/reference/cut_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"analysis_setup object containing directory paths data tables, produced prepare_reads function cutadapt_path Path Cutadapt program. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE.","code":""},{"path":"/reference/cut_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"Trimmed reads, primer counts, quality plots, ASV matrix.","code":""},{"path":"/reference/cut_trim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"samples comprised two different barcodes (like ITS1 rps10), reads also demultiplexed prior DADA2 trimming steps.","code":""},{"path":"/reference/cut_trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main command to trim primers using Cutadapt and core DADA2 functions — cut_trim","text":"","code":"# Remove remaining primers from raw reads, demultiplex pooled barcoded samples,  # and then trim reads based on specific DADA2 parameters analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpkUBPWj/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\",  overwrite_existing = TRUE ) #> Running Cutadapt 3.5 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 3.5 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences."},{"path":"/reference/filter_and_trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Wrapper function filterAndTrim function DADA2, used primer trimming","code":""},{"path":"/reference/filter_and_trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"","code":"filter_and_trim(   output_directory_path,   temp_directory_path,   cutadapt_data_barcode,   barcode_params,   barcode )"},{"path":"/reference/filter_and_trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"output_directory_path path directory resulting files output cutadapt_data_barcode directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/filter_and_trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for filterAndTrim function from DADA2, to be used after primer trimming — filter_and_trim","text":"Filtered trimmed reads","code":""},{"path":"/reference/format_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Format ASV abundance matrix — format_abund_matrix","title":"Format ASV abundance matrix — format_abund_matrix","text":"Format ASV abundance matrix","code":""},{"path":"/reference/format_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format ASV abundance matrix — format_abund_matrix","text":"","code":"format_abund_matrix(   data_tables,   asv_abund_matrix,   seq_tax_asv,   output_directory_path,   locus )"},{"path":"/reference/format_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format ASV abundance matrix — format_abund_matrix","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants seq_tax_asv amplified sequence variants matrix taxonomic information","code":""},{"path":"/reference/format_database.html","id":null,"dir":"Reference","previous_headings":"","what":"General functions to format user-specified databases — format_database","title":"General functions to format user-specified databases — format_database","text":"General functions format user-specified databases","code":""},{"path":"/reference/format_database.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"General functions to format user-specified databases — format_database","text":"","code":"format_database(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   barcode,   db_its,   db_rps10,   db_16S,   db_other1,   db_other2 )"},{"path":"/reference/format_database.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"General functions to format user-specified databases — format_database","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files barcode barcode database formatted","code":""},{"path":"/reference/format_database.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"General functions to format user-specified databases — format_database","text":"formatted database based specified barcode type","code":""},{"path":"/reference/format_db_16S.html","id":null,"dir":"Reference","previous_headings":"","what":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"16S database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_16S.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"","code":"format_db_16S(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_16S )"},{"path":"/reference/format_db_16S.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_16S name database","code":""},{"path":"/reference/format_db_16S.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An 16S database that has modified headers and is output in the reference_databases folder — format_db_16S","text":"16S database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":null,"dir":"Reference","previous_headings":"","what":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_its.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"","code":"format_db_its(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_its )"},{"path":"/reference/format_db_its.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_its name database","code":""},{"path":"/reference/format_db_its.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An ITS database that has modified headers and is output in the reference_databases folder — format_db_its","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other1.html","id":null,"dir":"Reference","previous_headings":"","what":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":", user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"","code":"format_db_other1(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other1 )"},{"path":"/reference/format_db_other1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other1 name database","code":""},{"path":"/reference/format_db_other1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An other, user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other1","text":"database modified headers output reference_databases folder.","code":""},{"path":"/reference/format_db_other2.html","id":null,"dir":"Reference","previous_headings":"","what":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"second user-specified database initially format specified DADA2 header simply taxonomic levels (kingdom species, separated semi-colons, ;)","code":""},{"path":"/reference/format_db_other2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"","code":"format_db_other2(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_other2 )"},{"path":"/reference/format_db_other2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_other2 name database","code":""},{"path":"/reference/format_db_other2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An second user-specified database that is initially in the format specified by DADA2 with header simply taxonomic levels (kingdom down to species, separated by semi-colons, ;) — format_db_other2","text":"database modified headers output reference_databases folder","code":""},{"path":"/reference/format_db_rps10.html","id":null,"dir":"Reference","previous_headings":"","what":"Create modified reference rps10 database for downstream analysis — format_db_rps10","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"Create modified reference rps10 database downstream analysis","code":""},{"path":"/reference/format_db_rps10.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"","code":"format_db_rps10(   data_tables,   data_path,   output_directory_path,   temp_directory_path,   db_rps10 )"},{"path":"/reference/format_db_rps10.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"data_tables data tables containing paths read files, metadata, primer sequences data_path Path data directory output_directory_path path directory resulting files output temp_directory_path User-defined temporary directory place reads throughout workflow metadata, primer_info files db_rps10 name database","code":""},{"path":"/reference/format_db_rps10.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create modified reference rps10 database for downstream analysis — format_db_rps10","text":"rps10 database modified headers output reference_databases folder.","code":""},{"path":"/reference/get_fastq_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"Retrieve paths filtered trimmed Fastq files","code":""},{"path":"/reference/get_fastq_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"","code":"get_fastq_paths(data_tables, my_direction, my_primer_pair_id)"},{"path":"/reference/get_fastq_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the paths of the filtered and trimmed Fastq files — get_fastq_paths","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Whether primer forward reverse direction my_primer_pair_id specific barcode id cutadapt_data directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/get_pids.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"Align ASV sequences reference sequences database get percent ID. Get percent identities.","code":""},{"path":"/reference/get_pids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"","code":"get_pids(tax_results, temp_directory_path, output_directory_path, db, locus)"},{"path":"/reference/get_pids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Get percent identities. — get_pids","text":"tax_results data frame containing taxonomic assignments","code":""},{"path":"/reference/get_post_trim_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_post_trim_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"","code":"get_post_trim_hits(primer_data, cutadapt_data, output_directory_path)"},{"path":"/reference/get_post_trim_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"primer_data primer data frame created orient_primers function cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_post_trim_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample after primer removal and trimming steps — get_post_trim_hits","text":"Table read counts across sample","code":""},{"path":"/reference/get_pre_primer_hits.html","id":null,"dir":"Reference","previous_headings":"","what":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"Get primer counts reach sample primer removal trimming steps","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"","code":"get_pre_primer_hits(primer_data, fastq_data, output_directory_path)"},{"path":"/reference/get_pre_primer_hits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"primer_data primer data data frame created orient_primers function fastq_data data frame FASTQ file paths, direction sequences, names sequences output_directory_path path directory resulting files output","code":""},{"path":"/reference/get_pre_primer_hits.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get primer counts for reach sample before primer removal and trimming steps — get_pre_primer_hits","text":"number reads primer found number reads primer found","code":""},{"path":"/reference/get_read_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"Final inventory read counts step input removal chimeras. function deals one sample. TODO optimize one sample","code":""},{"path":"/reference/get_read_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"","code":"get_read_counts(   asv_abund_matrix,   temp_directory_path,   output_directory_path,   locus )"},{"path":"/reference/get_read_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Final inventory of read counts after each step from input to removal of chimeras. This function deals with if you have more than one sample. TODO optimize for one sample — get_read_counts","text":"asv_abund_matrix abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/get_ref_seq.html","id":null,"dir":"Reference","previous_headings":"","what":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"Align ASV sequences reference sequences database get percent ID. Start retrieving reference sequences.","code":""},{"path":"/reference/get_ref_seq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"","code":"get_ref_seq(tax_results, db)"},{"path":"/reference/get_ref_seq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Align ASV sequences to reference sequences from database to get percent ID. Start by retrieving reference sequences. — get_ref_seq","text":"tax_results dataframe containing taxonomic assignments db reference database","code":""},{"path":"/reference/infer_asv_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to infer ASVs, for multiple loci — infer_asv_command","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"Function infer ASVs, multiple loci","code":""},{"path":"/reference/infer_asv_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"","code":"infer_asv_command(   output_directory_path,   temp_directory_path,   data_tables,   barcode_params,   barcode )"},{"path":"/reference/infer_asv_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to infer ASVs, for multiple loci — infer_asv_command","text":"output_directory_path path directory resulting files output data_tables data tables containing paths read files, metadata, primer sequences denoised_data_path Path saved intermediate denoised data","code":""},{"path":"/reference/infer_asvs.html","id":null,"dir":"Reference","previous_headings":"","what":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"Core DADA2 function learn errors infer ASVs","code":""},{"path":"/reference/infer_asvs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"","code":"infer_asvs(   data_tables,   my_direction,   my_primer_pair_id,   barcode_params,   output_directory_path )"},{"path":"/reference/infer_asvs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"data_tables data tables containing paths read files, metadata, primer sequences my_direction Location read files metadata file my_primer_pair_id specific barcode id output_directory_path path directory containing fastq, metadata, primerinfo_params files","code":""},{"path":"/reference/infer_asvs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core DADA2 function to learn errors and infer ASVs — infer_asvs","text":"asv_data","code":""},{"path":"/reference/make_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"Quality filtering remove chimeras short sequences","code":""},{"path":"/reference/make_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"","code":"make_abund_matrix(   raw_seqtab,   temp_directory_path,   barcode_params = barcode_params,   barcode )"},{"path":"/reference/make_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"raw_seqtab R data file raw sequence data prior removal chimeras","code":""},{"path":"/reference/make_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quality filtering to remove chimeras and short sequences — make_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"Make amplified sequence variant (ASV) abundance matrix input barcodes","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"","code":"make_asv_abund_matrix(analysis_setup, overwrite_existing = FALSE)"},{"path":"/reference/make_asv_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"analysis_setup analysis_setup object containing directory paths data tables, produced prepare_reads function overwrite_existing Logical, indicating whether overwrite existing results. Default FALSE.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"ASV abundance matrix (asv_abund_matrix)","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"function processes data unique barcode separately, inferring ASVs, merging reads, creating ASV abundance matrix. , DADA2 core denoising alogrithm used infer ASVs.","code":""},{"path":"/reference/make_asv_abund_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make an amplified sequence variant (ASV) abundance matrix for each of the input barcodes — make_asv_abund_matrix","text":"","code":"# The primary wrapper function for DADA2 ASV inference steps analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpkUBPWj/demulticoder_run_temp/prefiltered_sequences  cut_trim( analysis_setup, cutadapt_path=\"/usr/bin/cutadapt\", overwrite_existing = TRUE ) #> Running Cutadapt 3.5 for its sequence data  #> Read in 2564 paired-sequences, output 1479 (57.7%) filtered paired-sequences. #> Read in 1996 paired-sequences, output 1215 (60.9%) filtered paired-sequences. #> Running Cutadapt 3.5 for rps10 sequence data  #> Read in 1830 paired-sequences, output 1429 (78.1%) filtered paired-sequences. #> Read in 2090 paired-sequences, output 1506 (72.1%) filtered paired-sequences.  make_asv_abund_matrix( analysis_setup,  overwrite_existing = TRUE ) #> 710804 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Forward read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 660 unique sequences. #> Sample 2 - 1215 reads in 613 unique sequences. #> 724230 total bases in 2694 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair its  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1479 reads in 1019 unique sequences. #> Sample 2 - 1215 reads in 814 unique sequences. #> 1315 paired-reads (in 21 unique pairings) successfully merged out of 1416 (in 32 pairings) input. #> Duplicate sequences in merged output. #> 1063 paired-reads (in 25 unique pairings) successfully merged out of 1108 (in 28 pairings) input.  #> Duplicate sequences detected and merged. #> Identified 0 bimeras out of 38 input sequences. #> 824778 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #> Convergence after  2  rounds. #> Error rate plot for the Forward read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values. #> Sample 1 - 1429 reads in 933 unique sequences. #> Sample 2 - 1506 reads in 1018 unique sequences. #> 821851 total bases in 2935 reads from 2 samples will be used for learning the error rates. #> Initializing error rates to maximum possible estimate. #> selfConsist step 1 .. #>    selfConsist step 2 #>    selfConsist step 3 #> Convergence after  3  rounds. #> Error rate plot for the Reverse read of primer pair rps10  #> Warning: log-10 transformation introduced infinite values.  #> Sample 1 - 1429 reads in 1044 unique sequences. #> Sample 2 - 1506 reads in 1284 unique sequences. #> 1420 paired-reads (in 2 unique pairings) successfully merged out of 1422 (in 4 pairings) input. #> 1503 paired-reads (in 5 unique pairings) successfully merged out of 1504 (in 6 pairings) input.  #> Identified 0 bimeras out of 5 input sequences.  #> $its #> [1] \"/tmp/RtmpkUBPWj/demulticoder_run_temp/asvabund_matrixDADA2_its.RData\" #>  #> $rps10 #> [1] \"/tmp/RtmpkUBPWj/demulticoder_run_temp/asvabund_matrixDADA2_rps10.RData\" #>"},{"path":"/reference/make_cutadapt_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Prepare primmer trimming Cutaapt. Make new sub-directories specify paths trimmed untrimmed reads","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"","code":"make_cutadapt_tibble(fastq_data, metadata, temp_directory_path)"},{"path":"/reference/make_cutadapt_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"fastq_data path FASTQ files analysis metadata, primer_info files metadata Loaded metadata pairing user's metadata file primer data temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/make_cutadapt_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare for primmer trimming with Cutaapt. Make new sub-directories and specify paths for the trimmed and untrimmed reads — make_cutadapt_tibble","text":"Returns larger data frame containing paths temporary read directories, used input running Cutadapt","code":""},{"path":"/reference/make_seqhist.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"Plots histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/make_seqhist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"","code":"make_seqhist(asv_abund_matrix, output_directory_path)"},{"path":"/reference/make_seqhist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"asv_abund_matrix returned final ASV abundance matrix","code":""},{"path":"/reference/make_seqhist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a histogram of read length counts of all sequences within the ASV matrix — make_seqhist","text":"histogram read length counts sequences within ASV matrix","code":""},{"path":"/reference/merge_reads_command.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge forward and reverse reads — merge_reads_command","title":"Merge forward and reverse reads — merge_reads_command","text":"Merge forward reverse reads","code":""},{"path":"/reference/merge_reads_command.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge forward and reverse reads — merge_reads_command","text":"","code":"merge_reads_command(   output_directory_path,   temp_directory_path,   barcode_params,   barcode )"},{"path":"/reference/merge_reads_command.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge forward and reverse reads — merge_reads_command","text":"output_directory_path path directory resulting files output merged_read_data_path Path R data file containing merged read data","code":""},{"path":"/reference/merge_reads_command.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge forward and reverse reads — merge_reads_command","text":"merged_reads Intermediate merged read R data file","code":""},{"path":"/reference/orient_primers.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"Take user's forward reverse sequences creates complement, reverse, reverse complement primers one data frame","code":""},{"path":"/reference/orient_primers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"","code":"orient_primers(primers_params_path)"},{"path":"/reference/orient_primers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/orient_primers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's forward and reverse sequences and creates the complement, reverse, reverse complement of primers in one data frame — orient_primers","text":"data frame oriented primer information.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Wrapper script plotQualityProfile trim steps primer removal.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"","code":"plot_post_trim_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_post_trim_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_post_trim_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper script for plotQualityProfile after trim steps and primer removal. — plot_post_trim_qc","text":"Quality profiles reads primer trimming","code":""},{"path":"/reference/plot_qc.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for plotQualityProfile function — plot_qc","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Wrapper function plotQualityProfile function","code":""},{"path":"/reference/plot_qc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"","code":"plot_qc(cutadapt_data, output_directory_path, n = 5e+05)"},{"path":"/reference/plot_qc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"cutadapt_data directory_data folder trimmed filtered reads sample output_directory_path path directory resulting files output n (Optional). Default 500,000. number records sample fastq file.","code":""},{"path":"/reference/plot_qc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for plotQualityProfile function — plot_qc","text":"Dada2 wrapper function making quality profiles sample","code":""},{"path":"/reference/prep_abund_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare final ASV abundance matrix — prep_abund_matrix","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"Prepare final ASV abundance matrix","code":""},{"path":"/reference/prep_abund_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"","code":"prep_abund_matrix(cutadapt_data, asv_abund_matrix, data_tables, locus)"},{"path":"/reference/prep_abund_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare final ASV abundance matrix — prep_abund_matrix","text":"asv_abund_matrix returned final ASV abundance matrix locus barcode selected analysis directory_data folder trimmed filtered reads sample","code":""},{"path":"/reference/prepare_metadata_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"Read metadata file user combine reformat , given primer data. Included larger function prepare_reads.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"","code":"prepare_metadata_table(metadata_file_path, primer_data)"},{"path":"/reference/prepare_metadata_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"primer_data data frame oriented primer information returned orient_primers function. metadata_path path metadata file.","code":""},{"path":"/reference/prepare_metadata_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read metadata file from user and combine and reformat it, given primer data. Included in a larger function prepare_reads. — prepare_metadata_table","text":"dataframe containing merged metadata primer data.","code":""},{"path":"/reference/prepare_reads.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare reads for primer trimming using Cutadapt — prepare_reads","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"Prepare reads primer trimming using Cutadapt","code":""},{"path":"/reference/prepare_reads.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"prepare_reads(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\",   overwrite_existing = FALSE )"},{"path":"/reference/prepare_reads.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run. overwrite_existing Logical, indicating whether remove overwrite existing files directories previous runs. Default FALSE. multithread Logical, indicating whether use multithreading certain operations. Default FALSE.","code":""},{"path":"/reference/prepare_reads.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"list containing data tables, including metadata, primer sequences search based orientation, paths trimming reads, user-defined parameters subsequent steps.","code":""},{"path":"/reference/prepare_reads.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare reads for primer trimming using Cutadapt — prepare_reads","text":"","code":"# Pre-filter raw reads and parse metadata and primer_information to prepare  # for primer trimming and filter analysis_setup <- prepare_reads(   data_directory = system.file(\"extdata\", package = \"demulticoder\"),   output_directory = tempdir(),   tempdir_path = tempdir(),   tempdir_id = \"demulticoder_run_temp\",   overwrite_existing = TRUE ) #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 2 Columns: 23 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr  (3): primer_name, forward, reverse #> dbl (16): minCutadaptlength, maxN, maxEE_forward, maxEE_reverse, truncLen_fo... #> lgl  (4): already_trimmed, count_all_samples, multithread, verbose #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Rows: 4 Columns: 3 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (3): sample_name, primer_name, organism #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> Creating output directory: /tmp/RtmpkUBPWj/demulticoder_run_temp/prefiltered_sequences"},{"path":"/reference/primer_check.html","id":null,"dir":"Reference","previous_headings":"","what":"Matching Order Primer Check — primer_check","title":"Matching Order Primer Check — primer_check","text":"Matching Order Primer Check","code":""},{"path":"/reference/primer_check.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matching Order Primer Check — primer_check","text":"","code":"primer_check(fastq_data)"},{"path":"/reference/primer_check.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matching Order Primer Check — primer_check","text":"fastq_data data frame FASTQ file paths, direction sequences, names sequences","code":""},{"path":"/reference/primer_check.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matching Order Primer Check — primer_check","text":"None","code":""},{"path":"/reference/process_single_barcode.html","id":null,"dir":"Reference","previous_headings":"","what":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"Process information ASV abundance matrix run DADA2 single barcode","code":""},{"path":"/reference/process_single_barcode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"","code":"process_single_barcode(   data_tables,   temp_directory_path,   output_directory_path,   asv_abund_matrix,   tryRC = FALSE,   verbose = FALSE,   multithread = FALSE,   locus = barcode )"},{"path":"/reference/process_single_barcode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process the information from an ASV abundance matrix to run DADA2 for single barcode — process_single_barcode","text":"data_tables data tables containing paths read files, metadata, primer sequences asv_abund_matrix abundance matrix containing amplified sequence variants","code":""},{"path":"/reference/read_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"Takes FASTQ files user creates data frame paths files created used future. Included larger 'read_prefilt_fastq' function.","code":""},{"path":"/reference/read_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"","code":"read_fastq(data_directory_path, temp_directory_path)"},{"path":"/reference/read_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data_directory_path path directory containing FASTQ, metadata, primer_info files temp_directory_path User-defined temporary directory place reads throughout workflow.","code":""},{"path":"/reference/read_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Takes in the FASTQ files from the user and creates a data frame with the paths to files that will be created and used in the future. Included in a larger 'read_prefilt_fastq' function. — read_fastq","text":"data frame FASTQ file paths, primer orientations sequences, parsed sample names","code":""},{"path":"/reference/read_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"Take user's DADA2 parameters make dataframe downstream steps","code":""},{"path":"/reference/read_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"","code":"read_parameters(primers_params_path)"},{"path":"/reference/read_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"primers_params_path path CSV file holds primer information.","code":""},{"path":"/reference/read_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Take in user's DADA2 parameters and make a dataframe for downstream steps — read_parameters","text":"data frame information DADA2 parameters.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":null,"dir":"Reference","previous_headings":"","what":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"function calling read_fastq, primer_check, remove_ns functions. process edit FASTQ make ready trimming primers Cutadapt. Part larger 'prepare_reads' function.","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"","code":"read_prefilt_fastq(   data_directory_path = data_directory_path,   multithread,   temp_directory_path )"},{"path":"/reference/read_prefilt_fastq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"data_directory_path path directory containing FASTQ, metadata.csv, primerinfo_params.csv files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow","code":""},{"path":"/reference/read_prefilt_fastq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function for calling read_fastq, primer_check, and remove_ns functions. This will process and edit the FASTQ and make them ready for the trimming of primers with Cutadapt. Part of a larger 'prepare_reads' function. — read_prefilt_fastq","text":"Returns filtered reads Ns","code":""},{"path":"/reference/remove_ns.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Wrapper function core DADA2 filter trim function first filtering step","code":""},{"path":"/reference/remove_ns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"","code":"remove_ns(fastq_data, multithread, temp_directory_path)"},{"path":"/reference/remove_ns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"fastq_data data frame fastq file paths, direction sequences, names sequences metadata, primer_info files multithread (Optional). Default FALSE.  TRUE, input files filtered parallel via mclapply.  integer provided, passed mc.cores argument mclapply.  Note parallelization forking, process loading another fastq file  memory. option ignored Windows, Windows support forking, mc.cores set 1. memory issue, execute clean environment reduce chunk size n / number threads. temp_directory_path User-defined temporary directory output unfiltered, trimmed, filtered read directories throughout workflow metadata metadata containing concatenated metadata primer data","code":""},{"path":"/reference/remove_ns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper function for core DADA2 filter and trim function for first filtering step — remove_ns","text":"Return prefiltered reads Ns","code":""},{"path":"/reference/run_cutadapt.html","id":null,"dir":"Reference","previous_headings":"","what":"Core function for running cutadapt — run_cutadapt","title":"Core function for running cutadapt — run_cutadapt","text":"Core function running cutadapt","code":""},{"path":"/reference/run_cutadapt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Core function for running cutadapt — run_cutadapt","text":"","code":"run_cutadapt(   cutadapt_path,   cutadapt_data_barcode,   barcode_params,   minCutadaptlength )"},{"path":"/reference/run_cutadapt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Core function for running cutadapt — run_cutadapt","text":"cutadapt_path path cutadapt program. minCutadaptlength Read lengths lower threshold discarded. Default 50. cutadapt_data Directory_data folder trimmed filtered reads sample.","code":""},{"path":"/reference/run_cutadapt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Core function for running cutadapt — run_cutadapt","text":"Trimmed read.","code":""},{"path":"/reference/setup_directories.html","id":null,"dir":"Reference","previous_headings":"","what":"Set up directory paths for subsequent analyses — setup_directories","title":"Set up directory paths for subsequent analyses — setup_directories","text":"function sets directory paths subsequent analyses. checks whether specified output directories exist creates . function also provides paths primer metadata files within data directory.","code":""},{"path":"/reference/setup_directories.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set up directory paths for subsequent analyses — setup_directories","text":"","code":"setup_directories(   data_directory = \"data\",   output_directory = \"output\",   tempdir_path = NULL,   tempdir_id = \"demulticoder_run\" )"},{"path":"/reference/setup_directories.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set up directory paths for subsequent analyses — setup_directories","text":"data_directory User-specified directory path user placed raw FASTQ (forward reverse reads), metadata.csv, primerinfo_params.csv files. Default \"data\". output_directory User-specified directory outputs. Default \"output\". tempdir_path Path temporary directory. NULL, temporary directory path identified using tempdir() command. tempdir_id ID temporary directories. Default \"demulticoder_run\". user can provide helpful ID, whether date specific name run.","code":""},{"path":"/reference/setup_directories.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set up directory paths for subsequent analyses — setup_directories","text":"list paths data, output, temporary directories, primer, metadata files.","code":""}]
